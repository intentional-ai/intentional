{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"CONTRIBUTING/","title":"Contributing to Intentional","text":"<p>Intentional is still in its very early stages of development, so it's very unstable and major design decision can be made suddenly. If you're building an app on top of Intentional and you are worried about its stability, reach out to me and let's talk about it first.</p>"},{"location":"CONTRIBUTING/#rules","title":"Rules","text":""},{"location":"CONTRIBUTING/#open-an-issue","title":"Open an issue","text":"<p>Before contributing, always open an issue before writing any code!</p> <p>Make sure I am aware of what you're working on and that I am willing to include your changes before working on them. I am committed to reply to each issue promptly to let you know if it's worth working on a feature or not, but I also reserve the right to close unprompted PRs, however well-made, that for any reason don't match the current aim and design goals of the project.</p>"},{"location":"CONTRIBUTING/#code-reviews","title":"Code Reviews","text":"<p>I will only merge PRs after doing a code review.</p>"},{"location":"CONTRIBUTING/#ci-must-be-green","title":"CI must be green","text":"<p>PRs with a failing CI check won't be merged. If CI is failing and you believe it's not your code's fault, tag me and I'll check what's wrong.</p>"},{"location":"CONTRIBUTING/#contributor-license-agreement-cla","title":"Contributor License Agreement (CLA)","text":"<p>All contributors must sign a Contributor License Agreement (CLA). A GitHub bot will take care of making sure you sign it when you open your first PR.</p> <p>You can review the content of the CLA here.</p>"},{"location":"docs/cla/","title":"Fiduciary License Agreement (FLA) for Contributors","text":"<p>based on the Entity Contributor Exclusive License Agreement (including the Traditional Patent License OPTION)</p> <p>Thank you for your interest in contributing to ZanSara's Intentional Project (\"We\" or \"Us\").</p> <p>The purpose of this contributor agreement (\"Agreement\") is to clarify and document the rights granted by contributors to Us. To make this document effective, please follow the instructions at https://intentional-ai.github.io/intentional/CONTRIBUTING/#contributor-license-agreement-cla.</p> <p>NOTE: By adding your GitHub username to the list of contributors in the <code>.clabot</code> file, you agree to the terms of this document.</p>"},{"location":"docs/cla/#0-preamble","title":"0. Preamble","text":"<p>Software is deeply embedded in all aspects of our lives and it is important that it empower, rather than restrict us. Free Software gives everybody the rights to use, understand, adapt and share software. These rights help support other fundamental freedoms like freedom of speech, press and privacy.</p> <p>Development of Free Software can follow many patterns. In some cases whole development is handled by a sole programmer or a small group of people. But usually, the creation and maintenance of software is a complex process that requires the contribution of many individuals. This also affects who owns the rights to the software. In the latter case, rights in software are owned jointly by a great number of individuals.</p> <p>To tackle this issue some projects require a full copyright assignment to be signed by all contributors. The problem with such assignments is that they often lack checks and balances that would protect the contributors from potential abuse of power from the new copyright holder.</p> <p>FSFE's Fiduciary License Agreement (FLA) was created by the Free Software Foundation Europe e.V. with just that in mind -- to concentrate all deciding power within one entity and prevent fragmentation of rights on one hand, while on the other preventing that single entity from abusing its power. The main aim is to ensure that the software covered under the FLA will forever remain Free Software.</p> <p>This process only serves for the transfer of economic rights. So-called moral rights (e.g. authors right to be identified as author) remain with the original author(s) and are inalienable.</p>"},{"location":"docs/cla/#how-to-use-this-fla","title":"How to use this FLA","text":"<p>If You are an employee and have created the Contribution as part of your employment, You need to have Your employer approve this Agreement or sign the Entity version of this document. If You do not own the Copyright in the entire work of authorship, any other author of the Contribution should also sign this -- in any event, please contact Us at github@zansara.dev.</p>"},{"location":"docs/cla/#1-definitions","title":"1. Definitions","text":"<p>\"You\" means the individual Copyright owner who Submits a Contribution to Us.</p> <p>\"Contribution\" means any original work of authorship, including any original modifications or additions to an existing work of authorship, Submitted by You to Us, in which You own the Copyright.</p> <p>\"Copyright\" means all rights protecting works of authorship, including copyright, moral and neighboring rights, as appropriate, for the full term of their existence.</p> <p>\"Material\" means the software or documentation made available by Us to third parties. When this Agreement covers more than one software project, the Material means the software or documentation to which the Contribution was Submitted. After You Submit the Contribution, it may be included in the Material.</p> <p>\"Submit\" means any act by which a Contribution is transferred to Us by You by means of tangible or intangible media, including but not limited to electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, Us, but excluding any transfer that is conspicuously marked or otherwise designated in writing by You as \"Not a Contribution.\"</p> <p>\"Documentation\" means any non-software portion of a Contribution.</p>"},{"location":"docs/cla/#2-license-grant","title":"2. License grant","text":""},{"location":"docs/cla/#21-copyright-license-to-us","title":"2.1 Copyright license to Us","text":"<p>Subject to the terms and conditions of this Agreement, You hereby grant to Us a worldwide, royalty-free, exclusive, perpetual and irrevocable (except as stated in Section 8.2) license, with the right to transfer an unlimited number of non-exclusive licenses or to grant sublicenses to third parties, under the Copyright covering the Contribution to use the Contribution by all means, including, but not limited to:</p> <ul> <li>publish the Contribution,</li> <li>modify the Contribution,</li> <li>prepare derivative works based upon or containing the Contribution and/or to combine the Contribution with other Materials,</li> <li>reproduce the Contribution in original or modified form,</li> <li>distribute, to make the Contribution available to the public, display and publicly perform the Contribution in original or modified form.</li> </ul>"},{"location":"docs/cla/#22-moral-rights","title":"2.2 Moral rights","text":"<p>Moral Rights remain unaffected to the extent they are recognized and not waivable by applicable law. Notwithstanding, You may add your name to the attribution mechanism customary used in the Materials you Contribute to, such as the header of the source code files of Your Contribution, and We will respect this attribution when using Your Contribution.</p>"},{"location":"docs/cla/#23-copyright-license-back-to-you","title":"2.3 Copyright license back to You","text":"<p>Upon such grant of rights to Us, We immediately grant to You a worldwide, royalty-free, non-exclusive, perpetual and irrevocable license, with the right to transfer an unlimited number of non-exclusive licenses or to grant sublicenses to third parties, under the Copyright covering the Contribution to use the Contribution by all means, including, but not limited to:</p> <ul> <li>publish the Contribution,</li> <li>modify the Contribution,</li> <li>prepare derivative works based upon or containing the Contribution and/or to combine the Contribution with other Materials,</li> <li>reproduce the Contribution in original or modified form,</li> <li>distribute, to make the Contribution available to the public, display and publicly perform the Contribution in original or modified form.</li> </ul> <p>This license back is limited to the Contribution and does not provide any rights to the Material.</p>"},{"location":"docs/cla/#3-patents","title":"3. Patents","text":""},{"location":"docs/cla/#31-patent-license","title":"3.1 Patent license","text":"<p>Subject to the terms and conditions of this Agreement You hereby grant to Us and to recipients of Materials distributed by Us a worldwide, royalty-free, non-exclusive, perpetual and irrevocable (except as stated in Section 3.2) patent license, with the right to transfer an unlimited number of non-exclusive licenses or to grant sublicenses to third parties, to make, have made, use, sell, offer for sale, import and otherwise transfer the Contribution and the Contribution in combination with any Material (and portions of such combination). This license applies to all patents owned or controlled by You, whether already acquired or hereafter acquired, that would be infringed by making, having made, using, selling, offering for sale, importing or otherwise transferring of Your Contribution(s) alone or by combination of Your Contribution(s) with any Material. 3.2 Revocation of patent license</p> <p>You reserve the right to revoke the patent license stated in section 3.1 if We make any infringement claim that is targeted at your Contribution and not asserted for a Defensive Purpose. An assertion of claims of the Patents shall be considered for a \"Defensive Purpose\" if the claims are asserted against an entity that has filed, maintained, threatened, or voluntarily participated in a patent infringement lawsuit against Us or any of Our licensees.</p>"},{"location":"docs/cla/#4-license-obligations-by-us","title":"4. License obligations by Us","text":"<p>We agree to (sub)license the Contribution or any Materials containing, based on or derived from your Contribution under the terms of any licenses the Free Software Foundation classifies as Free Software License and which are approved by the Open Source Initiative as Open Source licenses.</p> <p>More specifically and in strict accordance with the above paragraph, we agree to (sub)license the Contribution or any Materials containing, based on or derived from the Contribution only in accordance with our licensing policy available at: https://github.com/intentional-ai/intentional/blob/main/LICENSE.txt.</p> <p>We agree to license patents owned or controlled by You only to the extent necessary to (sub)license Your Contribution(s) and the combination of Your Contribution(s) with the Material under the terms of any licenses the Free Software Foundation classifies as Free Software licenses and which are approved by the Open Source Initiative as Open Source licenses.</p>"},{"location":"docs/cla/#5-disclaimer","title":"5. Disclaimer","text":"<p>THE CONTRIBUTION IS PROVIDED \"AS IS\". MORE PARTICULARLY, ALL EXPRESS OR IMPLIED WARRANTIES INCLUDING, WITHOUT LIMITATION, ANY IMPLIED WARRANTY OF SATISFACTORY QUALITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT ARE EXPRESSLY DISCLAIMED BY YOU TO US AND BY US TO YOU. TO THE EXTENT THAT ANY SUCH WARRANTIES CANNOT BE DISCLAIMED, SUCH WARRANTY IS LIMITED IN DURATION AND EXTENT TO THE MINIMUM PERIOD AND EXTENT PERMITTED BY LAW.</p>"},{"location":"docs/cla/#6-consequential-damage-waiver","title":"6. Consequential damage waiver","text":"<p>TO THE MAXIMUM EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT WILL YOU OR WE BE LIABLE FOR ANY LOSS OF PROFITS, LOSS OF ANTICIPATED SAVINGS, LOSS OF DATA, INDIRECT, SPECIAL, INCIDENTAL, CONSEQUENTIAL AND EXEMPLARY DAMAGES ARISING OUT OF THIS AGREEMENT REGARDLESS OF THE LEGAL OR EQUITABLE THEORY (CONTRACT, TORT OR OTHERWISE) UPON WHICH THE CLAIM IS BASED.</p>"},{"location":"docs/cla/#7-approximation-of-disclaimer-and-damage-waiver","title":"7. Approximation of disclaimer and damage waiver","text":"<p>IF THE DISCLAIMER AND DAMAGE WAIVER MENTIONED IN SECTION 5. AND SECTION 6. CANNOT BE GIVEN LEGAL EFFECT UNDER APPLICABLE LOCAL LAW, REVIEWING COURTS SHALL APPLY LOCAL LAW THAT MOST CLOSELY APPROXIMATES AN ABSOLUTE WAIVER OF ALL CIVIL OR CONTRACTUAL LIABILITY IN CONNECTION WITH THE CONTRIBUTION.</p>"},{"location":"docs/cla/#8-term","title":"8. Term","text":""},{"location":"docs/cla/#81","title":"8.1","text":"<p>This Agreement shall come into effect upon Your acceptance of the terms and conditions.</p>"},{"location":"docs/cla/#82","title":"8.2","text":"<p>This Agreement shall apply for the term of the copyright and patents licensed here. However, You shall have the right to terminate the Agreement if We do not fulfill the obligations as set forth in Section 4. Such termination must be made in writing.</p>"},{"location":"docs/cla/#83","title":"8.3","text":"<p>In the event of a termination of this Agreement Sections 5., 6., 7., 8., and 9. shall survive such termination and shall remain in full force thereafter. For the avoidance of doubt, Free and Open Source Software (sub)licenses that have already been granted for Contributions at the date of the termination shall remain in full force after the termination of this Agreement.</p>"},{"location":"docs/cla/#9-miscellaneous","title":"9. Miscellaneous","text":""},{"location":"docs/cla/#91","title":"9.1","text":"<p>This Agreement and all disputes, claims, actions, suits or other proceedings arising out of this agreement or relating in any way to it shall be governed by the laws of Portugal excluding its private international law provisions.</p>"},{"location":"docs/cla/#92","title":"9.2","text":"<p>This Agreement sets out the entire agreement between You and Us for Your Contributions to Us and overrides all other agreements or understandings.</p>"},{"location":"docs/cla/#93","title":"9.3","text":"<p>In case of Your death, this agreement shall continue with Your heirs. In case of more than one heir, all heirs must exercise their rights through a commonly authorized person.</p>"},{"location":"docs/cla/#94","title":"9.4","text":"<p>If any provision of this Agreement is found void and unenforceable, such provision will be replaced to the extent possible with a provision that comes closest to the meaning of the original provision and that is enforceable. The terms and conditions set forth in this Agreement shall apply notwithstanding any failure of essential purpose of this Agreement or any limited remedy to the maximum extent possible under law.</p>"},{"location":"docs/cla/#95","title":"9.5","text":"<p>You agree to notify Us of any facts or circumstances of which you become aware that would make this Agreement inaccurate in any respect.</p>"},{"location":"docs/concepts/","title":"High-level Concepts","text":"<p>Intentional lets you control the way your chatbot behaves by specifying a conversation graph made of several stages.</p> <p>At each stage, the bot has a very specific goal to accomplish and will stick to it until it reaches one of the outcomes you specify. Once the bot is confident that one of the outcomes is reached, it will move over to the next stage and continue along the conversation graph in this way.</p> <p>For example, here is an example of a conversation graph:</p> <pre><code>graph TD;\n\nstart:::highlight --&gt; name[\"ask for name\"]\nname -- \"name given\" --&gt; age[\"ask for age\"]\nage -- \"age given\" --&gt; address[\"ask for address\"]\naddress -- \"address given\" --&gt; confirm[confirm data]\nconfirm -- \"user confirms\" --&gt; bye[\"say goodbye\"]\nbye --&gt; _end[\"end\"]:::highlight\nconfirm -- \"mistakes found\" --&gt; amend[amend data]\namend[amend data] -- \"correct data given\" --&gt; confirm\namend -- \"user refuses\" --&gt; bye\nname -- \"user refuses\" --&gt; bye\nage -- \"user refuses\" --&gt; bye\naddress -- \"user refuses\" --&gt; bye\nconfirm -- \"user refuses\" --&gt; bye\n\nclassDef highlight fill:#aaf,stroke:#aaf;\n</code></pre>"},{"location":"docs/concepts/#stages","title":"Stages","text":"<p>Each box of this diagram is a stage. The bot is always going to be in one of these stages, and each of them includes the description of a goal that the bot needs to accomplish. For example, the goal of the stage <code>ask for name</code> might be \"Ask the user for their name\".</p> <p>Note</p> <p>The dark boxes (<code>start</code>, <code>end</code>) are not stages, but represent special operations that the bot will do once the necessary conditions are met, such a starting the chat or ending it. To learn more about these operations, check out the section about the configuration file.</p>"},{"location":"docs/concepts/#outcomes","title":"Outcomes","text":"<p>Each stage has a number of named arrows connecting it to other stages. For example, <code>ask for name</code> is connected to <code>ask for age</code> through the <code>name given</code> arrow and to <code>say goodbye</code> through the <code>user refuses</code> arrow. These arrows represent all the outcomes that we think are possible from a given stage, and make the bot transition from one stage to the next. Outcomes have their own description to help the bot understand when they are reached: for example, the description of the <code>name given</code> outcome may be \"The user clearly stated their name\".</p> <p>Note</p> <p>Have you noticed that there are no arrows connecting one stage to itself?</p> <p>There are also no generic outcomes that represent situations where the user did not understand the question, or when their reply is not matching the question in some way.</p> <p>This is because the bot can take care of these issues on its own. By default it will not change state until one of the described outcomes is reached. In all other cases, it will stay on the same stage and try again to get the conversation to one of the outcomes we defined and nothing else.</p>"},{"location":"docs/concepts/#tools","title":"Tools","text":"<p>Each stage can specify a list of tools that it must have access to. If a stage specifies a tool, it will only be available from that stage: for example, the <code>ask for address</code> stage may have access to a Maps API to validate the address that the user has given. Such tool won't be available when the bot is in any other stage, making it easier for the bot to avoid calling it by mistake.</p>"},{"location":"docs/concepts/#external-stages-and-backtracking","title":"External stages and backtracking","text":"<p>In real life conversations the bot may need to handle generic interruptions of its workflow. For example, the user may wonder why the bot is collecting this information about them only when they reach the address stage.</p> <p>Here is an example of a conversation graph with such a stage, which we are going to call <code>questions</code>.</p> <pre><code>graph TD;\n\nstart:::highlight --&gt; name[\"ask for name\"]\nname -- \"name given\" --&gt; age[\"ask for age\"]\nage -- \"age given\" --&gt; address[\"ask for address\"]\naddress -- \"address given\" --&gt; confirm[confirm data]\nconfirm -- \"user confirms\" --&gt; bye[\"say goodbye\"]\nbye --&gt; _end[\"end\"]:::highlight\nconfirm -- \"mistakes found\" --&gt; amend[amend data]\namend[amend data] -- \"correct data given\" --&gt; confirm\namend -- \"user refuses\" --&gt; bye\nname -- \"user refuses\" --&gt; bye\nage -- \"user refuses\" --&gt; bye\naddress -- \"user refuses\" --&gt; bye\nconfirm -- \"user refuses\" --&gt; bye\n\nall:::highlight -- \"user asks the bot about itself\" --&gt; questions\nquestions -- \"user has no more questions\" --&gt; backtrack:::highlight\n\nclassDef highlight fill:#aaf,stroke:#aaf;\n</code></pre> <p>In this case, the <code>question</code> state not only has a goal, but it also has a description: the bot should switch to it, regardless where it is right now, when the user does something that matched its description. In this case, the description may be \"The user asks the bot a question about itself\".</p> <p>Once the bot enters this stage, it must be able to return to wherever it was before jumping to this stage. For example, if it was asking the user for their address, it switch back to that state and ask for their address again. This will be done by the <code>backtrack</code> operation.</p> <p>Note</p> <p>In this simple example, the <code>question</code> stage is accessible from all other stages. However, stages can restrict access to themselves with an inclusion or an exclusion list. See the section about the configuration file to understand how this works.</p>"},{"location":"docs/concepts/#under-the-hood","title":"Under the hood","text":"<p>Warning</p> <p>Work in progress</p>"},{"location":"docs/config-file/","title":"Configuration File","text":""},{"location":"docs/config-file/#the-configuration-file","title":"The configuration file","text":"<p>The configuration file is the core of Intentional bots. They are YAML files that define a conversation such as the one we've seen above.</p> <p>Here is an example of a conversation file. Don't feel overwhelmed just yet! Each part will be explained separately.</p> <pre><code>interface: textual_ui\nmodality: text_messages\nbot:\n  type: direct_to_llm\n  llm:\n    client: openai\n    name: gpt-4o\n\nplugins:\n- intentional_textual_ui\n- intentional_openai\n\nconversation:\n  background: \"You're Jane, an interviewer calling a person to collect some data about them.\"\n  stages:\n    ask_for_name:\n      goal: ask the user for their name.\n      outcomes:\n        name_given:\n          description: user tells you their name\n          move_to: ask_for_age\n        user_refuses:\n          description: user stated clearly they don't want to tell you their name\n          move_to: bye\n\n    ask_for_age:\n      goal: ask the user for their age.\n      outcomes:\n        age_given:\n          description: user tells you their age\n          move_to: ask_for_city\n        user_refuses:\n          description: user stated clearly they don't want to tell you their age\n          move_to: bye\n\n    ask_for_address:\n      goal: ask the user for their current address.\n      outcomes:\n        address_given:\n          description: user tells you their current address and the address exists.\n          move_to: confirm_data\n        user_refuses:\n          description: user stated clearly they don't want to tell you their current address\n          move_to: bye\n      tools:\n        - address_exists\n\n    confirm_data:\n      goal: Repeat the user's name, age and address you have gathered and ask them to confirm that everything is correct.\n      outcomes:\n        user_confirms:\n          description: user confirms everything you said is correct.\n          move_to: bye\n        user_refuses:\n          description: user stated clearly they won't say if the data you gathered is correct or not.\n          move_to: bye\n        mistakes_found:\n          description: user states that there are mistakes in the data you repeated.\n          move_to: amend_data\n\n    amend_data:\n      goal: ask the user which information they want to correct and ask them to provide the correct information. If they already provided it while asking you to correct said data, repeat it to them and ask them to confirm it.\n      outcomes:\n        correct_data_given:\n          description: user provides the correct information\n          move_to: confirm\n        user_refuses:\n          description: user stated clearly they won't confirm the data is now correct or not.\n          move_to: bye\n\n    bye:\n      goal: thank the user for their time and say goodbye to them.\n      outcomes:\n        ok:\n          description: user says goodbye too\n          move_to: _end_\n\n    questions:\n      accessible_from: _all_\n      description: the user asks you something about yourself, your task, or why you're calling them and collecting this data about them.\n      goal: answer all of the user's question regarding yourself, your task, and why you're calling them and collecting this data about them. After you answer, always ask the user if they have any more question for you.\n      outcomes:\n        no_more_questions:\n          description: the user has no more questions.\n          move_to: _backtrack_\n      tools:\n       - knowledge_base\n</code></pre>"},{"location":"docs/config-file/#bot-configuration","title":"Bot configuration","text":"<pre><code>interface: textual_ui\nmodality: text_messages\nbot:\n  type: text_chat\n  llm:\n    client: openai\n    name: gpt-4o\n</code></pre> <p>Intentional supports several styles of bots, so the configuration file must first of all specify what sort of bot we're building. The <code>bot</code> section and a few other related fields take care of this definition.</p>"},{"location":"docs/config-file/#interface","title":"Interface","text":"<p><code>interface</code> makes you configure the user interface the bot will use to communicate. If you want the bot to show its replies in the commend line, use <code>interface: terminal</code>. Do you prefer to use a chat application? Intentional can spin up a Telegram bot for you if you specify <code>interface: telegram</code>. Need a FastAPI endpoint? <code>interface: fastapi</code>. And so on.</p> <p>Note</p> <p>Interfaces are always provided by a plugin: <code>intentional</code> will install the <code>intentional-terminal</code> plugin to help you get started, but <code>intentional-core</code> comes with no interfaces by default. Make sure to install the plugins you need for your interface to work.</p> <p>You can find a list of available plugins in the API Reference sidebar. Better documentation of the available plugins is coming soon.</p>"},{"location":"docs/config-file/#modality","title":"Modality","text":"<p>Last, let's specify the <code>modality</code>. The modality is the medium the bot uses to communicate with the user, such as text messages, audio messages, audio stream, even video stream (not supported yet).</p> <p>Some bot interfaces support more than one modality, so we need to specify what our bot is supposed to use as its primary modality.</p> <p>Right now, most bots support either one of these modalities:</p> <ul> <li><code>text_messages</code>: classic chat-style messages.</li> <li><code>audio_stream</code>: telephone-like interaction where bot and user freely talk together.</li> </ul>"},{"location":"docs/config-file/#bot-type","title":"Bot type","text":"<p>First, we need to specify the <code>type</code>, which defines the implementation style of the bot, any intermediate steps that need to be done to make the user's input understandable to the LLM. Right now Intentional supports a few types of bots:</p> <ul> <li><code>direct_to_llm</code>: the LLM is able to handle directly the messages of the user. For example, if the user is using text, the LLM is able to read the messages as they are. If the user is talking, the LLM is able to understand their voice without transcription.</li> </ul> <p>Note</p> <p>More documentation on the <code>type</code> field coming soon!</p>"},{"location":"docs/config-file/#llm","title":"LLM","text":"<p>Next, we need to specify what LLM we want to use. The <code>llm</code> field takes two parameters:</p> <ul> <li> <p><code>client</code>: which client to use to connect to the LLM. For example, <code>openai</code> (provided by the <code>intentional-openai</code> plugin, see below) will tell Intentional to use the OpenAI SDK to connect to the LLM.</p> </li> <li> <p><code>name</code>: the name of the LLM (if required by the specified client). In this case, we specify <code>gpt-4o</code>.</p> </li> </ul> <p>If the client you specified requires any other parameters, they can be listed in this section.</p>"},{"location":"docs/config-file/#plugins","title":"Plugins","text":"<pre><code>plugins:\n- intentional_textual_ui\n- intentional_openai\n</code></pre> <p>Intentional is highly modular, and some of the parameters highlighted above require their own plugins. You can specify which plugins your bot needs by listing them in this section.</p> <p>In our example we're using OpenAI as the LLM provider and Textual UI as our UI. Therefore we can list these two plugins to make sure Intentional loads them properly.</p> <p>Note</p> <p>If this section is not specified, Intentional will search your virtual environment for any package that begins with <code>intentional_</code> and will try to import it. In many cases this may be sufficient.</p>"},{"location":"docs/config-file/#conversation","title":"Conversation","text":"<pre><code>conversation:\n  background: \"You're Jane, an interviewer calling a person to collect some data about them.\"\n  stages:\n</code></pre> <p>The conversation block is where we define all the stages and the transitions among them. Together with the <code>stages</code> field, which is where all the definition of the stages will go, you can also define a <code>background</code> field that includes some basic information about the bot's personality and overall goal.</p> <p>Note</p> <p>Avoid using the <code>background</code> field to describe the bot's goal in a specific way. Giving the bot too much information about its goals will make it more likely to hallucinate questions and improvise transitions between stages!</p> <p>You should use the <code>background</code> field to specify the bot's personality, a few very basic information about itself, and the context the bot will be in (such as whether they're calling over the phone, having a text chat, etc)</p>"},{"location":"docs/config-file/#stages","title":"Stages","text":"<pre><code>ask_for_address:\n    goal: ask the user for their current address.\n    outcomes:\n        address_given:\n            description: user tells you their current address and the address exists.\n            move_to: confirm_data\n        user_refuses:\n            description: user stated clearly they don't want to tell you their current address\n            move_to: bye\n        tools:\n        - address_exists\n</code></pre> <p>This is the basic structure of a stage.</p> <p>Stages have a name, such as <code>ask_for_address</code>, which will be shown in the diagram of a conversation such as the ones above.</p> <p>Stages have a <code>goal</code> field: here you should describe the goal the bot must pursue while it's in this stage. For example, when the bot is in the <code>ask_for_address</code> stage, its goal is to <code>ask the user for their current address</code>. This makes sure that the bot does what you expect it to.</p> <p>Stages have <code>outcomes</code>: each of these outcomes is named, such as <code>address_given</code>. Each outcome has two properties:</p> <ul> <li> <p>a <code>description</code>, like <code>user tells you their current address and the address exists.</code>, that the bot will use to check whether this outcome has been accomplished or not.</p> </li> <li> <p>a <code>move_to</code> field, that points to the next stage the bot should move to once this outcome is reached. For example, if the <code>address_given</code> outcome has been reached, the bot should move on to <code>confirm_data</code>.</p> </li> </ul> <p>Stages also have a list of <code>tools</code> that they should have access to. For example, <code>ask_for_address</code> needs access to the <code>address_exists</code> tool. The tool itself will contain all the information needed for the bot to use it, but if further configuration is required, it can be listed under the tool as well.</p> <p>Note</p> <p>Listing the tools by name is sufficient for the bot to find them, but they must have been imported by the Python process running your bot in order to be found. See more in the Tools section of the docs.</p> <p>There are a few more fields that may be used to define stages outside of the main conversation flow, such as the <code>questions</code> stage:</p> <pre><code>    questions:\n      accessible_from: _all_\n      description: the user asks you something about yourself, your task, or why you're calling them and collecting this data about them.\n      goal: answer all of the user's question regarding yourself, your task, and why you're calling them and collecting this data about them. After you answer, always ask the user if they have any more question for you.\n      outcomes:\n        no_more_questions:\n          description: the user has no more questions.\n          move_to: _backtrack_\n      tools:\n       - knowledge_base\n</code></pre> <p>This stage has two additional fields:</p> <ul> <li> <p><code>accessible_from</code> lets you specify from which stages the bot should be able to jump here in case of need. It can take a few values:</p> <ul> <li><code>_all_</code> means that this stage is accessible by all other stages.</li> <li>a list of stages: in this case, the bot will only be able to jump here from one of the stages listed and nowhere else.</li> </ul> </li> <li> <p><code>description</code>: much like the <code>description</code> field of outcomes, this field describes when the bot should leave the stage it find itself in and jump here instead.</p> </li> </ul> <p>Outcomes as well are different in this stage. The <code>move_to</code> field is set to <code>_backtrack_</code>, which tells the bot that once this outcome is reached, the bot should jump back to whatever stage it was in before landing here. For example, if the user asked the question \"Why do you need my address\"? in the <code>ask_for_address</code> stage, once the bot replied and the user is happy with its response, the <code>_backtrack_</code> field tells the bot to jump back to where it was before, which is <code>ask_for_address</code>.</p>"},{"location":"docs/core-reference/","title":"API Reference - Core Package","text":""},{"location":"docs/core-reference/#intentional-core.src","title":"<code>src</code>","text":""},{"location":"docs/core-reference/#intentional-core.src.intentional_core","title":"<code>intentional_core</code>","text":"<p>Init file for <code>intentional_core</code>.</p>"},{"location":"docs/core-reference/#intentional-core.src.intentional_core.__about__","title":"<code>__about__</code>","text":"<p>Package descriptors for intentional-core.</p>"},{"location":"docs/core-reference/#intentional-core.src.intentional_core.bot_interface","title":"<code>bot_interface</code>","text":"<p>Functions to load bots from config files.</p>"},{"location":"docs/core-reference/#intentional-core.src.intentional_core.bot_interface.BotInterface","title":"<code>BotInterface</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Tiny base class used to recognize Intentional bots interfaces.</p> <p>The class name is meant to represent the communication channel you will use to interact with your bot. For example an interface that uses a local command line interface would be called \"TerminalBotInterface\", one that uses Whatsapp would be called \"WhatsappBotInterface\", one that uses Twilio would be called \"TwilioBotInterface\", etc.</p> <p>In order for your bot to be usable, you need to assign a value to the <code>name</code> class variable in the class definition.</p> Source code in <code>intentional-core/src/intentional_core/bot_interface.py</code> <pre><code>class BotInterface(ABC):\n    \"\"\"\n    Tiny base class used to recognize Intentional bots interfaces.\n\n    The class name is meant to represent the **communication channel** you will use to interact with your bot.\n    For example an interface that uses a local command line interface would be called \"TerminalBotInterface\", one that\n    uses Whatsapp would be called \"WhatsappBotInterface\", one that uses Twilio would be called \"TwilioBotInterface\",\n    etc.\n\n    In order for your bot to be usable, you need to assign a value to the `name` class variable in the class definition.\n    \"\"\"\n\n    name: Optional[str] = None\n    \"\"\"\n    The name of the bot interface. This should be a unique identifier for the bot interface.\n    This string will be used in configuration files to identify the bot interface.\n\n    The bot interface name should directly recall the class name as much as possible.\n    For example, the name of \"LocalBotInterface\" should be \"local\", the name of \"WhatsappBotInterface\" should be\n    \"whatsapp\", etc.\n    \"\"\"\n\n    @abstractmethod\n    async def run(self):\n        \"\"\"\n        Run the bot interface.\n\n        This method should be overridden by the subclass to implement the bot's main loop.\n        \"\"\"\n        raise NotImplementedError(\"BotInterface subclasses must implement the run method.\")\n</code></pre>"},{"location":"docs/core-reference/#intentional-core.src.intentional_core.bot_interface.BotInterface.name","title":"<code>name = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The name of the bot interface. This should be a unique identifier for the bot interface. This string will be used in configuration files to identify the bot interface.</p> <p>The bot interface name should directly recall the class name as much as possible. For example, the name of \"LocalBotInterface\" should be \"local\", the name of \"WhatsappBotInterface\" should be \"whatsapp\", etc.</p>"},{"location":"docs/core-reference/#intentional-core.src.intentional_core.bot_interface.BotInterface.run","title":"<code>run()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Run the bot interface.</p> <p>This method should be overridden by the subclass to implement the bot's main loop.</p> Source code in <code>intentional-core/src/intentional_core/bot_interface.py</code> <pre><code>@abstractmethod\nasync def run(self):\n    \"\"\"\n    Run the bot interface.\n\n    This method should be overridden by the subclass to implement the bot's main loop.\n    \"\"\"\n    raise NotImplementedError(\"BotInterface subclasses must implement the run method.\")\n</code></pre>"},{"location":"docs/core-reference/#intentional-core.src.intentional_core.bot_interface.load_bot_interface_from_dict","title":"<code>load_bot_interface_from_dict(config)</code>","text":"<p>Load a bot interface, and all its inner classes, from a dictionary configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Dict[str, Any]</code> <p>The configuration dictionary.</p> required <p>Returns:</p> Type Description <code>BotInterface</code> <p>The bot interface instance.</p> Source code in <code>intentional-core/src/intentional_core/bot_interface.py</code> <pre><code>def load_bot_interface_from_dict(config: Dict[str, Any]) -&gt; BotInterface:\n    \"\"\"\n    Load a bot interface, and all its inner classes, from a dictionary configuration.\n\n    Args:\n        config: The configuration dictionary.\n\n    Returns:\n        The bot interface instance.\n    \"\"\"\n    log.debug(\n        \"Loading bot interface from configuration:\",\n        bot_interface_config=json.dumps(config, indent=4),\n    )\n\n    # Import all the necessary plugins\n    plugins = config.pop(\"plugins\", None)\n    if plugins:\n        log.debug(\"Found plugins to import\", plugins=plugins)\n        for plugin in plugins:\n            log.debug(\"Importing plugin\", plugin=plugin)\n            import_plugin(plugin)\n    else:\n        import_all_plugins()\n\n    # Initialize the intent router\n    log.debug(\"Creating intent router\")\n    intent_router = IntentRouter(config.pop(\"conversation\", {}))\n\n    # Get all the subclasses of Bot\n    subclasses: Set[BotInterface] = inheritors(BotInterface)\n    log.debug(\"Collected bot interface classes\", bot_interfaces=subclasses)\n\n    for subclass in subclasses:\n        if not subclass.name:\n            log.error(\n                \"Bot interface class '%s' does not have a name. This bot type will not be usable.\",\n                subclass,\n                bot_interface_class=subclass,\n            )\n            continue\n\n        if subclass.name in _BOT_INTERFACES:\n            log.warning(\n                \"Duplicate bot interface type '%s' found. The older class will be replaced by the newly imported one.\",\n                subclass.name,\n                old_bot_interface_name=subclass.name,\n                old_bot_interface_class=_BOT_INTERFACES[subclass.name],\n                new_bot_interface_class=subclass,\n            )\n        _BOT_INTERFACES[subclass.name] = subclass\n\n    # Identify the type of bot interface and see if it's known\n    interface_class = config.pop(\"interface\", None)\n    if not interface_class:\n        raise ValueError(\"Bot configuration must contain an 'interface' key to know which interface to use.\")\n\n    if interface_class not in _BOT_INTERFACES:\n        raise ValueError(\n            f\"Unknown bot interface type '{interface_class}'. Available types: {list(_BOT_INTERFACES)}. \"\n            \"Did you forget to add the correct plugin name in the configuration file, or to install it?\"\n        )\n\n    # Handoff to the subclass' init\n    log.debug(\"Creating bot interface\", bot_interface_class=interface_class)\n    return _BOT_INTERFACES[interface_class](intent_router=intent_router, config=config)\n</code></pre>"},{"location":"docs/core-reference/#intentional-core.src.intentional_core.bot_interface.load_configuration_file","title":"<code>load_configuration_file(path)</code>","text":"<p>Load an Intentional bot from a YAML configuration file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to the YAML configuration file.</p> required <p>Returns:</p> Type Description <code>BotInterface</code> <p>The bot instance.</p> Source code in <code>intentional-core/src/intentional_core/bot_interface.py</code> <pre><code>def load_configuration_file(path: Path) -&gt; BotInterface:\n    \"\"\"\n    Load an Intentional bot from a YAML configuration file.\n\n    Args:\n        path: Path to the YAML configuration file.\n\n    Returns:\n        The bot instance.\n    \"\"\"\n    log.debug(\"Loading YAML configuration file\", config_file_path=path)\n    with open(path, \"r\", encoding=\"utf-8\") as file:\n        config = yaml.safe_load(file)\n    return load_bot_interface_from_dict(config)\n</code></pre>"},{"location":"docs/core-reference/#intentional-core.src.intentional_core.bot_structures","title":"<code>bot_structures</code>","text":"<p>Bot structures supported by Intentional.</p>"},{"location":"docs/core-reference/#intentional-core.src.intentional_core.bot_structures.bot_structure","title":"<code>bot_structure</code>","text":"<p>Functions to load bot structure classes from config files.</p>"},{"location":"docs/core-reference/#intentional-core.src.intentional_core.bot_structures.bot_structure.BotStructure","title":"<code>BotStructure</code>","text":"<p>               Bases: <code>EventListener</code></p> <p>Tiny base class used to recognize Intentional bot structure classes.</p> <p>The bot structure's name is meant to represent the structure of the bot. For example a bot that uses a direct WebSocket connection to a LLM such as OpenAI's Realtime API could be called \"RealtimeAPIBotStructure\", one that uses a VAD-STT-LLM-TTS stack could be called \"AudioToTextBotStructure\", and so on</p> <p>In order for your bot structure to be usable, you need to assign a value to the <code>name</code> class variable in the bot structure class' definition.</p> Source code in <code>intentional-core/src/intentional_core/bot_structures/bot_structure.py</code> <pre><code>class BotStructure(EventListener):\n    \"\"\"\n    Tiny base class used to recognize Intentional bot structure classes.\n\n    The bot structure's name is meant to represent the **structure** of the bot. For example a bot that uses a direct\n    WebSocket connection to a LLM such as OpenAI's Realtime API could be called \"RealtimeAPIBotStructure\", one that\n    uses a VAD-STT-LLM-TTS stack could be called \"AudioToTextBotStructure\", and so on\n\n    In order for your bot structure to be usable, you need to assign a value to the `name` class variable in the bot\n    structure class' definition.\n    \"\"\"\n\n    name: Optional[str] = None\n    \"\"\"\n    The name of this bot's structure. This should be a unique identifier for the bot structure type.\n\n    The bot structure's name should directly recall the class name as much as possible. For example, the name of\n    \"WebsocketBotStructure\" should be \"websocket\", the name of \"AudioToTextBotStructure\" should be \"audio_to_text\",\n    etc.\n    \"\"\"\n\n    async def connect(self) -&gt; None:\n        \"\"\"\n        Connect to the bot.\n        \"\"\"\n\n    async def disconnect(self) -&gt; None:\n        \"\"\"\n        Disconnect from the bot.\n        \"\"\"\n\n    @abstractmethod\n    async def run(self) -&gt; None:\n        \"\"\"\n        Main loop for the bot.\n        \"\"\"\n\n    @abstractmethod\n    async def send(self, data: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"\n        Send a message to the bot.\n        \"\"\"\n\n    @abstractmethod\n    async def handle_interruption(self, lenght_to_interruption: int) -&gt; None:\n        \"\"\"\n        Handle an interruption in the streaming.\n\n        Args:\n            lenght_to_interruption: The length of the data that was produced to the user before the interruption.\n                This value could be number of characters, number of words, milliseconds, number of audio frames, etc.\n                depending on the bot structure that implements it.\n        \"\"\"\n</code></pre> <code>name = None</code> <code>class-attribute</code> <code>instance-attribute</code> # <p>The name of this bot's structure. This should be a unique identifier for the bot structure type.</p> <p>The bot structure's name should directly recall the class name as much as possible. For example, the name of \"WebsocketBotStructure\" should be \"websocket\", the name of \"AudioToTextBotStructure\" should be \"audio_to_text\", etc.</p> <code>connect()</code> <code>async</code> # <p>Connect to the bot.</p> Source code in <code>intentional-core/src/intentional_core/bot_structures/bot_structure.py</code> <pre><code>async def connect(self) -&gt; None:\n    \"\"\"\n    Connect to the bot.\n    \"\"\"\n</code></pre> <code>disconnect()</code> <code>async</code> # <p>Disconnect from the bot.</p> Source code in <code>intentional-core/src/intentional_core/bot_structures/bot_structure.py</code> <pre><code>async def disconnect(self) -&gt; None:\n    \"\"\"\n    Disconnect from the bot.\n    \"\"\"\n</code></pre> <code>handle_interruption(lenght_to_interruption)</code> <code>abstractmethod</code> <code>async</code> # <p>Handle an interruption in the streaming.</p> <p>Parameters:</p> Name Type Description Default <code>lenght_to_interruption</code> <code>int</code> <p>The length of the data that was produced to the user before the interruption. This value could be number of characters, number of words, milliseconds, number of audio frames, etc. depending on the bot structure that implements it.</p> required Source code in <code>intentional-core/src/intentional_core/bot_structures/bot_structure.py</code> <pre><code>@abstractmethod\nasync def handle_interruption(self, lenght_to_interruption: int) -&gt; None:\n    \"\"\"\n    Handle an interruption in the streaming.\n\n    Args:\n        lenght_to_interruption: The length of the data that was produced to the user before the interruption.\n            This value could be number of characters, number of words, milliseconds, number of audio frames, etc.\n            depending on the bot structure that implements it.\n    \"\"\"\n</code></pre> <code>run()</code> <code>abstractmethod</code> <code>async</code> # <p>Main loop for the bot.</p> Source code in <code>intentional-core/src/intentional_core/bot_structures/bot_structure.py</code> <pre><code>@abstractmethod\nasync def run(self) -&gt; None:\n    \"\"\"\n    Main loop for the bot.\n    \"\"\"\n</code></pre> <code>send(data)</code> <code>abstractmethod</code> <code>async</code> # <p>Send a message to the bot.</p> Source code in <code>intentional-core/src/intentional_core/bot_structures/bot_structure.py</code> <pre><code>@abstractmethod\nasync def send(self, data: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"\n    Send a message to the bot.\n    \"\"\"\n</code></pre>"},{"location":"docs/core-reference/#intentional-core.src.intentional_core.bot_structures.bot_structure.load_bot_structure_from_dict","title":"<code>load_bot_structure_from_dict(intent_router, config)</code>","text":"<p>Load a bot structure from a dictionary configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Dict[str, Any]</code> <p>The configuration dictionary.</p> required <p>Returns:</p> Type Description <code>BotStructure</code> <p>The BotStructure instance.</p> Source code in <code>intentional-core/src/intentional_core/bot_structures/bot_structure.py</code> <pre><code>def load_bot_structure_from_dict(intent_router: IntentRouter, config: Dict[str, Any]) -&gt; BotStructure:\n    \"\"\"\n    Load a bot structure from a dictionary configuration.\n\n    Args:\n        config: The configuration dictionary.\n\n    Returns:\n        The BotStructure instance.\n    \"\"\"\n    # Get all the subclasses of Bot\n    subclasses: Set[BotStructure] = inheritors(BotStructure)\n    log.debug(\"Collected bot structure classes\", bot_structure_classes=subclasses)\n    for subclass in subclasses:\n        if not subclass.name:\n            log.error(\n                \"BotStructure class '%s' does not have a name. This bot structure type will not be usable.\",\n                subclass,\n                bot_structure_class=subclass,\n            )\n            continue\n\n        if subclass.name in _BOT_STRUCTURES:\n            log.warning(\n                \"Duplicate bot structure type '%s' found. The older class will be replaced by the newly imported one.\",\n                subclass.name,\n                old_bot_structure_name=subclass.name,\n                old_bot_structure_class=_BOT_STRUCTURES[subclass.name],\n                new_bot_structure_class=subclass,\n            )\n        _BOT_STRUCTURES[subclass.name] = subclass\n\n    # Identify the type of bot and see if it's known\n    bot_structure_class = config.pop(\"type\")\n    log.debug(\"Creating bot structure\", bot_structure_class=bot_structure_class)\n    if bot_structure_class not in _BOT_STRUCTURES:\n        raise ValueError(\n            f\"Unknown bot structure type '{bot_structure_class}'. Available types: {list(_BOT_STRUCTURES)}. \"\n            \"Did you forget to install your plugin?\"\n        )\n\n    # Handoff to the subclass' init\n    return _BOT_STRUCTURES[bot_structure_class](config, intent_router)\n</code></pre>"},{"location":"docs/core-reference/#intentional-core.src.intentional_core.bot_structures.direct_to_llm","title":"<code>direct_to_llm</code>","text":"<p>Bot structure to support text chat for Intentional.</p>"},{"location":"docs/core-reference/#intentional-core.src.intentional_core.bot_structures.direct_to_llm.DirectToLLMBotStructure","title":"<code>DirectToLLMBotStructure</code>","text":"<p>               Bases: <code>BotStructure</code></p> <p>Bot structure implementation for text chat.</p> Source code in <code>intentional-core/src/intentional_core/bot_structures/direct_to_llm.py</code> <pre><code>class DirectToLLMBotStructure(BotStructure):\n    \"\"\"\n    Bot structure implementation for text chat.\n    \"\"\"\n\n    name = \"direct_to_llm\"\n\n    def __init__(self, config: Dict[str, Any], intent_router: IntentRouter):\n        \"\"\"\n        Args:\n            config:\n                The configuration dictionary for the bot structure.\n        \"\"\"\n        super().__init__()\n        log.debug(\"Loading bot structure from config\", bot_structure_config=config)\n\n        # Init the model client\n        llm_config = config.pop(\"llm\", None)\n        if not llm_config:\n            raise ValueError(f\"{self.__class__.__name__} requires a 'llm' configuration key.\")\n        self.llm: LLMClient = load_llm_client_from_dict(parent=self, intent_router=intent_router, config=llm_config)\n\n    async def connect(self) -&gt; None:\n        \"\"\"\n        Initializes the model and connects to it as/if necessary.\n        \"\"\"\n        await self.llm.connect()\n\n    async def disconnect(self) -&gt; None:\n        \"\"\"\n        Disconnects from the model and unloads/closes it as/if necessary.\n        \"\"\"\n        await self.llm.disconnect()\n\n    async def run(self) -&gt; None:\n        \"\"\"\n        Main loop for the bot.\n        \"\"\"\n        await self.llm.run()\n\n    async def send(self, data: Dict[str, Any]) -&gt; AsyncGenerator[Dict[str, Any], None]:\n        \"\"\"\n        Sends a message to the model and forward the response.\n\n        Args:\n            data: The message to send to the model in OpenAI format, like {\"role\": \"user\", \"content\": \"Hello!\"}\n        \"\"\"\n        await self.llm.send(data)\n\n    async def handle_interruption(self, lenght_to_interruption: int) -&gt; None:\n        \"\"\"\n        Handle an interruption in the streaming.\n\n        Args:\n            lenght_to_interruption: The length of the data that was produced to the user before the interruption.\n                This value could be number of characters, number of words, milliseconds, number of audio frames, etc.\n                depending on the bot structure that implements it.\n        \"\"\"\n        await self.llm.handle_interruption(lenght_to_interruption)\n</code></pre> <code>__init__(config, intent_router)</code> # <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Dict[str, Any]</code> <p>The configuration dictionary for the bot structure.</p> required Source code in <code>intentional-core/src/intentional_core/bot_structures/direct_to_llm.py</code> <pre><code>def __init__(self, config: Dict[str, Any], intent_router: IntentRouter):\n    \"\"\"\n    Args:\n        config:\n            The configuration dictionary for the bot structure.\n    \"\"\"\n    super().__init__()\n    log.debug(\"Loading bot structure from config\", bot_structure_config=config)\n\n    # Init the model client\n    llm_config = config.pop(\"llm\", None)\n    if not llm_config:\n        raise ValueError(f\"{self.__class__.__name__} requires a 'llm' configuration key.\")\n    self.llm: LLMClient = load_llm_client_from_dict(parent=self, intent_router=intent_router, config=llm_config)\n</code></pre> <code>connect()</code> <code>async</code> # <p>Initializes the model and connects to it as/if necessary.</p> Source code in <code>intentional-core/src/intentional_core/bot_structures/direct_to_llm.py</code> <pre><code>async def connect(self) -&gt; None:\n    \"\"\"\n    Initializes the model and connects to it as/if necessary.\n    \"\"\"\n    await self.llm.connect()\n</code></pre> <code>disconnect()</code> <code>async</code> # <p>Disconnects from the model and unloads/closes it as/if necessary.</p> Source code in <code>intentional-core/src/intentional_core/bot_structures/direct_to_llm.py</code> <pre><code>async def disconnect(self) -&gt; None:\n    \"\"\"\n    Disconnects from the model and unloads/closes it as/if necessary.\n    \"\"\"\n    await self.llm.disconnect()\n</code></pre> <code>handle_interruption(lenght_to_interruption)</code> <code>async</code> # <p>Handle an interruption in the streaming.</p> <p>Parameters:</p> Name Type Description Default <code>lenght_to_interruption</code> <code>int</code> <p>The length of the data that was produced to the user before the interruption. This value could be number of characters, number of words, milliseconds, number of audio frames, etc. depending on the bot structure that implements it.</p> required Source code in <code>intentional-core/src/intentional_core/bot_structures/direct_to_llm.py</code> <pre><code>async def handle_interruption(self, lenght_to_interruption: int) -&gt; None:\n    \"\"\"\n    Handle an interruption in the streaming.\n\n    Args:\n        lenght_to_interruption: The length of the data that was produced to the user before the interruption.\n            This value could be number of characters, number of words, milliseconds, number of audio frames, etc.\n            depending on the bot structure that implements it.\n    \"\"\"\n    await self.llm.handle_interruption(lenght_to_interruption)\n</code></pre> <code>run()</code> <code>async</code> # <p>Main loop for the bot.</p> Source code in <code>intentional-core/src/intentional_core/bot_structures/direct_to_llm.py</code> <pre><code>async def run(self) -&gt; None:\n    \"\"\"\n    Main loop for the bot.\n    \"\"\"\n    await self.llm.run()\n</code></pre> <code>send(data)</code> <code>async</code> # <p>Sends a message to the model and forward the response.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Any]</code> <p>The message to send to the model in OpenAI format, like {\"role\": \"user\", \"content\": \"Hello!\"}</p> required Source code in <code>intentional-core/src/intentional_core/bot_structures/direct_to_llm.py</code> <pre><code>async def send(self, data: Dict[str, Any]) -&gt; AsyncGenerator[Dict[str, Any], None]:\n    \"\"\"\n    Sends a message to the model and forward the response.\n\n    Args:\n        data: The message to send to the model in OpenAI format, like {\"role\": \"user\", \"content\": \"Hello!\"}\n    \"\"\"\n    await self.llm.send(data)\n</code></pre>"},{"location":"docs/core-reference/#intentional-core.src.intentional_core.end_conversation","title":"<code>end_conversation</code>","text":"<p>Tool that ends and resets the conversation. Once a conversation reaches this stage, the bot should restart from the start stage. See IntentRouter.</p>"},{"location":"docs/core-reference/#intentional-core.src.intentional_core.end_conversation.EndConversationTool","title":"<code>EndConversationTool</code>","text":"<p>               Bases: <code>Tool</code></p> <p>Tool to end the conversation. Resets the intent router to its initial stage.</p> Source code in <code>intentional-core/src/intentional_core/end_conversation.py</code> <pre><code>class EndConversationTool(Tool):\n    \"\"\"\n    Tool to end the conversation. Resets the intent router to its initial stage.\n    \"\"\"\n\n    id = \"end_conversation\"\n    name = \"end_conversation\"\n    description = \"End the conversation.\"\n    parameters = []\n\n    def __init__(self, intent_router: \"IntentRouter\"):\n        self.router = intent_router\n\n    async def run(self, params=None) -&gt; str:\n        \"\"\"\n        Ends the conversation and resets the intent router to its initial stage.\n        \"\"\"\n        log.debug(\"The conversation has ended.\")\n        self.router.current_stage_name = self.router.initial_stage\n        log.debug(\n            \"Intent router reset to initial stage.\",\n            initial_stage=self.router.initial_stage,\n        )\n</code></pre>"},{"location":"docs/core-reference/#intentional-core.src.intentional_core.end_conversation.EndConversationTool.run","title":"<code>run(params=None)</code>  <code>async</code>","text":"<p>Ends the conversation and resets the intent router to its initial stage.</p> Source code in <code>intentional-core/src/intentional_core/end_conversation.py</code> <pre><code>async def run(self, params=None) -&gt; str:\n    \"\"\"\n    Ends the conversation and resets the intent router to its initial stage.\n    \"\"\"\n    log.debug(\"The conversation has ended.\")\n    self.router.current_stage_name = self.router.initial_stage\n    log.debug(\n        \"Intent router reset to initial stage.\",\n        initial_stage=self.router.initial_stage,\n    )\n</code></pre>"},{"location":"docs/core-reference/#intentional-core.src.intentional_core.events","title":"<code>events</code>","text":"<p>Base class for very simplified event emitter and listener.</p>"},{"location":"docs/core-reference/#intentional-core.src.intentional_core.events.EventEmitter","title":"<code>EventEmitter</code>","text":"<p>Sends any event to the listener. TODO see if there's any scenario where we need more as this pattern is easy to extend but can get messy.</p> Source code in <code>intentional-core/src/intentional_core/events.py</code> <pre><code>class EventEmitter:\n    \"\"\"\n    Sends any event to the listener.\n    TODO see if there's any scenario where we need more as this pattern is easy to extend but can get messy.\n    \"\"\"\n\n    def __init__(self, listener: EventListener):\n        \"\"\"\n        Register the listener.\n        \"\"\"\n        self._events_listener = listener\n\n    async def emit(self, event_name: str, event: Dict[str, Any]):\n        \"\"\"\n        Send the event to the listener.\n        \"\"\"\n        log.debug(\"Emitting event\", event_name=event_name)\n        await self._events_listener.handle_event(event_name, event)\n</code></pre>"},{"location":"docs/core-reference/#intentional-core.src.intentional_core.events.EventEmitter.__init__","title":"<code>__init__(listener)</code>","text":"<p>Register the listener.</p> Source code in <code>intentional-core/src/intentional_core/events.py</code> <pre><code>def __init__(self, listener: EventListener):\n    \"\"\"\n    Register the listener.\n    \"\"\"\n    self._events_listener = listener\n</code></pre>"},{"location":"docs/core-reference/#intentional-core.src.intentional_core.events.EventEmitter.emit","title":"<code>emit(event_name, event)</code>  <code>async</code>","text":"<p>Send the event to the listener.</p> Source code in <code>intentional-core/src/intentional_core/events.py</code> <pre><code>async def emit(self, event_name: str, event: Dict[str, Any]):\n    \"\"\"\n    Send the event to the listener.\n    \"\"\"\n    log.debug(\"Emitting event\", event_name=event_name)\n    await self._events_listener.handle_event(event_name, event)\n</code></pre>"},{"location":"docs/core-reference/#intentional-core.src.intentional_core.events.EventListener","title":"<code>EventListener</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Listens to events and handles them.</p> Source code in <code>intentional-core/src/intentional_core/events.py</code> <pre><code>class EventListener(ABC):\n    \"\"\"\n    Listens to events and handles them.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"\n        Initialize the bot structure.\n        \"\"\"\n        self.event_handlers: Dict[str, Callable] = {}\n\n    def add_event_handler(self, event_name: str, handler: Callable) -&gt; None:\n        \"\"\"\n        Add an event handler for a specific event type.\n\n        Args:\n            event_name: The name of the event to handle.\n            handler: The handler function to call when the event is received.\n        \"\"\"\n        if event_name in self.event_handlers:\n            log.debug(\n                \"Event handler for '%s' was already assigned. The older handler will be replaced by the new one.\",\n                event_name,\n                event_name=event_name,\n                event_handler=self.event_handlers[event_name],\n            )\n        log.debug(\"Adding event handler\", event_name=event_name, event_handler=handler)\n        self.event_handlers[event_name] = handler\n\n    async def handle_event(self, event_name: str, event: Dict[str, Any]) -&gt; None:\n        \"\"\"\n        Handle different types of events that the LLM may generate.\n        \"\"\"\n        if \"*\" in self.event_handlers:\n            log.debug(\"Calling wildcard event handler\", event_name=event_name)\n            await self.event_handlers[\"*\"](event)\n\n        if event_name in self.event_handlers:\n            log.debug(\"Calling event handler\", event_name=event_name)\n            await self.event_handlers[event_name](event)\n        else:\n            log.debug(\"No event handler for event\", event_name=event_name)\n</code></pre>"},{"location":"docs/core-reference/#intentional-core.src.intentional_core.events.EventListener.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the bot structure.</p> Source code in <code>intentional-core/src/intentional_core/events.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"\n    Initialize the bot structure.\n    \"\"\"\n    self.event_handlers: Dict[str, Callable] = {}\n</code></pre>"},{"location":"docs/core-reference/#intentional-core.src.intentional_core.events.EventListener.add_event_handler","title":"<code>add_event_handler(event_name, handler)</code>","text":"<p>Add an event handler for a specific event type.</p> <p>Parameters:</p> Name Type Description Default <code>event_name</code> <code>str</code> <p>The name of the event to handle.</p> required <code>handler</code> <code>Callable</code> <p>The handler function to call when the event is received.</p> required Source code in <code>intentional-core/src/intentional_core/events.py</code> <pre><code>def add_event_handler(self, event_name: str, handler: Callable) -&gt; None:\n    \"\"\"\n    Add an event handler for a specific event type.\n\n    Args:\n        event_name: The name of the event to handle.\n        handler: The handler function to call when the event is received.\n    \"\"\"\n    if event_name in self.event_handlers:\n        log.debug(\n            \"Event handler for '%s' was already assigned. The older handler will be replaced by the new one.\",\n            event_name,\n            event_name=event_name,\n            event_handler=self.event_handlers[event_name],\n        )\n    log.debug(\"Adding event handler\", event_name=event_name, event_handler=handler)\n    self.event_handlers[event_name] = handler\n</code></pre>"},{"location":"docs/core-reference/#intentional-core.src.intentional_core.events.EventListener.handle_event","title":"<code>handle_event(event_name, event)</code>  <code>async</code>","text":"<p>Handle different types of events that the LLM may generate.</p> Source code in <code>intentional-core/src/intentional_core/events.py</code> <pre><code>async def handle_event(self, event_name: str, event: Dict[str, Any]) -&gt; None:\n    \"\"\"\n    Handle different types of events that the LLM may generate.\n    \"\"\"\n    if \"*\" in self.event_handlers:\n        log.debug(\"Calling wildcard event handler\", event_name=event_name)\n        await self.event_handlers[\"*\"](event)\n\n    if event_name in self.event_handlers:\n        log.debug(\"Calling event handler\", event_name=event_name)\n        await self.event_handlers[event_name](event)\n    else:\n        log.debug(\"No event handler for event\", event_name=event_name)\n</code></pre>"},{"location":"docs/core-reference/#intentional-core.src.intentional_core.intent_routing","title":"<code>intent_routing</code>","text":"<p>Intent routing logic.</p>"},{"location":"docs/core-reference/#intentional-core.src.intentional_core.intent_routing.IntentRouter","title":"<code>IntentRouter</code>","text":"<p>               Bases: <code>Tool</code></p> <p>Special tool used to alter the system prompt depending on the user's response.</p> Source code in <code>intentional-core/src/intentional_core/intent_routing.py</code> <pre><code>class IntentRouter(Tool):\n    \"\"\"\n    Special tool used to alter the system prompt depending on the user's response.\n    \"\"\"\n\n    id = \"classify_response\"\n    name = \"classify_response\"\n    description = \"Classify the user's response for later use.\"\n    parameters = [\n        ToolParameter(\n            \"outcome\",\n            \"The outcome the conversation reached, among the ones described in the prompt.\",\n            \"string\",\n            True,\n            None,\n        ),\n    ]\n\n    def __init__(self, config: Dict[str, Any]) -&gt; None:\n        self.background = config.get(\"background\", \"You're a helpful assistant.\")\n        self.initial_message = config.get(\"initial_message\", None)\n        self.graph = networkx.MultiDiGraph()\n\n        # Init the stages\n        self.stages = {}\n        if \"stages\" not in config or not config[\"stages\"]:\n            raise ValueError(\"The conversation must have at least one stage.\")\n        for name, stage_config in config[\"stages\"].items():\n            log.debug(\"Adding stage\", stage_name=name)\n            self.stages[name] = Stage(name, stage_config)\n            self.stages[name].tools[self.name] = self  # Add the intent router to the tools list of each stage\n            self.graph.add_node(name)\n\n        # Add end stage\n        name = \"_end_\"\n        log.debug(\"Adding stage\", stage_name=name)\n        end_tool = EndConversationTool(intent_router=self)\n        self.stages[name] = Stage(\n            name,\n            {\"custom_template\": f\"The conversation is over. Call the '{end_tool.name}' tool.\"},\n        )\n        self.stages[name].tools[end_tool.name] = end_tool\n        self.graph.add_node(\"_end_\")\n\n        # Connect the stages\n        for name, stage in self.stages.items():\n            for outcome_name, outcome_config in stage.outcomes.items():\n                if outcome_config[\"move_to\"] not in [\n                    *self.stages,\n                    BACKTRACKING_CONNECTION,\n                ]:\n                    raise ValueError(\n                        f\"Stage '{name}' has an outcome leading to an unknown stage '{outcome_config['move_to']}'\"\n                    )\n                log.debug(\n                    \"Adding connection\",\n                    origin=name,\n                    target=outcome_config[\"move_to\"],\n                    outcome=outcome_name,\n                )\n                self.graph.add_edge(name, outcome_config[\"move_to\"], key=outcome_name)\n\n        # Find initial stage\n        self.initial_stage = \"\"\n        for name, stage in self.stages.items():\n            if START_CONNECTION in stage.accessible_from:\n                if self.initial_stage:\n                    raise ValueError(\"Multiple start stages found!\")\n                log.debug(\"Found start stage\", stage_name=name)\n                self.initial_stage = name\n        if not self.initial_stage:\n            raise ValueError(\"No start stage found!\")\n\n        self.current_stage_name = self.initial_stage\n        self.backtracking_stack = []\n\n    @property\n    def current_stage(self):\n        \"\"\"\n        Shorthand to get the current stage instance.\n        \"\"\"\n        return self.stages[self.current_stage_name]\n\n    async def run(self, params: Optional[Dict[str, Any]] = None) -&gt; str:\n        \"\"\"\n        Given the response's classification, returns the new system prompt and the tools accessible in this stage.\n\n        Args:\n            params: The parameters for the tool. Contains the `response_type`.\n\n        Returns:\n            The new system prompt and the tools accessible in this stage.\n        \"\"\"\n        selected_outcome = params[\"outcome\"]\n        transitions = self.get_external_transitions()\n\n        if selected_outcome not in self.current_stage.outcomes and selected_outcome not in transitions:\n            raise ValueError(f\"Unknown outcome '{params['outcome']}' for stage '{self.current_stage_name}'\")\n\n        if selected_outcome in self.current_stage.outcomes:\n            next_stage = self.current_stage.outcomes[params[\"outcome\"]][\"move_to\"]\n\n            if next_stage != BACKTRACKING_CONNECTION:\n                # Direct stage to stage connection\n                self.current_stage_name = next_stage\n            else:\n                # Backtracking connection\n                self.current_stage_name = self.backtracking_stack.pop()\n        else:\n            # Indirect transition, needs to be tracked in the stack\n            self.backtracking_stack.append(self.current_stage_name)\n            self.current_stage_name = selected_outcome\n\n        return self.get_prompt(), self.current_stage.tools\n\n    def get_prompt(self):\n        \"\"\"\n        Get the prompt for the current stage.\n        \"\"\"\n        outcomes = \"You need to reach one of these situations:\\n\" + \"\\n\".join(\n            f\"  - {name}: {data['description']}\" for name, data in self.current_stage.outcomes.items()\n        )\n        transitions = \"\\n\".join(\n            f\"  - {stage}: {self.stages[stage].description}\" for stage in self.get_external_transitions()\n        )\n        template = self.current_stage.custom_template or DEFAULT_PROMPT_TEMPLATE\n        return template.format(\n            intent_router_tool=self.name,\n            stage_name=self.current_stage_name,\n            background=self.background,\n            current_goal=self.current_stage.goal,\n            outcomes=outcomes,\n            transitions=transitions,\n        )\n\n    def get_external_transitions(self):\n        \"\"\"\n        Return a list of all the stages that can be reached from the current stage that are not direct connections.\n        \"\"\"\n        return [\n            name\n            for name, stage in self.stages.items()\n            if (\n                (self.current_stage_name in stage.accessible_from or \"_all_\" in stage.accessible_from)\n                and name != self.current_stage_name\n            )\n        ]\n</code></pre>"},{"location":"docs/core-reference/#intentional-core.src.intentional_core.intent_routing.IntentRouter.current_stage","title":"<code>current_stage</code>  <code>property</code>","text":"<p>Shorthand to get the current stage instance.</p>"},{"location":"docs/core-reference/#intentional-core.src.intentional_core.intent_routing.IntentRouter.get_external_transitions","title":"<code>get_external_transitions()</code>","text":"<p>Return a list of all the stages that can be reached from the current stage that are not direct connections.</p> Source code in <code>intentional-core/src/intentional_core/intent_routing.py</code> <pre><code>def get_external_transitions(self):\n    \"\"\"\n    Return a list of all the stages that can be reached from the current stage that are not direct connections.\n    \"\"\"\n    return [\n        name\n        for name, stage in self.stages.items()\n        if (\n            (self.current_stage_name in stage.accessible_from or \"_all_\" in stage.accessible_from)\n            and name != self.current_stage_name\n        )\n    ]\n</code></pre>"},{"location":"docs/core-reference/#intentional-core.src.intentional_core.intent_routing.IntentRouter.get_prompt","title":"<code>get_prompt()</code>","text":"<p>Get the prompt for the current stage.</p> Source code in <code>intentional-core/src/intentional_core/intent_routing.py</code> <pre><code>def get_prompt(self):\n    \"\"\"\n    Get the prompt for the current stage.\n    \"\"\"\n    outcomes = \"You need to reach one of these situations:\\n\" + \"\\n\".join(\n        f\"  - {name}: {data['description']}\" for name, data in self.current_stage.outcomes.items()\n    )\n    transitions = \"\\n\".join(\n        f\"  - {stage}: {self.stages[stage].description}\" for stage in self.get_external_transitions()\n    )\n    template = self.current_stage.custom_template or DEFAULT_PROMPT_TEMPLATE\n    return template.format(\n        intent_router_tool=self.name,\n        stage_name=self.current_stage_name,\n        background=self.background,\n        current_goal=self.current_stage.goal,\n        outcomes=outcomes,\n        transitions=transitions,\n    )\n</code></pre>"},{"location":"docs/core-reference/#intentional-core.src.intentional_core.intent_routing.IntentRouter.run","title":"<code>run(params=None)</code>  <code>async</code>","text":"<p>Given the response's classification, returns the new system prompt and the tools accessible in this stage.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Optional[Dict[str, Any]]</code> <p>The parameters for the tool. Contains the <code>response_type</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The new system prompt and the tools accessible in this stage.</p> Source code in <code>intentional-core/src/intentional_core/intent_routing.py</code> <pre><code>async def run(self, params: Optional[Dict[str, Any]] = None) -&gt; str:\n    \"\"\"\n    Given the response's classification, returns the new system prompt and the tools accessible in this stage.\n\n    Args:\n        params: The parameters for the tool. Contains the `response_type`.\n\n    Returns:\n        The new system prompt and the tools accessible in this stage.\n    \"\"\"\n    selected_outcome = params[\"outcome\"]\n    transitions = self.get_external_transitions()\n\n    if selected_outcome not in self.current_stage.outcomes and selected_outcome not in transitions:\n        raise ValueError(f\"Unknown outcome '{params['outcome']}' for stage '{self.current_stage_name}'\")\n\n    if selected_outcome in self.current_stage.outcomes:\n        next_stage = self.current_stage.outcomes[params[\"outcome\"]][\"move_to\"]\n\n        if next_stage != BACKTRACKING_CONNECTION:\n            # Direct stage to stage connection\n            self.current_stage_name = next_stage\n        else:\n            # Backtracking connection\n            self.current_stage_name = self.backtracking_stack.pop()\n    else:\n        # Indirect transition, needs to be tracked in the stack\n        self.backtracking_stack.append(self.current_stage_name)\n        self.current_stage_name = selected_outcome\n\n    return self.get_prompt(), self.current_stage.tools\n</code></pre>"},{"location":"docs/core-reference/#intentional-core.src.intentional_core.intent_routing.Stage","title":"<code>Stage</code>","text":"<p>Describes a stage in the bot's conversation.</p> Source code in <code>intentional-core/src/intentional_core/intent_routing.py</code> <pre><code>class Stage:\n    \"\"\"\n    Describes a stage in the bot's conversation.\n    \"\"\"\n\n    def __init__(self, stage_name, config: Dict[str, Any]) -&gt; None:\n        self.custom_template = config.get(\"custom_template\", None)\n        self.goal = config.get(\"goal\", None)\n        self.description = config.get(\"description\", None)\n        self.accessible_from = config.get(\"accessible_from\", [])\n        if isinstance(self.accessible_from, str):\n            self.accessible_from = [self.accessible_from]\n        self.tools = load_tools_from_dict(config.get(\"tools\", {}))\n        self.outcomes = config.get(\"outcomes\", {})\n\n        # If a custom template is given, nothing else is strictly needed\n        if not self.custom_template:\n            # Make sure the stage has a goal\n            if not self.goal:\n                raise ValueError(f\"Stage '{stage_name}' is missing a goal.\")\n            # If the stage is accessible from somewhere else than a direct transition, it needs a description\n            if self.accessible_from and self.accessible_from != [\"_start_\"] and not self.description:\n                raise ValueError(\n                    \"Stages that set the 'accessible_from' field also need a description. \"\n                    f\"'{stage_name}' has 'accessible_from' set to {self.accessible_from}, but no 'description' field.\"\n                )\n        # Make sure all outcomes have a description\n        for name, outcome in self.outcomes.items():\n            if \"description\" not in outcome:\n                raise ValueError(f\"Outcome '{name}' in stage '{stage_name}' is missing a description.\")\n            if \"move_to\" not in outcome:\n                raise ValueError(f\"Outcome '{name}' in stage '{stage_name}' is missing a 'move_to' field.\")\n\n        log.debug(\n            \"Stage loaded\",\n            custom_template=self.custom_template,\n            stage_goal=self.goal,\n            stage_description=self.description,\n            stage_accessible_from=self.accessible_from,\n            stage_tools=self.tools,\n            outcomes=self.outcomes,\n        )\n</code></pre>"},{"location":"docs/core-reference/#intentional-core.src.intentional_core.llm_client","title":"<code>llm_client</code>","text":"<p>Functions to load LLM client classes from config files.</p>"},{"location":"docs/core-reference/#intentional-core.src.intentional_core.llm_client.LLMClient","title":"<code>LLMClient</code>","text":"<p>               Bases: <code>ABC</code>, <code>EventEmitter</code></p> <p>Tiny base class used to recognize Intentional LLM clients.</p> <p>In order for your client to be usable, you need to assign a value to the <code>name</code> class variable in the client class' definition.</p> Source code in <code>intentional-core/src/intentional_core/llm_client.py</code> <pre><code>class LLMClient(ABC, EventEmitter):\n    \"\"\"\n    Tiny base class used to recognize Intentional LLM clients.\n\n    In order for your client to be usable, you need to assign a value to the `name` class variable\n    in the client class' definition.\n    \"\"\"\n\n    name: Optional[str] = None\n    \"\"\"\n    The name of the client. This should be a unique identifier for the client type.\n    This string will be used in configuration files to identify the type of client to serve a LLM from.\n    \"\"\"\n\n    def __init__(self, parent: \"BotStructure\", intent_router: IntentRouter) -&gt; None:\n        \"\"\"\n        Initialize the LLM client.\n\n        Args:\n            parent: The parent bot structure.\n        \"\"\"\n        super().__init__(parent)\n        self.intent_router = intent_router\n\n    async def connect(self) -&gt; None:\n        \"\"\"\n        Connect to the LLM.\n        \"\"\"\n        await self.emit(\"on_llm_connection\", {})\n\n    async def disconnect(self) -&gt; None:\n        \"\"\"\n        Disconnect from the LLM.\n        \"\"\"\n        await self.emit(\"on_llm_disconnection\", {})\n\n    @abstractmethod\n    async def run(self) -&gt; None:\n        \"\"\"\n        Handle events from the LLM by either processing them internally or by translating them into higher-level\n        events that the BotStructure class can understand, then re-emitting them.\n        \"\"\"\n\n    @abstractmethod\n    async def send(self, data: Dict[str, Any]) -&gt; None:\n        \"\"\"\n        Send a unit of data to the LLM. The response is streamed out as an async generator.\n        \"\"\"\n\n    @abstractmethod\n    async def handle_interruption(self, lenght_to_interruption: int) -&gt; None:\n        \"\"\"\n        Handle an interruption while rendering the output to the user.\n\n        Args:\n            lenght_to_interruption: The length of the data that was produced to the user before the interruption.\n                This value could be number of characters, number of words, milliseconds, number of audio frames, etc.\n                depending on the bot structure that implements it.\n        \"\"\"\n</code></pre>"},{"location":"docs/core-reference/#intentional-core.src.intentional_core.llm_client.LLMClient.name","title":"<code>name = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The name of the client. This should be a unique identifier for the client type. This string will be used in configuration files to identify the type of client to serve a LLM from.</p>"},{"location":"docs/core-reference/#intentional-core.src.intentional_core.llm_client.LLMClient.__init__","title":"<code>__init__(parent, intent_router)</code>","text":"<p>Initialize the LLM client.</p> <p>Parameters:</p> Name Type Description Default <code>parent</code> <code>BotStructure</code> <p>The parent bot structure.</p> required Source code in <code>intentional-core/src/intentional_core/llm_client.py</code> <pre><code>def __init__(self, parent: \"BotStructure\", intent_router: IntentRouter) -&gt; None:\n    \"\"\"\n    Initialize the LLM client.\n\n    Args:\n        parent: The parent bot structure.\n    \"\"\"\n    super().__init__(parent)\n    self.intent_router = intent_router\n</code></pre>"},{"location":"docs/core-reference/#intentional-core.src.intentional_core.llm_client.LLMClient.connect","title":"<code>connect()</code>  <code>async</code>","text":"<p>Connect to the LLM.</p> Source code in <code>intentional-core/src/intentional_core/llm_client.py</code> <pre><code>async def connect(self) -&gt; None:\n    \"\"\"\n    Connect to the LLM.\n    \"\"\"\n    await self.emit(\"on_llm_connection\", {})\n</code></pre>"},{"location":"docs/core-reference/#intentional-core.src.intentional_core.llm_client.LLMClient.disconnect","title":"<code>disconnect()</code>  <code>async</code>","text":"<p>Disconnect from the LLM.</p> Source code in <code>intentional-core/src/intentional_core/llm_client.py</code> <pre><code>async def disconnect(self) -&gt; None:\n    \"\"\"\n    Disconnect from the LLM.\n    \"\"\"\n    await self.emit(\"on_llm_disconnection\", {})\n</code></pre>"},{"location":"docs/core-reference/#intentional-core.src.intentional_core.llm_client.LLMClient.handle_interruption","title":"<code>handle_interruption(lenght_to_interruption)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Handle an interruption while rendering the output to the user.</p> <p>Parameters:</p> Name Type Description Default <code>lenght_to_interruption</code> <code>int</code> <p>The length of the data that was produced to the user before the interruption. This value could be number of characters, number of words, milliseconds, number of audio frames, etc. depending on the bot structure that implements it.</p> required Source code in <code>intentional-core/src/intentional_core/llm_client.py</code> <pre><code>@abstractmethod\nasync def handle_interruption(self, lenght_to_interruption: int) -&gt; None:\n    \"\"\"\n    Handle an interruption while rendering the output to the user.\n\n    Args:\n        lenght_to_interruption: The length of the data that was produced to the user before the interruption.\n            This value could be number of characters, number of words, milliseconds, number of audio frames, etc.\n            depending on the bot structure that implements it.\n    \"\"\"\n</code></pre>"},{"location":"docs/core-reference/#intentional-core.src.intentional_core.llm_client.LLMClient.run","title":"<code>run()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Handle events from the LLM by either processing them internally or by translating them into higher-level events that the BotStructure class can understand, then re-emitting them.</p> Source code in <code>intentional-core/src/intentional_core/llm_client.py</code> <pre><code>@abstractmethod\nasync def run(self) -&gt; None:\n    \"\"\"\n    Handle events from the LLM by either processing them internally or by translating them into higher-level\n    events that the BotStructure class can understand, then re-emitting them.\n    \"\"\"\n</code></pre>"},{"location":"docs/core-reference/#intentional-core.src.intentional_core.llm_client.LLMClient.send","title":"<code>send(data)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Send a unit of data to the LLM. The response is streamed out as an async generator.</p> Source code in <code>intentional-core/src/intentional_core/llm_client.py</code> <pre><code>@abstractmethod\nasync def send(self, data: Dict[str, Any]) -&gt; None:\n    \"\"\"\n    Send a unit of data to the LLM. The response is streamed out as an async generator.\n    \"\"\"\n</code></pre>"},{"location":"docs/core-reference/#intentional-core.src.intentional_core.llm_client.load_llm_client_from_dict","title":"<code>load_llm_client_from_dict(parent, intent_router, config)</code>","text":"<p>Load a LLM client from a dictionary configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Dict[str, Any]</code> <p>The configuration dictionary.</p> required <p>Returns:</p> Type Description <code>LLMClient</code> <p>The LLMClient instance.</p> Source code in <code>intentional-core/src/intentional_core/llm_client.py</code> <pre><code>def load_llm_client_from_dict(parent: \"BotStructure\", intent_router: IntentRouter, config: Dict[str, Any]) -&gt; LLMClient:\n    \"\"\"\n    Load a LLM client from a dictionary configuration.\n\n    Args:\n        config: The configuration dictionary.\n\n    Returns:\n        The LLMClient instance.\n    \"\"\"\n    # Get all the subclasses of LLMClient\n    subclasses: Set[LLMClient] = inheritors(LLMClient)\n    log.debug(\"Collected LLM client classes\", llm_client_classes=subclasses)\n    for subclass in subclasses:\n        if not subclass.name:\n            log.error(\n                \"LLM client class '%s' does not have a name. This LLM client type will not be usable.\",\n                subclass,\n                llm_client_class=subclass,\n            )\n            continue\n\n        if subclass.name in _LLM_CLIENTS:\n            log.warning(\n                \"Duplicate LLM client type '%s' found. The older class will be replaced by the newly imported one.\",\n                subclass.name,\n                old_llm_client_name=subclass.name,\n                old_llm_client_class=_LLM_CLIENTS[subclass.name],\n                new_llm_client_class=subclass,\n            )\n        _LLM_CLIENTS[subclass.name] = subclass\n\n    # Identify the type of bot and see if it's known\n    llm_client_class = config.pop(\"client\")\n    log.debug(\"Creating LLM client\", llm_client_class=llm_client_class)\n    if llm_client_class not in _LLM_CLIENTS:\n        raise ValueError(\n            f\"Unknown LLM client type '{llm_client_class}'. Available types: {list(_LLM_CLIENTS)}. \"\n            \"Did you forget to install your plugin?\"\n        )\n\n    # Handoff to the subclass' init\n    return _LLM_CLIENTS[llm_client_class](parent=parent, intent_router=intent_router, config=config)\n</code></pre>"},{"location":"docs/core-reference/#intentional-core.src.intentional_core.tools","title":"<code>tools</code>","text":"<p>Tools baseclass for Intentional.</p>"},{"location":"docs/core-reference/#intentional-core.src.intentional_core.tools.Tool","title":"<code>Tool</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Tools baseclass for Intentional.</p> Source code in <code>intentional-core/src/intentional_core/tools.py</code> <pre><code>class Tool(ABC):\n    \"\"\"\n    Tools baseclass for Intentional.\n    \"\"\"\n\n    id: str = None\n    name: str = None\n    description: str = None\n    parameters: List[ToolParameter] = None\n\n    def __repr__(self) -&gt; str:\n        return (\n            f\"&lt;{self.__class__.__name__} id={self.id}, description={self.description}, \"\n            f\"parameters={repr(self.parameters)}&gt;\"\n        )\n\n    @abstractmethod\n    async def run(self, params: Optional[Dict[str, Any]] = None) -&gt; Any:\n        \"\"\"\n        Run the tool.\n        \"\"\"\n</code></pre>"},{"location":"docs/core-reference/#intentional-core.src.intentional_core.tools.Tool.run","title":"<code>run(params=None)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Run the tool.</p> Source code in <code>intentional-core/src/intentional_core/tools.py</code> <pre><code>@abstractmethod\nasync def run(self, params: Optional[Dict[str, Any]] = None) -&gt; Any:\n    \"\"\"\n    Run the tool.\n    \"\"\"\n</code></pre>"},{"location":"docs/core-reference/#intentional-core.src.intentional_core.tools.ToolParameter","title":"<code>ToolParameter</code>  <code>dataclass</code>","text":"<p>A parameter for an Intentional tool.</p> Source code in <code>intentional-core/src/intentional_core/tools.py</code> <pre><code>@dataclass\nclass ToolParameter:\n    \"\"\"\n    A parameter for an Intentional tool.\n    \"\"\"\n\n    name: str\n    description: str\n    type: Any\n    required: bool\n    default: Any\n\n    def __repr__(self) -&gt; str:\n        return (\n            f\"&lt;{self.__class__.__name__} name={self.name}, description={self.description}, type={self.type}, \"\n            f\"required={self.required}, default={self.default}&gt;\"\n        )\n</code></pre>"},{"location":"docs/core-reference/#intentional-core.src.intentional_core.tools.load_tools_from_dict","title":"<code>load_tools_from_dict(config)</code>","text":"<p>Load a list of tools from a dictionary configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>List[Dict[str, Any]]</code> <p>The configuration dictionary.</p> required <p>Returns:</p> Type Description <code>Dict[str, Tool]</code> <p>A list of Tool instances.</p> Source code in <code>intentional-core/src/intentional_core/tools.py</code> <pre><code>def load_tools_from_dict(config: List[Dict[str, Any]]) -&gt; Dict[str, Tool]:\n    \"\"\"\n    Load a list of tools from a dictionary configuration.\n\n    Args:\n        config: The configuration dictionary.\n\n    Returns:\n        A list of Tool instances.\n    \"\"\"\n    # Get all the subclasses of Tool\n    subclasses: Set[Tool] = inheritors(Tool)\n    log.debug(\"Collected tool classes\", tool_classes=subclasses)\n    for subclass in subclasses:\n        if not subclass.id:\n            log.error(\n                \"Tool class '%s' does not have an id. This tool will not be usable.\",\n                subclass.__name__,\n                tool_class=subclass,\n            )\n            continue\n\n        if subclass.id in _TOOL_CLASSES:\n            log.debug(\n                \"Duplicate tool '%s' found. The older class will be replaced by the newly imported one.\",\n                subclass.id,\n                old_tool_id=subclass.id,\n                old_tool_class=_TOOL_CLASSES[subclass.id],\n                new_tool_class=subclass,\n            )\n        _TOOL_CLASSES[subclass.id] = subclass\n\n    # Initialize the tools\n    tools = {}\n    for tool_config in config:\n        tool_id = tool_config.pop(\"id\", None)\n        if not tool_id:\n            raise ValueError(\"Tool definitions must have an 'id' field.\")\n        log.debug(\"Creating tool\", tool_id=tool_id)\n        if tool_id not in _TOOL_CLASSES:\n            raise ValueError(\n                f\"Unknown tool '{tool_id}'. Available tools: {list(_TOOL_CLASSES)}. \"\n                \"Did you forget to install a plugin?\"\n            )\n        tool_instance: Tool = _TOOL_CLASSES[tool_id](**tool_config)\n        if getattr(tool_instance, \"name\", None) is None:\n            raise ValueError(f\"Tool '{tool_id}' must have a name.\")\n        if getattr(tool_instance, \"description\", None) is None:\n            raise ValueError(f\"Tool '{tool_id}' must have a description.\")\n        if getattr(tool_instance, \"parameters\", None) is None:\n            raise ValueError(f\"Tool '{tool_id}' must have parameters.\")\n        tools[tool_instance.name] = tool_instance\n\n    return tools\n</code></pre>"},{"location":"docs/core-reference/#intentional-core.src.intentional_core.utils","title":"<code>utils</code>","text":"<p>Utilities for Intentional.</p>"},{"location":"docs/core-reference/#intentional-core.src.intentional_core.utils.importing","title":"<code>importing</code>","text":"<p>Module import functions to handle dynamic plugins import.</p>"},{"location":"docs/core-reference/#intentional-core.src.intentional_core.utils.importing.import_all_plugins","title":"<code>import_all_plugins()</code>","text":"<p>Imports all the <code>intentional-*</code> packages found in the current environment.</p> Source code in <code>intentional-core/src/intentional_core/utils/importing.py</code> <pre><code>def import_all_plugins():\n    \"\"\"\n    Imports all the `intentional-*` packages found in the current environment.\n    \"\"\"\n    for dist in importlib.metadata.distributions():\n        if not hasattr(dist, \"_path\"):\n            log.debug(\"'_path' not found in '%s', ignoring\", dist, dist=dist)\n        path = dist._path  # pylint: disable=protected-access\n        if path.name.startswith(\"intentional_\"):\n            if os.path.exists(path / \"top_level.txt\"):\n                with open(path / \"top_level.txt\", encoding=\"utf-8\") as file:\n                    for name in file.read().splitlines():\n                        import_plugin(name)\n            else:\n                name = path.name.split(\"-\", 1)[0]\n                import_plugin(name)\n</code></pre>"},{"location":"docs/core-reference/#intentional-core.src.intentional_core.utils.importing.import_plugin","title":"<code>import_plugin(name)</code>","text":"<p>Imports the specified package. It does NOT check if this is an Intentional package or not.</p> Source code in <code>intentional-core/src/intentional_core/utils/importing.py</code> <pre><code>def import_plugin(name: str):\n    \"\"\"\n    Imports the specified package. It does NOT check if this is an Intentional package or not.\n    \"\"\"\n    try:\n        log.debug(\"Importing module\", module_name=name)\n        module = importlib.import_module(name)\n        # Print all classes in the module\n        class_found = False\n        for _, obj in inspect.getmembers(module):\n            if inspect.isclass(obj):\n                log.debug(\"Class found in module\", module_name=name, found_class=obj)\n                class_found = True\n        if not class_found:\n            log.debug(\n                \"No classes found in module: are they imported in the top-level __init__ file of the plugin?\",\n                module_name=name,\n            )\n    except ModuleNotFoundError:\n        log.exception(\"Module '%s' not found for import, is it installed?\", name, module_name=name)\n</code></pre>"},{"location":"docs/core-reference/#intentional-core.src.intentional_core.utils.inheritance","title":"<code>inheritance</code>","text":"<p>Utils for inheritance checks, to discover subclasses of a base Intentional class.</p>"},{"location":"docs/core-reference/#intentional-core.src.intentional_core.utils.inheritance.inheritors","title":"<code>inheritors(class_, include_abstract=False)</code>","text":"<p>Find all subclasses of a class, regardless of depth.</p> <p>Parameters:</p> Name Type Description Default <code>class_</code> <code>Any</code> <p>The class to find subclasses of.</p> required <code>include_abstract</code> <code>bool</code> <p>Whether to include abstract classes in the results.</p> <code>False</code> Source code in <code>intentional-core/src/intentional_core/utils/inheritance.py</code> <pre><code>def inheritors(class_: Any, include_abstract: bool = False) -&gt; Set[Any]:\n    \"\"\"\n    Find all subclasses of a class, regardless of depth.\n\n    Args:\n        class_: The class to find subclasses of.\n        include_abstract: Whether to include abstract classes in the results.\n    \"\"\"\n    subclasses = set()\n    to_process = [class_]\n    while to_process:\n        parent = to_process.pop()\n        for child in parent.__subclasses__():\n            if child not in subclasses:\n                to_process.append(child)\n                if not include_abstract and inspect.isabstract(child):\n                    log.debug(\n                        \"Skipping abstract class from inheritor's list.\",\n                        abstract_class=child,\n                        abstract_methods=list(child.__abstractmethods__),\n                    )\n                else:\n                    subclasses.add(child)\n\n    return subclasses\n</code></pre>"},{"location":"docs/home/","title":"Intentional","text":"<p>Intentional is an open-source Python framework to build reliable LLM chatbots that actually talk and behave as you expect.</p> <p>Pure LLM-based chatbots are very hard to control: when a lot of very specific instructions are pushed into their system prompt, their performance will get worse and worse the more instructions you add. These bots will work very well on small demos, but don\u2019t scale to real use cases, where you may need the bot to follow a very specific set of workflows depending on the situation it find itself in, without improvising.</p> <p>Intentional introduces a new way of prompting the LLM in a way that gives the developer full control on the conversation at scale while retaining the smooth conversational skills of the LLM.</p>"},{"location":"docs/home/#getting-started","title":"Getting started","text":"<p>First, install Intentional:</p> <pre><code>pip install intentional\n</code></pre> <p>Next, get a configuration file. For your first test run you should pick this file, which needs no additional plugins, but you can find a few other examples here.</p> <p>Note</p> <p>The example here also requires an OpenAI key. Export it as an environment variable called <code>OPENAI_API_KEY</code> before proceeding.</p> <p>Assuming your configuration file is called <code>intentional_bot.yml</code>, you can now launch your bot by doing:</p> <pre><code>intentional intentional_bot.yml\n</code></pre> <p>The output should look like:</p> <pre><code>==&gt; Chat is ready!\n\nUser:\n</code></pre> <p>Type in your message and the bot is going to respond.</p>"},{"location":"docs/home/#draw-the-conversation","title":"Draw the conversation","text":"<p>To see the graph of the conversation defined by this configuration file, run:</p> <pre><code>intentional intentional_bot.yml --draw\n</code></pre> <p>The graph will be saved next to your configuration file as <code>intentional_bot.png</code>.</p>"},{"location":"docs/home/#running-from-code","title":"Running from code","text":"<p>If you want to run your bot from code instead of using the command line tool, this is how you can do it:</p> <pre><code>import asyncio\nfrom intentional_core import load_configuration_file\n\ndef main():\n    bot = load_configuration_file(\"intentional_bot.yml\")\n    asyncio.run(bot.run())\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>You can also load an Intentional bot directly from a dictionary configuration instead of using a YAML file:</p> <pre><code>import asyncio\nfrom intentional_core import load_bot_interface_from_dict\n\ndef main():\n    bot = load_bot_interface_from_dict({ ... your configuration ...})\n    asyncio.run(bot.run())\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>There are other methods to load only parts of your Intentional bot, such as skipping the bot interface entirely if you want to interact with it using the Python API. To find out which methods you can use, have a look at the API Reference.</p>"},{"location":"docs/home/#what-next","title":"What next?","text":"<p>Once you ran your first example, you should head to High-level Concepts to understand how to use Intentional, and then check out the specs of Intentional's configuration file to start building your own bots.</p>"},{"location":"docs/installation/","title":"Installation","text":""},{"location":"docs/installation/#default-install","title":"Default install","text":"<p>The easiest way to get started with Intentional is to install the <code>intentional</code> package:</p> <pre><code>pip install intentional\n</code></pre> <p>This will install the core of Intentional, a small CLI utility, the conversation graph drawing functionality and a couple of basic plugins (<code>intentional-openai</code> and <code>intentional-terminal</code> to get you started).</p>"},{"location":"docs/installation/#bare-bone-install","title":"Bare-bone install","text":"<p>If you want only Intentional, with no plugin and utils, you should install <code>intentional-core</code> instead:</p> <pre><code>pip install intentional-core\n</code></pre>"},{"location":"docs/installation/#install-from-source","title":"Install from source","text":"<p>If you plan to contribute to Intentional, you should install it from source. Clone the repo and the install the packages one by one.</p> <p>Note that if you are contributing to the <code>intentional</code> package (not <code>intentional-core</code> only) the packages need to be installed in the correct order if you want to install all of them from source:</p> <pre><code>pip install intentional-core/\npip install plugins/intentional-openai/\npip install plugins/intentional-terminal/\npip install intentional/\n</code></pre> <p>If you forget to do so, everything will work well, but the packages will be installed from PyPI instead of being installed from your local copy.</p> <p>In the same vein, most plugins only need <code>intentional-core</code> installed before them, such as:</p> <pre><code>pip install intentional-core/\npip install plugins/intentional-myplugin/\n</code></pre> <p>In general, most plugins don't depend on each other. However, some do: if that's the case, make sure to install those plugins first.</p> <p>For example, <code>intentional-textual-ui</code> depends on <code>intentional-terminal</code>. To have a full from-source install of the entire stack, the order of installation is:</p> <pre><code>pip install intentional-core/\npip install plugins/intentional-terminal/\npip install plugins/intentional-textual-ui/\n</code></pre>"},{"location":"docs/plugins/","title":"Plugins","text":"<p>Warning</p> <p>Work in progress</p> <p>Every layer of an Intentional bot that is configurable from the configuration file (interfaces, bot structures, LLM client, tools etc) can be expanded through a plugin.</p>"},{"location":"docs/plugins/#writing-your-own-plugins","title":"Writing your own plugins","text":"<p>To get started:</p> <ul> <li>Check the Contributing guidelines for an idea of how to contribute a plugin to the main Intentional repo.</li> <li>You can host your plugins in your own repo, it's not necessary to have it merged in the main repo.</li> <li>You can use the Copier template under the plugins folder here.</li> </ul>"},{"location":"docs/reference/","title":"API Reference - Main Package","text":"<p>Init file for Intentional's sample tools.</p>"},{"location":"docs/reference/#intentional.GetCurrentDateTimeTool","title":"<code>GetCurrentDateTimeTool</code>","text":"<p>               Bases: <code>Tool</code></p> <p>Simple tool to get the current date and time.</p> Source code in <code>intentional/sample_tools.py</code> <pre><code>class GetCurrentDateTimeTool(Tool):\n    \"\"\"\n    Simple tool to get the current date and time.\n    \"\"\"\n\n    id = \"get_current_date_and_time\"\n    name = \"get_current_date_and_time\"\n    description = \"Get the current date and time in the format 'YYYY-MM-DD HH:MM:SS'.\"\n    parameters = []\n\n    async def run(self, params: Optional[Dict[str, Any]] = None) -&gt; str:\n        \"\"\"\n        Returns the current time.\n        \"\"\"\n        current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        log.debug(\"Getting the current date and time.\", current_date_time=current_datetime)\n        return current_datetime\n</code></pre>"},{"location":"docs/reference/#intentional.GetCurrentDateTimeTool.run","title":"<code>run(params=None)</code>  <code>async</code>","text":"<p>Returns the current time.</p> Source code in <code>intentional/sample_tools.py</code> <pre><code>async def run(self, params: Optional[Dict[str, Any]] = None) -&gt; str:\n    \"\"\"\n    Returns the current time.\n    \"\"\"\n    current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    log.debug(\"Getting the current date and time.\", current_date_time=current_datetime)\n    return current_datetime\n</code></pre>"},{"location":"docs/reference/#intentional.RescheduleInterviewTool","title":"<code>RescheduleInterviewTool</code>","text":"<p>               Bases: <code>Tool</code></p> <p>Mock tool to reschedule an interview.</p> Source code in <code>intentional/sample_tools.py</code> <pre><code>class RescheduleInterviewTool(Tool):\n    \"\"\"\n    Mock tool to reschedule an interview.\n    \"\"\"\n\n    id = \"reschedule_interview\"\n    name = \"reschedule_interview\"\n    description = \"Set a new date and time for the interview in the database.\"\n    parameters = [\n        ToolParameter(\n            \"date\",\n            \"The new date for the interview.\",\n            \"string\",\n            True,\n            None,\n        ),\n        ToolParameter(\n            \"time\",\n            \"The new time for the interview.\",\n            \"string\",\n            True,\n            None,\n        ),\n    ]\n\n    async def run(self, params: Optional[Dict[str, Any]] = None) -&gt; str:\n        \"\"\"\n        Returns the current time.\n        \"\"\"\n        log.debug(\"Rescheduling the interview.\")\n        return \"The interview was rescheduled successfully.\"\n</code></pre>"},{"location":"docs/reference/#intentional.RescheduleInterviewTool.run","title":"<code>run(params=None)</code>  <code>async</code>","text":"<p>Returns the current time.</p> Source code in <code>intentional/sample_tools.py</code> <pre><code>async def run(self, params: Optional[Dict[str, Any]] = None) -&gt; str:\n    \"\"\"\n    Returns the current time.\n    \"\"\"\n    log.debug(\"Rescheduling the interview.\")\n    return \"The interview was rescheduled successfully.\"\n</code></pre>"},{"location":"docs/reference/#intentional.__about__","title":"<code>__about__</code>","text":"<p>Package descriptors for intentional.</p>"},{"location":"docs/reference/#intentional.cli","title":"<code>cli</code>","text":"<p>Entry point for the Intentional CLI.</p>"},{"location":"docs/reference/#intentional.cli.draw_intent_graph_from_config","title":"<code>draw_intent_graph_from_config(path)</code>  <code>async</code>","text":"<p>Load the intent router from the configuration file.</p> Source code in <code>intentional/cli.py</code> <pre><code>async def draw_intent_graph_from_config(path: str) -&gt; IntentRouter:\n    \"\"\"\n    Load the intent router from the configuration file.\n    \"\"\"\n    log.debug(\"Loading YAML configuration file\", config_file_path=path)\n\n    with open(path, \"r\", encoding=\"utf-8\") as file:\n        config = yaml.safe_load(file)\n    log.debug(\"Loading bot interface\", bot_interface_config=json.dumps(config, indent=4))\n\n    plugins = config.pop(\"plugins\")\n    log.debug(\"Collected_plugins\", plugins=plugins)\n    for plugin in plugins:\n        import_plugin(plugin)\n\n    # Remove YAML extension from path\n    path = path.rsplit(\".\", 1)[0]\n    intent_router = IntentRouter(config.pop(\"conversation\", {}))\n    return await to_image(intent_router, path + \".png\")\n</code></pre>"},{"location":"docs/reference/#intentional.cli.main","title":"<code>main()</code>","text":"<p>Entry point for the Intentional CLI.</p> Source code in <code>intentional/cli.py</code> <pre><code>def main():\n    \"\"\"\n    Entry point for the Intentional CLI.\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"path\", help=\"the path to the configuration file to load.\", type=str)\n    parser.add_argument(\"--draw\", help=\"Draw the graph.\", action=\"store_true\")\n    parser.add_argument(\n        \"--log-cli-level\",\n        help=\"Select the logging level to the console.\",\n        type=str,\n        default=\"INFO\",\n    )\n    parser.add_argument(\"--log-file\", help=\"Path to the log file.\", type=str)\n    parser.add_argument(\n        \"--log-file-level\",\n        help=\"Select the logging level to the file. Ignore if no path is specified with --log-file\",\n        type=str,\n        default=\"DEBUG\",\n    )\n    args = parser.parse_args()\n\n    # Set the CLI log level\n    cli_level = logging.getLevelName(args.log_cli_level.upper())\n    structlog.configure(wrapper_class=structlog.make_filtering_bound_logger(cli_level))\n\n    if args.log_file:\n        # https://www.structlog.org/en/stable/standard-library.html\n        file_level = logging.getLevelName(args.log_file_level.upper())\n        timestamper = structlog.processors.TimeStamper(fmt=\"iso\")\n        pre_chain = [\n            structlog.stdlib.add_log_level,\n            structlog.stdlib.ExtraAdder(),\n            timestamper,\n        ]\n        logging.config.dictConfig(\n            {\n                \"version\": 1,\n                \"disable_existing_loggers\": False,\n                \"handlers\": {\n                    \"default\": {\n                        \"level\": cli_level,\n                        \"class\": \"logging.StreamHandler\",\n                        \"formatter\": \"colored\",\n                    },\n                    \"file\": {\n                        \"level\": file_level,\n                        \"class\": \"logging.handlers.WatchedFileHandler\",\n                        \"filename\": args.log_file,\n                        \"formatter\": \"json\",\n                    },\n                },\n                \"formatters\": {\n                    \"json\": {\n                        \"()\": \"pythonjsonlogger.jsonlogger.JsonFormatter\",\n                        \"fmt\": \"%(asctime)s %(levelname)s %(message)s\",\n                    },\n                    \"colored\": {\n                        \"()\": structlog.stdlib.ProcessorFormatter,\n                        \"processors\": [\n                            structlog.stdlib.ProcessorFormatter.remove_processors_meta,\n                            structlog.dev.ConsoleRenderer(colors=True),\n                        ],\n                        \"foreign_pre_chain\": pre_chain,\n                    },\n                },\n                \"loggers\": {\n                    \"\": {\n                        \"handlers\": [\"default\", \"file\"],\n                        \"level\": \"DEBUG\",\n                        \"propagate\": True,\n                    },\n                },\n            }\n        )\n        structlog.configure(\n            processors=[\n                structlog.stdlib.add_log_level,\n                structlog.stdlib.PositionalArgumentsFormatter(),\n                timestamper,\n                structlog.processors.StackInfoRenderer(),\n                structlog.processors.format_exc_info,\n                structlog.stdlib.ProcessorFormatter.wrap_for_formatter,\n            ],\n            logger_factory=structlog.stdlib.LoggerFactory(),\n            wrapper_class=structlog.stdlib.BoundLogger,\n            cache_logger_on_first_use=True,\n        )\n\n    if args.draw:\n        asyncio.run(draw_intent_graph_from_config(args.path))\n        return\n\n    bot = load_configuration_file(args.path)\n    asyncio.run(bot.run())\n</code></pre>"},{"location":"docs/reference/#intentional.draw","title":"<code>draw</code>","text":"<p>Helpers that allow the user to draw the bot's graph.</p>"},{"location":"docs/reference/#intentional.draw.to_bytes","title":"<code>to_bytes(intent_router, mermaid_domain='https://mermaid.ink/img/')</code>  <code>async</code>","text":"<p>Uses mermaid.ink to render the intent's graph into an image.</p> <p>Parameters:</p> Name Type Description Default <code>intent_router</code> <code>IntentRouter</code> <p>the intents graph to draw.</p> required <code>mermaid_domain</code> <code>str</code> <p>the domain of your Mermaid instance, if you have your own. Defaults to the public mermaid.ink domain.</p> <code>'https://mermaid.ink/img/'</code> <p>Returns:</p> Type Description <code>bytes</code> <p>The bytes of the resulting image. To save them into an image file, do:</p> <code>bytes</code> <p>```python</p> <code>bytes</code> <p>image = to_image(intent_router)</p> <code>bytes</code> <p>with open(image_path, \"wb\") as imagefile: imagefile.write(image)</p> <code>bytes</code> <p>```</p> Source code in <code>intentional/draw.py</code> <pre><code>async def to_bytes(intent_router: IntentRouter, mermaid_domain: str = \"https://mermaid.ink/img/\") -&gt; bytes:\n    \"\"\"\n    Uses mermaid.ink to render the intent's graph into an image.\n\n    Args:\n        intent_router: the intents graph to draw.\n        mermaid_domain: the domain of your Mermaid instance, if you have your own.\n            Defaults to the public mermaid.ink domain.\n\n    Returns:\n        The bytes of the resulting image. To save them into an image file, do:\n\n        ```python\n        image = to_image(intent_router)\n        with open(image_path, \"wb\") as imagefile:\n            imagefile.write(image)\n        ```\n    \"\"\"\n    url = to_mermaid_link(intent_router, mermaid_domain)\n    resp = requests.get(url, timeout=10)\n    if resp.status_code &gt;= 400:\n        resp.raise_for_status()\n    return resp.content\n</code></pre>"},{"location":"docs/reference/#intentional.draw.to_image","title":"<code>to_image(intent_router, image_path)</code>  <code>async</code>","text":"<p>Saves an image of the intent's graph at the given path.</p> <p>Parameters:</p> Name Type Description Default <code>intent_router</code> <code>IntentRouter</code> <p>the intents graph to draw.</p> required <code>image_path</code> <code>Path</code> <p>where to save the resulting image</p> required Source code in <code>intentional/draw.py</code> <pre><code>async def to_image(intent_router: IntentRouter, image_path: Path) -&gt; None:\n    \"\"\"\n    Saves an image of the intent's graph at the given path.\n\n    Args:\n        intent_router: the intents graph to draw.\n        image_path: where to save the resulting image\n    \"\"\"\n    image = await to_bytes(intent_router)\n    with open(image_path, \"wb\") as imagefile:\n        imagefile.write(image)\n</code></pre>"},{"location":"docs/reference/#intentional.draw.to_mermaid_diagram","title":"<code>to_mermaid_diagram(intent_router)</code>","text":"<p>Creates a textual representation of the intents graph in a way that Mermaid.ink can render.</p> <p>Keep in mind that this function should be able to render also malformed graphs as far as possible, because it can be used as a debugging tool to visualize bad bot configurations within error messages.</p> <p>Parameters:</p> Name Type Description Default <code>intent_router</code> <code>IntentRouter</code> <p>the intents graph to draw.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A string containing the description of the graph.</p> Source code in <code>intentional/draw.py</code> <pre><code>def to_mermaid_diagram(intent_router: IntentRouter) -&gt; str:\n    \"\"\"\n    Creates a textual representation of the intents graph in a way that Mermaid.ink can render.\n\n    Keep in mind that this function should be able to render also malformed graphs as\n    far as possible, because it can be used as a debugging tool to visualize bad bot\n    configurations within error messages.\n\n    Args:\n        intent_router: the intents graph to draw.\n\n    Returns:\n        A string containing the description of the graph.\n    \"\"\"\n    stages = {node: f'{node}[\"&lt;b&gt;{node}&lt;/b&gt;\"]' for node in intent_router.stages}\n    unique_counter = 0\n    connections_list = []\n\n    processed_nodes = set()\n    for origin, target, key in intent_router.graph.edges:\n        # 'end' is reserved in Mermaid\n        if target == \"_end_\":\n            unique_counter += 1\n            edge_string = f'{stages[origin]} -- {key} --&gt; END{unique_counter}[\"&lt;b&gt;end&lt;/b&gt;\"]:::highlight'\n        elif target == \"_backtrack_\":\n            unique_counter += 1\n            edge_string = f'{stages[origin]} -- {key} --&gt; BACKTRACK{unique_counter}(\"&lt;b&gt;backtrack&lt;/b&gt;\"):::highlight'\n\n        else:\n            edge_string = f\"{stages[origin]} -- {key} --&gt; {stages.get(target, target)}\"\n        connections_list.append(edge_string)\n\n        # Indirect connections\n        if origin not in processed_nodes:\n            processed_nodes.add(origin)\n\n            if intent_router.stages[origin].accessible_from == [\"_start_\"]:\n                edge_string = f'START(\"&lt;b&gt;start&lt;/b&gt;\"):::highlight ---&gt; {stages[origin]}'\n            elif intent_router.stages[origin].accessible_from == [\"_all_\"]:\n                unique_counter += 1\n                edge_string = f'ALL{unique_counter}(\"&lt;b&gt;all&lt;/b&gt;\"):::highlight ---&gt; {stages[origin]}'\n            elif intent_router.stages[origin].accessible_from:\n                unique_counter += 1\n                accessible_from_str = \",&lt;br&gt;\".join(intent_router.stages[origin].accessible_from)\n                edge_string = f'FROM{unique_counter}(\"&lt;b&gt;{accessible_from_str}&lt;/b&gt;\"):::highlight ---&gt; {stages[origin]}'\n            else:\n                continue\n            connections_list.append(edge_string)\n\n    connections = \"\\n\".join(connections_list)\n    graph_styled = MERMAID_STYLED_TEMPLATE.format(connections=connections)\n    log.debug(\"Mermaid graph created\", mermaid_graph=graph_styled)\n    return graph_styled\n</code></pre>"},{"location":"docs/reference/#intentional.draw.to_mermaid_link","title":"<code>to_mermaid_link(intent_router, mermaid_domain='https://mermaid.ink/img/')</code>","text":"<p>Generated a URL that contains a rendering of the graph of the intents into an image.</p> <p>Parameters:</p> Name Type Description Default <code>intent_router</code> <code>IntentRouter</code> <p>the intents graph to draw.</p> required <code>mermaid_domain</code> <code>str</code> <p>the domain of your Mermaid instance, if you have your own. Defaults to the public mermaid.ink domain.</p> <code>'https://mermaid.ink/img/'</code> <p>Returns:</p> Type Description <code>str</code> <p>A URL on Mermaid.ink with the graph of the intents.</p> Source code in <code>intentional/draw.py</code> <pre><code>def to_mermaid_link(intent_router: IntentRouter, mermaid_domain: str = \"https://mermaid.ink/img/\") -&gt; str:\n    \"\"\"\n    Generated a URL that contains a rendering of the graph of the intents into an image.\n\n    Args:\n        intent_router: the intents graph to draw.\n        mermaid_domain: the domain of your Mermaid instance, if you have your own.\n            Defaults to the public mermaid.ink domain.\n\n    Returns:\n        A URL on Mermaid.ink with the graph of the intents.\n    \"\"\"\n    graph_styled = to_mermaid_diagram(intent_router)\n    graphbytes = graph_styled.encode(\"ascii\")\n    base64_bytes = base64.b64encode(graphbytes)\n    base64_string = base64_bytes.decode(\"ascii\")\n    return mermaid_domain + base64_string\n</code></pre>"},{"location":"docs/reference/#intentional.sample_tools","title":"<code>sample_tools</code>","text":"<p>Sample tools for Intentional's examples.</p>"},{"location":"docs/reference/#intentional.sample_tools.GetCurrentDateTimeTool","title":"<code>GetCurrentDateTimeTool</code>","text":"<p>               Bases: <code>Tool</code></p> <p>Simple tool to get the current date and time.</p> Source code in <code>intentional/sample_tools.py</code> <pre><code>class GetCurrentDateTimeTool(Tool):\n    \"\"\"\n    Simple tool to get the current date and time.\n    \"\"\"\n\n    id = \"get_current_date_and_time\"\n    name = \"get_current_date_and_time\"\n    description = \"Get the current date and time in the format 'YYYY-MM-DD HH:MM:SS'.\"\n    parameters = []\n\n    async def run(self, params: Optional[Dict[str, Any]] = None) -&gt; str:\n        \"\"\"\n        Returns the current time.\n        \"\"\"\n        current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        log.debug(\"Getting the current date and time.\", current_date_time=current_datetime)\n        return current_datetime\n</code></pre>"},{"location":"docs/reference/#intentional.sample_tools.GetCurrentDateTimeTool.run","title":"<code>run(params=None)</code>  <code>async</code>","text":"<p>Returns the current time.</p> Source code in <code>intentional/sample_tools.py</code> <pre><code>async def run(self, params: Optional[Dict[str, Any]] = None) -&gt; str:\n    \"\"\"\n    Returns the current time.\n    \"\"\"\n    current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    log.debug(\"Getting the current date and time.\", current_date_time=current_datetime)\n    return current_datetime\n</code></pre>"},{"location":"docs/reference/#intentional.sample_tools.MockTool","title":"<code>MockTool</code>","text":"<p>               Bases: <code>Tool</code></p> <p>Simple tool that returns a fixed response to a fixed parameter value.</p> <p>Accepts a single parameter, \"request\", which is a string.</p> Source code in <code>intentional/sample_tools.py</code> <pre><code>class MockTool(Tool):\n    \"\"\"\n    Simple tool that returns a fixed response to a fixed parameter value.\n\n    Accepts a single parameter, \"request\", which is a string.\n    \"\"\"\n\n    id = \"mock_tool\"\n\n    def __init__(\n        self, name, description, input_description, responses_dictionary=None, default_response=None\n    ):  # pylint: disable=too-many-arguments,too-many-positional-arguments\n        self.name = name\n        self.description = description\n        self.parameters = [ToolParameter(\"request\", input_description, \"string\", True, None)]\n        self.responses_dictionary = responses_dictionary or {}\n        self.default_response = default_response\n\n    async def run(self, params: Optional[Dict[str, Any]] = None) -&gt; str:\n        \"\"\"\n        Returns a fixed response to a fixed parameter value.\n        \"\"\"\n        response = self.responses_dictionary.get(params[\"request\"], self.default_response)\n        if response:\n            log.debug(\"ExampleTool found a match\", request=params[\"request\"], response=response)\n        else:\n            log.debug(\"ExampleTool did not find a match\", request=params[\"request\"])\n        return response\n</code></pre>"},{"location":"docs/reference/#intentional.sample_tools.MockTool.run","title":"<code>run(params=None)</code>  <code>async</code>","text":"<p>Returns a fixed response to a fixed parameter value.</p> Source code in <code>intentional/sample_tools.py</code> <pre><code>async def run(self, params: Optional[Dict[str, Any]] = None) -&gt; str:\n    \"\"\"\n    Returns a fixed response to a fixed parameter value.\n    \"\"\"\n    response = self.responses_dictionary.get(params[\"request\"], self.default_response)\n    if response:\n        log.debug(\"ExampleTool found a match\", request=params[\"request\"], response=response)\n    else:\n        log.debug(\"ExampleTool did not find a match\", request=params[\"request\"])\n    return response\n</code></pre>"},{"location":"docs/reference/#intentional.sample_tools.RescheduleInterviewTool","title":"<code>RescheduleInterviewTool</code>","text":"<p>               Bases: <code>Tool</code></p> <p>Mock tool to reschedule an interview.</p> Source code in <code>intentional/sample_tools.py</code> <pre><code>class RescheduleInterviewTool(Tool):\n    \"\"\"\n    Mock tool to reschedule an interview.\n    \"\"\"\n\n    id = \"reschedule_interview\"\n    name = \"reschedule_interview\"\n    description = \"Set a new date and time for the interview in the database.\"\n    parameters = [\n        ToolParameter(\n            \"date\",\n            \"The new date for the interview.\",\n            \"string\",\n            True,\n            None,\n        ),\n        ToolParameter(\n            \"time\",\n            \"The new time for the interview.\",\n            \"string\",\n            True,\n            None,\n        ),\n    ]\n\n    async def run(self, params: Optional[Dict[str, Any]] = None) -&gt; str:\n        \"\"\"\n        Returns the current time.\n        \"\"\"\n        log.debug(\"Rescheduling the interview.\")\n        return \"The interview was rescheduled successfully.\"\n</code></pre>"},{"location":"docs/reference/#intentional.sample_tools.RescheduleInterviewTool.run","title":"<code>run(params=None)</code>  <code>async</code>","text":"<p>Returns the current time.</p> Source code in <code>intentional/sample_tools.py</code> <pre><code>async def run(self, params: Optional[Dict[str, Any]] = None) -&gt; str:\n    \"\"\"\n    Returns the current time.\n    \"\"\"\n    log.debug(\"Rescheduling the interview.\")\n    return \"The interview was rescheduled successfully.\"\n</code></pre>"},{"location":"plugins/intentional-fastapi/","title":"Intentional - FastAPI REST API","text":"<p>Plugin that lets you interact with your Intentional bots through a FastAPI-based REST API interface</p>"},{"location":"plugins/intentional-fastapi/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Installation</li> <li>License</li> </ul>"},{"location":"plugins/intentional-fastapi/#installation","title":"Installation","text":"<pre><code>pip install intentional-fastapi\n</code></pre>"},{"location":"plugins/intentional-fastapi/#license","title":"License","text":"<p><code>intentional</code> is distributed under the terms of the AGPL license. If that doesn't work for you, get in touch.</p>"},{"location":"plugins/intentional-fastapi/docs/reference/","title":"API Reference - FastAPI REST API","text":""},{"location":"plugins/intentional-fastapi/docs/reference/#plugins.intentional-fastapi.src","title":"<code>src</code>","text":""},{"location":"plugins/intentional-fastapi/docs/reference/#plugins.intentional-fastapi.src.intentional_fastapi","title":"<code>intentional_fastapi</code>","text":"<p>FastAPI REST API interface for Intentional.</p>"},{"location":"plugins/intentional-fastapi/docs/reference/#plugins.intentional-fastapi.src.intentional_fastapi.__about__","title":"<code>__about__</code>","text":"<p>Package descriptors for intentional-fastapi.</p>"},{"location":"plugins/intentional-fastapi/docs/reference/#plugins.intentional-fastapi.src.intentional_fastapi.bot_interface","title":"<code>bot_interface</code>","text":"<p>FastAPI REST API interface for Intentional.</p>"},{"location":"plugins/intentional-fastapi/docs/reference/#plugins.intentional-fastapi.src.intentional_fastapi.bot_interface.FastAPIBotInterface","title":"<code>FastAPIBotInterface</code>","text":"<p>               Bases: <code>BotInterface</code></p> <p>Bot that lets you use a FastAPI REST API to interact with the user.</p> Source code in <code>plugins/intentional-fastapi/src/intentional_fastapi/bot_interface.py</code> <pre><code>class FastAPIBotInterface(BotInterface):\n    \"\"\"\n    Bot that lets you use a FastAPI REST API to interact with the user.\n    \"\"\"\n\n    name = \"fastapi\"\n\n    def __init__(self, config: Dict[str, Any], intent_router: IntentRouter):\n        # Init the structure\n        bot_structure_config = config.pop(\"bot\", None)\n        if not bot_structure_config:\n            raise ValueError(\n                f\"{self.__class__.__name__} requires a 'bot' configuration key to know how to structure the bot.\"\n            )\n        log.debug(\"Creating bot structure\", bot_structure_config=bot_structure_config)\n        self.bot: BotStructure = load_bot_structure_from_dict(intent_router=intent_router, config=bot_structure_config)\n\n        # Check the modality\n        self.modality = config.pop(\"modality\")\n        log.debug(\"Modality for %s is set\", self.__class__.__name__, modality=self.modality)\n\n    async def run(self) -&gt; None:\n        \"\"\"\n        Chooses the specific loop to use for this combination of bot and modality and kicks it off.\n        \"\"\"\n        if self.modality == \"text_messages\":\n            await self._run_text_messages(self.bot)\n        elif self.modality == \"audio_stream\":\n            await self._run_audio_stream(self.bot)\n        else:\n            raise ValueError(\n                f\"Modality '{self.modality}' is not yet supported.\"\n                \"These are the supported modalities: 'text_messages', 'audio_stream'.\"\n            )\n\n    async def handle_response_chunks(self, response: ResponseChunksIterator, event: Dict[str, Any]) -&gt; None:\n        \"\"\"\n        Stream out text messages from the bot through a TextChunksIterator.\n        \"\"\"\n        if event[\"delta\"]:\n            await response.asend(event[\"delta\"])\n\n    async def _run_text_messages(self, bot: BotStructure) -&gt; None:\n        \"\"\"\n        Runs the interface for the text turns modality.\n        \"\"\"\n        app = FastAPI(title=\"Intentional FastAPI\")\n\n        @app.get(\"/send\")\n        async def send_message(message: str):\n            \"\"\"\n            Send a message to the bot.\n            \"\"\"\n            response = ResponseChunksIterator()\n            bot.add_event_handler(\n                \"on_text_message_from_llm\",\n                lambda event: self.handle_response_chunks(response, event),\n            )\n            await self.bot.send({\"text_message\": {\"role\": \"user\", \"content\": message}})\n            return StreamingResponse(response)\n\n        await bot.connect()\n\n        config = uvicorn.Config(app, host=\"0.0.0.0\", port=8000)\n        server = uvicorn.Server(config)\n        await server.serve()\n\n    # TODO TEST THIS MODALITY!\n    async def _run_audio_stream(self, bot: BotStructure) -&gt; None:\n        \"\"\"\n        Runs the interface for the audio stream modality.\n        \"\"\"\n\n        app = FastAPI(title=\"Intentional FastAPI\")\n\n        @app.get(\"/ws/input\")\n        async def input_stream(websocket: WebSocket):\n            await websocket.accept()\n            while True:\n                data = await websocket.receive()\n                self.bot.send({\"audio_chunk\": data})\n\n        @app.get(\"/ws/output\")\n        async def output_stream(websocket: WebSocket):\n            await websocket.accept()\n\n            async def send_audio_chunk(event: Dict[str, Any]) -&gt; None:\n                if event[\"delta\"]:\n                    await websocket.send_bytes(event[\"delta\"])\n\n            response = ResponseChunksIterator()\n            bot.add_event_handler(\"on_audio_message_from_llm\", send_audio_chunk)\n            return StreamingResponse(response)\n\n        await bot.connect()\n\n        config = uvicorn.Config(app, host=\"0.0.0.0\", port=8000)\n        server = uvicorn.Server(config)\n        await server.serve()\n</code></pre>"},{"location":"plugins/intentional-fastapi/docs/reference/#plugins.intentional-fastapi.src.intentional_fastapi.bot_interface.FastAPIBotInterface.handle_response_chunks","title":"<code>handle_response_chunks(response, event)</code>  <code>async</code>","text":"<p>Stream out text messages from the bot through a TextChunksIterator.</p> Source code in <code>plugins/intentional-fastapi/src/intentional_fastapi/bot_interface.py</code> <pre><code>async def handle_response_chunks(self, response: ResponseChunksIterator, event: Dict[str, Any]) -&gt; None:\n    \"\"\"\n    Stream out text messages from the bot through a TextChunksIterator.\n    \"\"\"\n    if event[\"delta\"]:\n        await response.asend(event[\"delta\"])\n</code></pre>"},{"location":"plugins/intentional-fastapi/docs/reference/#plugins.intentional-fastapi.src.intentional_fastapi.bot_interface.FastAPIBotInterface.run","title":"<code>run()</code>  <code>async</code>","text":"<p>Chooses the specific loop to use for this combination of bot and modality and kicks it off.</p> Source code in <code>plugins/intentional-fastapi/src/intentional_fastapi/bot_interface.py</code> <pre><code>async def run(self) -&gt; None:\n    \"\"\"\n    Chooses the specific loop to use for this combination of bot and modality and kicks it off.\n    \"\"\"\n    if self.modality == \"text_messages\":\n        await self._run_text_messages(self.bot)\n    elif self.modality == \"audio_stream\":\n        await self._run_audio_stream(self.bot)\n    else:\n        raise ValueError(\n            f\"Modality '{self.modality}' is not yet supported.\"\n            \"These are the supported modalities: 'text_messages', 'audio_stream'.\"\n        )\n</code></pre>"},{"location":"plugins/intentional-fastapi/docs/reference/#plugins.intentional-fastapi.src.intentional_fastapi.bot_interface.ResponseChunksIterator","title":"<code>ResponseChunksIterator</code>","text":"<p>Async iterator that collects the response chunks from the bot and streams them out.</p> Source code in <code>plugins/intentional-fastapi/src/intentional_fastapi/bot_interface.py</code> <pre><code>class ResponseChunksIterator:\n    \"\"\"\n    Async iterator that collects the response chunks from the bot and streams them out.\n    \"\"\"\n\n    def __init__(self):\n        self.buffer = []\n\n    def __aiter__(self):\n        return self\n\n    async def __anext__(self):\n        if not self.buffer:\n            raise StopAsyncIteration\n\n        next_chunk = self.buffer[0]\n        self.buffer = self.buffer[1:]\n        return next_chunk\n\n    async def asend(self, value):  # pylint: disable=missing-function-docstring\n        self.buffer.append(value)\n        return\n</code></pre>"},{"location":"plugins/intentional-openai/","title":"Intentional - OpenAI","text":"<p>Intentional plugin for OpenAI.</p>"},{"location":"plugins/intentional-openai/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Installation</li> <li>License</li> </ul>"},{"location":"plugins/intentional-openai/#installation","title":"Installation","text":"<pre><code>pip install intentional-openai\n</code></pre> <p>To use the Realtime API:</p> <pre><code>sudo apt install ffmpeg portaudio19-dev\n</code></pre>"},{"location":"plugins/intentional-openai/#license","title":"License","text":"<p><code>intentional</code> is distributed under the terms of the AGPL license. If that doesn't work for you, get in touch.</p>"},{"location":"plugins/intentional-openai/docs/reference/","title":"API Reference - OpenAI","text":""},{"location":"plugins/intentional-openai/docs/reference/#plugins.intentional-openai.src","title":"<code>src</code>","text":""},{"location":"plugins/intentional-openai/docs/reference/#plugins.intentional-openai.src.intentional_openai","title":"<code>intentional_openai</code>","text":"<p>Init file for <code>intentional_openai</code>.</p>"},{"location":"plugins/intentional-openai/docs/reference/#plugins.intentional-openai.src.intentional_openai.__about__","title":"<code>__about__</code>","text":"<p>Package descriptors for intentional-openai.</p>"},{"location":"plugins/intentional-openai/docs/reference/#plugins.intentional-openai.src.intentional_openai.chatcompletion_api","title":"<code>chatcompletion_api</code>","text":"<p>Client for OpenAI's Chat Completion API.</p>"},{"location":"plugins/intentional-openai/docs/reference/#plugins.intentional-openai.src.intentional_openai.chatcompletion_api.ChatCompletionAPIClient","title":"<code>ChatCompletionAPIClient</code>","text":"<p>               Bases: <code>LLMClient</code></p> <p>A client for interacting with the OpenAI Chat Completion API.</p> Source code in <code>plugins/intentional-openai/src/intentional_openai/chatcompletion_api.py</code> <pre><code>class ChatCompletionAPIClient(LLMClient):\n    \"\"\"\n    A client for interacting with the OpenAI Chat Completion API.\n    \"\"\"\n\n    name: str = \"openai\"\n\n    def __init__(\n        self,\n        parent: \"BotStructure\",\n        intent_router: IntentRouter,\n        config: Dict[str, Any],\n    ):\n        \"\"\"\n        A client for interacting with the OpenAI Chat Completion API.\n\n        Args:\n            parent: The parent bot structure.\n            intent_router: The intent router.\n            config: The configuration dictionary.\n        \"\"\"\n        log.debug(\"Loading ChatCompletionAPIClient from config\", llm_client_config=config)\n        super().__init__(parent, intent_router)\n\n        self.llm_name = config.get(\"name\")\n        if not self.llm_name:\n            raise ValueError(\"ChatCompletionAPIClient requires a 'name' configuration key to know which LLM to use.\")\n        if \"realtime\" in self.llm_name:\n            raise ValueError(\n                \"ChatCompletionAPIClient doesn't support Realtime API. \"\n                \"To use the Realtime API, use RealtimeAPIClient instead (client: openai_realtime)\"\n            )\n\n        self.api_key_name = config.get(\"api_key_name\", \"OPENAI_API_KEY\")\n        if not os.environ.get(self.api_key_name):\n            raise ValueError(\n                \"ChatCompletionAPIClient requires an API key to authenticate with OpenAI. \"\n                f\"The provided environment variable name ({self.api_key_name}) is not set or is empty.\"\n            )\n        self.api_key = os.environ.get(self.api_key_name)\n        self.client = openai.AsyncOpenAI(api_key=self.api_key)\n        self.system_prompt = None\n        self.tools = None\n        self.setup_initial_prompt()\n        self.conversation = [{\"role\": \"system\", \"content\": self.system_prompt}]\n\n    def setup_initial_prompt(self) -&gt; None:\n        \"\"\"\n        Setup initial prompt and tools. Used also after conversation end to reset the state.\n        \"\"\"\n        self.system_prompt = self.intent_router.get_prompt()\n        self.tools = self.intent_router.current_stage.tools\n        self.conversation: List[Dict[str, Any]] = [{\"role\": \"system\", \"content\": self.system_prompt}]\n        log.debug(\"Initial system prompt set\", system_prompt=self.system_prompt)\n\n    async def run(self) -&gt; None:\n        \"\"\"\n        Handle events from the LLM by either processing them internally or by translating them into higher-level\n        events that the BotStructure class can understand, then re-emitting them.\n        \"\"\"\n        log.debug(\"ChatCompletionAPIClient.run() is no-op for now\")\n\n    async def update_system_prompt(self) -&gt; None:\n        \"\"\"\n        Update the system prompt in the LLM.\n        \"\"\"\n        self.conversation = [{\"role\": \"system\", \"content\": self.system_prompt}] + self.conversation[1:]\n        await self.emit(\"on_system_prompt_updated\", {\"system_prompt\": self.system_prompt})\n\n    async def handle_interruption(self, lenght_to_interruption: int) -&gt; None:\n        \"\"\"\n        Handle an interruption while rendering the output to the user.\n\n        Args:\n            lenght_to_interruption: The length of the data that was produced to the user before the interruption.\n                This value could be number of characters, number of words, milliseconds, number of audio frames, etc.\n                depending on the bot structure that implements it.\n        \"\"\"\n        log.warning(\"TODO! Implement handle_interruption in ChatCompletionAPIClient\")\n\n    async def send(self, data: Dict[str, Any]) -&gt; None:\n        \"\"\"\n        Send a message to the LLM.\n        \"\"\"\n        await self.emit(\"on_llm_starts_generating_response\", {})\n\n        # Generate a response\n        message = data[\"text_message\"]\n        response = await self._send_message(message)\n\n        # Unwrap the response to make sure it contains no function calls to handle\n        call_id = \"\"\n        function_name = \"\"\n        function_args = \"\"\n        assistant_response = \"\"\n        async for r in response:\n            if not call_id:\n                call_id = r.to_dict()[\"id\"]\n            delta = r.to_dict()[\"choices\"][0][\"delta\"]\n\n            if \"tool_calls\" not in delta:\n                # If this is not a function call, just stream out\n                await self.emit(\"on_text_message_from_llm\", {\"delta\": delta.get(\"content\")})\n                assistant_response += delta.get(\"content\") or \"\"\n            else:\n                # TODO handle multiple parallel function calls\n                if delta[\"tool_calls\"][0][\"index\"] &gt; 0 or len(delta[\"tool_calls\"]) &gt; 1:\n                    log.error(\"TODO: Multiple parallel function calls not supported yet. Please open an issue.\")\n                    log.debug(\"Multiple parallel function calls\", delta=delta)\n                # Consume the response to understand which tool to call with which parameters\n                for tool_call in delta[\"tool_calls\"]:\n                    if not function_name:\n                        function_name = tool_call[\"function\"].get(\"name\")\n                    function_args += tool_call[\"function\"][\"arguments\"]\n\n        if not function_name:\n            # If there was no function call, update the conversation history and return\n            self.conversation.append(message)\n            self.conversation.append({\"role\": \"assistant\", \"content\": assistant_response})\n        else:\n            # Otherwise deal with the function call\n            await self._handle_function_call(message, call_id, function_name, function_args)\n\n        await self.emit(\"on_llm_stops_generating_response\", {})\n\n    async def _send_message(self, message: Dict[str, Any]) -&gt; AsyncGenerator[Dict[str, Any], None]:\n        \"\"\"\n        Generate a response to a message.\n\n        Args:\n            message: The message to respond to.\n        \"\"\"\n        return await self.client.chat.completions.create(\n            model=self.llm_name,\n            messages=self.conversation + [message],\n            stream=True,\n            tools=[{\"type\": \"function\", \"function\": to_openai_tool(t)} for t in self.tools.values()],\n            tool_choice=\"auto\",\n            n=1,\n        )\n\n    async def _handle_function_call(\n        self,\n        message: Dict[str, Any],\n        call_id: str,\n        function_name: str,\n        function_args: str,\n    ):\n        \"\"\"\n        Handle a function call from the LLM.\n        \"\"\"\n        log.debug(\n            \"Function call detected\",\n            function_name=function_name,\n            function_args=function_args,\n        )\n        function_args = json.loads(function_args)\n\n        # Routing function call - this is special because it should not be recorded in the conversation history\n        if function_name == self.intent_router.name:\n            await self._route(function_args)\n            # Send the same message again with the new system prompt and no trace of the routing call.\n            # We don't append the user message to the history in order to avoid message duplication.\n            await self.send({\"text_message\": message})\n\n        # Check if the conversation should end\n        elif function_name == EndConversationTool.name:\n            await self.tools[EndConversationTool.name].run()\n            self.setup_initial_prompt()\n            await self.emit(\"on_conversation_ended\", {})\n\n        else:\n            # Handle a regular function call - this one shows up in the history as normal\n            # so we start by appending the user message\n            self.conversation.append(message)\n            output = await self._call_tool(call_id, function_name, function_args)\n            await self.send(\n                {\n                    \"text_message\": {\n                        \"role\": \"tool\",\n                        \"content\": json.dumps(output),\n                        \"tool_call_id\": call_id,\n                    }\n                }\n            )\n\n    async def _route(self, routing_info: Dict[str, Any]) -&gt; None:\n        \"\"\"\n        Runs the router to determine the next system prompt and tools to use.\n        \"\"\"\n        self.system_prompt, self.tools = await self.intent_router.run(routing_info)\n        await self.update_system_prompt()\n        log.debug(\"System prompt updated\", system_prompt=self.system_prompt)\n        log.debug(\"Tools updated\", tools=self.tools)\n\n    async def _call_tool(self, call_id, function_name, function_args):\n        \"\"\"\n        Call a tool with the given arguments.\n\n        Args:\n            call_id: The ID of the tool call.\n            function_name: The name of the tool function to call.\n            function_args: The arguments to pass to the tool\n        \"\"\"\n        await self.emit(\"on_tool_invoked\", {\"name\": function_name, \"args\": function_args})\n\n        # Record the tool invocation in the conversation\n        self.conversation.append(\n            {\n                \"role\": \"assistant\",\n                \"tool_calls\": [\n                    {\n                        \"id\": call_id,\n                        \"type\": \"function\",\n                        \"function\": {\n                            \"arguments\": json.dumps(function_args),\n                            \"name\": function_name,\n                        },\n                    }\n                ],\n            }\n        )\n\n        # Get the tool output\n        if function_name not in self.tools:\n            log.debug(\"The LLM called a non-existing tool.\", tool=function_name)\n            output = f\"Tool '{function_name}' not found.\"\n        else:\n            log.debug(\"Calling tool\", function_name=function_name, function_args=function_args)\n            output = await self.tools[function_name].run(function_args)\n        log.debug(\"Tool run\", tool_output=output)\n        return output\n</code></pre>"},{"location":"plugins/intentional-openai/docs/reference/#plugins.intentional-openai.src.intentional_openai.chatcompletion_api.ChatCompletionAPIClient.__init__","title":"<code>__init__(parent, intent_router, config)</code>","text":"<p>A client for interacting with the OpenAI Chat Completion API.</p> <p>Parameters:</p> Name Type Description Default <code>parent</code> <code>BotStructure</code> <p>The parent bot structure.</p> required <code>intent_router</code> <code>IntentRouter</code> <p>The intent router.</p> required <code>config</code> <code>Dict[str, Any]</code> <p>The configuration dictionary.</p> required Source code in <code>plugins/intentional-openai/src/intentional_openai/chatcompletion_api.py</code> <pre><code>def __init__(\n    self,\n    parent: \"BotStructure\",\n    intent_router: IntentRouter,\n    config: Dict[str, Any],\n):\n    \"\"\"\n    A client for interacting with the OpenAI Chat Completion API.\n\n    Args:\n        parent: The parent bot structure.\n        intent_router: The intent router.\n        config: The configuration dictionary.\n    \"\"\"\n    log.debug(\"Loading ChatCompletionAPIClient from config\", llm_client_config=config)\n    super().__init__(parent, intent_router)\n\n    self.llm_name = config.get(\"name\")\n    if not self.llm_name:\n        raise ValueError(\"ChatCompletionAPIClient requires a 'name' configuration key to know which LLM to use.\")\n    if \"realtime\" in self.llm_name:\n        raise ValueError(\n            \"ChatCompletionAPIClient doesn't support Realtime API. \"\n            \"To use the Realtime API, use RealtimeAPIClient instead (client: openai_realtime)\"\n        )\n\n    self.api_key_name = config.get(\"api_key_name\", \"OPENAI_API_KEY\")\n    if not os.environ.get(self.api_key_name):\n        raise ValueError(\n            \"ChatCompletionAPIClient requires an API key to authenticate with OpenAI. \"\n            f\"The provided environment variable name ({self.api_key_name}) is not set or is empty.\"\n        )\n    self.api_key = os.environ.get(self.api_key_name)\n    self.client = openai.AsyncOpenAI(api_key=self.api_key)\n    self.system_prompt = None\n    self.tools = None\n    self.setup_initial_prompt()\n    self.conversation = [{\"role\": \"system\", \"content\": self.system_prompt}]\n</code></pre>"},{"location":"plugins/intentional-openai/docs/reference/#plugins.intentional-openai.src.intentional_openai.chatcompletion_api.ChatCompletionAPIClient.handle_interruption","title":"<code>handle_interruption(lenght_to_interruption)</code>  <code>async</code>","text":"<p>Handle an interruption while rendering the output to the user.</p> <p>Parameters:</p> Name Type Description Default <code>lenght_to_interruption</code> <code>int</code> <p>The length of the data that was produced to the user before the interruption. This value could be number of characters, number of words, milliseconds, number of audio frames, etc. depending on the bot structure that implements it.</p> required Source code in <code>plugins/intentional-openai/src/intentional_openai/chatcompletion_api.py</code> <pre><code>async def handle_interruption(self, lenght_to_interruption: int) -&gt; None:\n    \"\"\"\n    Handle an interruption while rendering the output to the user.\n\n    Args:\n        lenght_to_interruption: The length of the data that was produced to the user before the interruption.\n            This value could be number of characters, number of words, milliseconds, number of audio frames, etc.\n            depending on the bot structure that implements it.\n    \"\"\"\n    log.warning(\"TODO! Implement handle_interruption in ChatCompletionAPIClient\")\n</code></pre>"},{"location":"plugins/intentional-openai/docs/reference/#plugins.intentional-openai.src.intentional_openai.chatcompletion_api.ChatCompletionAPIClient.run","title":"<code>run()</code>  <code>async</code>","text":"<p>Handle events from the LLM by either processing them internally or by translating them into higher-level events that the BotStructure class can understand, then re-emitting them.</p> Source code in <code>plugins/intentional-openai/src/intentional_openai/chatcompletion_api.py</code> <pre><code>async def run(self) -&gt; None:\n    \"\"\"\n    Handle events from the LLM by either processing them internally or by translating them into higher-level\n    events that the BotStructure class can understand, then re-emitting them.\n    \"\"\"\n    log.debug(\"ChatCompletionAPIClient.run() is no-op for now\")\n</code></pre>"},{"location":"plugins/intentional-openai/docs/reference/#plugins.intentional-openai.src.intentional_openai.chatcompletion_api.ChatCompletionAPIClient.send","title":"<code>send(data)</code>  <code>async</code>","text":"<p>Send a message to the LLM.</p> Source code in <code>plugins/intentional-openai/src/intentional_openai/chatcompletion_api.py</code> <pre><code>async def send(self, data: Dict[str, Any]) -&gt; None:\n    \"\"\"\n    Send a message to the LLM.\n    \"\"\"\n    await self.emit(\"on_llm_starts_generating_response\", {})\n\n    # Generate a response\n    message = data[\"text_message\"]\n    response = await self._send_message(message)\n\n    # Unwrap the response to make sure it contains no function calls to handle\n    call_id = \"\"\n    function_name = \"\"\n    function_args = \"\"\n    assistant_response = \"\"\n    async for r in response:\n        if not call_id:\n            call_id = r.to_dict()[\"id\"]\n        delta = r.to_dict()[\"choices\"][0][\"delta\"]\n\n        if \"tool_calls\" not in delta:\n            # If this is not a function call, just stream out\n            await self.emit(\"on_text_message_from_llm\", {\"delta\": delta.get(\"content\")})\n            assistant_response += delta.get(\"content\") or \"\"\n        else:\n            # TODO handle multiple parallel function calls\n            if delta[\"tool_calls\"][0][\"index\"] &gt; 0 or len(delta[\"tool_calls\"]) &gt; 1:\n                log.error(\"TODO: Multiple parallel function calls not supported yet. Please open an issue.\")\n                log.debug(\"Multiple parallel function calls\", delta=delta)\n            # Consume the response to understand which tool to call with which parameters\n            for tool_call in delta[\"tool_calls\"]:\n                if not function_name:\n                    function_name = tool_call[\"function\"].get(\"name\")\n                function_args += tool_call[\"function\"][\"arguments\"]\n\n    if not function_name:\n        # If there was no function call, update the conversation history and return\n        self.conversation.append(message)\n        self.conversation.append({\"role\": \"assistant\", \"content\": assistant_response})\n    else:\n        # Otherwise deal with the function call\n        await self._handle_function_call(message, call_id, function_name, function_args)\n\n    await self.emit(\"on_llm_stops_generating_response\", {})\n</code></pre>"},{"location":"plugins/intentional-openai/docs/reference/#plugins.intentional-openai.src.intentional_openai.chatcompletion_api.ChatCompletionAPIClient.setup_initial_prompt","title":"<code>setup_initial_prompt()</code>","text":"<p>Setup initial prompt and tools. Used also after conversation end to reset the state.</p> Source code in <code>plugins/intentional-openai/src/intentional_openai/chatcompletion_api.py</code> <pre><code>def setup_initial_prompt(self) -&gt; None:\n    \"\"\"\n    Setup initial prompt and tools. Used also after conversation end to reset the state.\n    \"\"\"\n    self.system_prompt = self.intent_router.get_prompt()\n    self.tools = self.intent_router.current_stage.tools\n    self.conversation: List[Dict[str, Any]] = [{\"role\": \"system\", \"content\": self.system_prompt}]\n    log.debug(\"Initial system prompt set\", system_prompt=self.system_prompt)\n</code></pre>"},{"location":"plugins/intentional-openai/docs/reference/#plugins.intentional-openai.src.intentional_openai.chatcompletion_api.ChatCompletionAPIClient.update_system_prompt","title":"<code>update_system_prompt()</code>  <code>async</code>","text":"<p>Update the system prompt in the LLM.</p> Source code in <code>plugins/intentional-openai/src/intentional_openai/chatcompletion_api.py</code> <pre><code>async def update_system_prompt(self) -&gt; None:\n    \"\"\"\n    Update the system prompt in the LLM.\n    \"\"\"\n    self.conversation = [{\"role\": \"system\", \"content\": self.system_prompt}] + self.conversation[1:]\n    await self.emit(\"on_system_prompt_updated\", {\"system_prompt\": self.system_prompt})\n</code></pre>"},{"location":"plugins/intentional-openai/docs/reference/#plugins.intentional-openai.src.intentional_openai.realtime_api","title":"<code>realtime_api</code>","text":"<p>Client for OpenAI's Realtime API.</p>"},{"location":"plugins/intentional-openai/docs/reference/#plugins.intentional-openai.src.intentional_openai.realtime_api.RealtimeAPIClient","title":"<code>RealtimeAPIClient</code>","text":"<p>               Bases: <code>LLMClient</code></p> <p>A client for interacting with the OpenAI Realtime API that lets you manage the WebSocket connection, send text and audio data, and handle responses and events.</p> Source code in <code>plugins/intentional-openai/src/intentional_openai/realtime_api.py</code> <pre><code>class RealtimeAPIClient(LLMClient):\n    \"\"\"\n    A client for interacting with the OpenAI Realtime API that lets you manage the WebSocket connection, send text and\n    audio data, and handle responses and events.\n    \"\"\"\n\n    name = \"openai_realtime\"\n\n    events_translation = {\n        \"error\": \"on_error\",\n        \"response.text.delta\": \"on_text_message_from_llm\",\n        \"response.audio.delta\": \"on_audio_message_from_llm\",\n        \"response.created\": \"on_llm_starts_generating_response\",\n        \"response.done\": \"on_llm_stops_generating_response\",\n        \"input_audio_buffer.speech_started\": \"on_user_speech_started\",\n        \"input_audio_buffer.speech_stopped\": \"on_user_speech_ended\",\n        \"conversation.item.input_audio_transcription.completed\": \"on_user_speech_transcribed\",\n        \"response.audio_transcript.done\": \"on_llm_speech_transcribed\",\n    }\n\n    def __init__(self, parent: Callable, intent_router: IntentRouter, config: Dict[str, Any]):\n        \"\"\"\n        A client for interacting with the OpenAI Realtime API that lets you manage the WebSocket connection, send text\n        and audio data, and handle responses and events.\n        \"\"\"\n        log.debug(\"Loading %s from config\", self.__class__.__name__, llm_client_config=config)\n        super().__init__(parent, intent_router)\n\n        self.llm_name = config.get(\"name\")\n        if not self.llm_name:\n            raise ValueError(\"RealtimeAPIClient requires a 'name' configuration key to know which LLM to use.\")\n        if \"realtime\" not in self.llm_name:\n            raise ValueError(\n                \"RealtimeAPIClient requires a 'realtime' LLM to use the Realtime API. \"\n                \"To use any other OpenAI LLM, use the OpenAIClient instead.\"\n            )\n\n        self.api_key_name = config.get(\"api_key_name\", \"OPENAI_API_KEY\")\n        if not os.environ.get(self.api_key_name):\n            raise ValueError(\n                \"RealtimeAPIClient requires an API key to authenticate with OpenAI. \"\n                f\"The provided environment variable name ({self.api_key_name}) is not set or is empty.\"\n            )\n        self.api_key = os.environ.get(self.api_key_name)\n        self.voice = config.get(\"voice\", \"alloy\")\n\n        # WebSocket connection data\n        self.ws = None\n        self.base_url = \"wss://api.openai.com/v1/realtime\"\n\n        # Track current response state\n        self._connecting = False\n        self._updating_system_prompt = False\n        self._current_response_id = None\n        self._current_item_id = None\n\n        # Intent routering data\n        self.intent_router = intent_router\n        self.system_prompt = None\n        self.tools = None\n        self.setup_initial_prompt()\n\n    def setup_initial_prompt(self) -&gt; None:\n        \"\"\"\n        Setup initial prompt and tools. Used also after conversation end to reset the state.\n        \"\"\"\n        self.system_prompt = self.intent_router.get_prompt()\n        self.tools = self.intent_router.current_stage.tools\n\n    async def connect(self) -&gt; None:\n        \"\"\"\n        Establish WebSocket connection with the Realtime API.\n        \"\"\"\n        log.debug(\"Initializing websocket connection to OpenAI Realtime API\")\n\n        url = f\"{self.base_url}?model={self.llm_name}\"\n        headers = {\n            \"Authorization\": f\"Bearer {self.api_key}\",\n            \"OpenAI-Beta\": \"realtime=v1\",\n        }\n        self.ws = await websockets.connect(url, extra_headers=headers)\n\n        await self._update_session(\n            {\n                \"modalities\": [\"text\", \"audio\"],\n                \"instructions\": self.system_prompt,\n                \"voice\": self.voice,\n                \"input_audio_format\": \"pcm16\",\n                \"output_audio_format\": \"pcm16\",\n                \"input_audio_transcription\": {\"model\": \"whisper-1\"},\n                \"turn_detection\": {\n                    \"type\": \"server_vad\",\n                    \"threshold\": 0.5,\n                    \"prefix_padding_ms\": 500,\n                    \"silence_duration_ms\": 200,\n                },\n                \"tools\": [to_openai_tool(tool) for tool in self.tools.values()],\n                \"tool_choice\": \"auto\",\n                \"temperature\": 0.8,\n            }\n        )\n        # Flag that we're connecting and look for this event in the run loop\n        self._connecting = True\n\n    async def disconnect(self) -&gt; None:\n        \"\"\"\n        Close the WebSocket connection.\n        \"\"\"\n        if self.ws:\n            log.debug(\"Disconnecting from OpenAI Realtime API\")\n            await self.ws.close()\n        else:\n            log.debug(\"Attempted disconnection of a OpenAIRealtimeAPIClient that was never connected, nothing done.\")\n        await self.emit(\"on_llm_disconnection\", {})\n\n    async def run(self) -&gt; None:  # pylint: disable=too-many-branches, too-many-statements\n        \"\"\"\n        Handles events coming from the WebSocket connection.\n\n        This method is an infinite loop that listens for messages from the WebSocket connection and processes them\n        accordingly. It also triggers the event handlers for the corresponding event types.\n        \"\"\"\n        try:\n            async for message in self.ws:\n                event = json.loads(message)\n                event_name = event.get(\"type\")\n                log.debug(\"Received event\", event_name=event_name)\n\n                # Handle errors\n                if event_name == \"error\":\n                    log.error(\"An error response was returned\", event_data=event)\n                elif event_name == \"conversation.item.input_audio_transcription.failed\":\n                    log.error(\"An error happened during transcription\", event_data=event)\n\n                elif event_name == \"session.updated\":\n                    log.debug(\"Session configuration updated\", event_data=event)\n                    # Check why we updated the session and emit the corresponding event\n                    if self._connecting:\n                        self._connecting = False\n                        await self.emit(\"on_llm_connection\", event)\n                    if self._updating_system_prompt:\n                        self._updating_system_prompt = False\n                        await self.emit(\n                            \"on_system_prompt_updated\",\n                            {\"system_prompt\": event[\"session\"][\"instructions\"]},\n                        )\n\n                # Track agent response state\n                elif event_name == \"response.created\":\n                    self._current_response_id = event.get(\"response\", {}).get(\"id\")\n                    log.debug(\n                        \"Agent started responding. Response created.\",\n                        response_id=self._current_response_id,\n                    )\n\n                elif event_name == \"response.output_item.added\":\n                    self._current_item_id = event.get(\"item\", {}).get(\"id\")\n                    log.debug(\n                        \"Agent is responding. Added response item.\",\n                        response_id=self._current_item_id,\n                    )\n\n                elif event_name == \"response.done\":\n                    log.debug(\n                        \"Agent finished generating a response.\",\n                        response_id=self._current_item_id,\n                    )\n\n                # Tool call\n                elif event_name == \"response.function_call_arguments.done\":\n                    await self._call_tool(event)\n\n                # Events from VAD related to the user's input\n                elif event_name == \"input_audio_buffer.speech_started\":\n                    log.debug(\"Speech detected.\")\n\n                elif event_name == \"input_audio_buffer.speech_stopped\":\n                    log.debug(\"Speech ended.\")\n\n                # Decode the audio from base64\n                elif event_name == \"response.audio.delta\":\n                    audio_b64 = event.get(\"delta\", \"\")\n                    audio_bytes = base64.b64decode(audio_b64)\n                    event[\"delta\"] = audio_bytes\n\n                # Relay the event to the parent BotStructure - regardless whether it was processed above or not\n                if event_name in self.events_translation:\n                    log.debug(\n                        \"Translating event\",\n                        old_event_name=event_name,\n                        new_event_name=self.events_translation[event_name],\n                    )\n                    event[\"type\"] = self.events_translation[event_name]\n                    await self.emit(self.events_translation[event_name], event)\n                else:\n                    log.debug(\"Sending native event to parent\", event_name=event_name)\n                    await self.emit(event_name, event)\n\n        except websockets.exceptions.ConnectionClosedOK:\n            await asyncio.sleep(1)\n            log.warning(\"Connection closed.\")\n            return\n\n        except Exception:  # pylint: disable=broad-except\n            await asyncio.sleep(1)\n            log.exception(\"Error in message handling\")\n            return\n\n        log.debug(\".run() exited without errors.\")\n\n    async def send(self, data: Dict[str, Any]) -&gt; None:\n        \"\"\"\n        Stream data to the API.\n\n        Args:\n            data:\n                The data chunk to stream. It should be in the format {\"audio\": bytes}.\n        \"\"\"\n        if \"audio_chunk\" in data:\n            await self._send_audio_stream(data[\"audio_chunk\"])\n        # if \"audio_message\" in data:\n        #     await self._send_audio(data[\"audio_message\"])\n        # if \"text_message\" in data:\n        #     await self._send_text_message(data[\"text_message\"])\n\n    async def update_system_prompt(self) -&gt; None:\n        \"\"\"\n        Update the system prompt to use in the conversation.\n        \"\"\"\n        log.debug(\"Setting new system prompt\", system_prompt=self.system_prompt)\n        log.debug(\"Setting new tools\", tools=list(self.tools.keys()))\n        await self._update_session(\n            {\n                \"instructions\": self.system_prompt,\n                \"tools\": [to_openai_tool(t) for t in self.tools.values()],\n            }\n        )\n        # Flag that we're updating the system prompt and look for this event in the run loop\n        self._updating_system_prompt = True\n\n    async def handle_interruption(self, lenght_to_interruption: int) -&gt; None:\n        \"\"\"\n        Handle user interruption of the current response.\n\n        Args:\n            lenght_to_interruption (int):\n                The length in milliseconds of the audio that was played to the user before the interruption.\n                May be zero if the interruption happened before any audio was played.\n        \"\"\"\n        log.info(\n            \"[Handling interruption at %s ms]\",\n            lenght_to_interruption,\n            interruption_time=lenght_to_interruption,\n        )\n\n        # Cancel the current response\n        # Cancelling responses is effective when the response is still being generated by the LLM.\n        if self._current_response_id:\n            log.debug(\n                \"Cancelling response due to a user's interruption.\",\n                response_id=self._current_response_id,\n            )\n            event = {\"type\": \"response.cancel\"}\n            await self.ws.send(json.dumps(event))\n        else:\n            log.warning(\"No response ID found to cancel.\")\n\n        # Truncate the conversation item to what was actually played\n        # Truncating the response is effective when the response has already been generated by the LLM and is being\n        # played out.\n        if lenght_to_interruption:\n            log.debug(\n                \"Truncating the response due to a user's interruption at %s ms\",\n                lenght_to_interruption,\n                interruption_time=lenght_to_interruption,\n            )\n            event = {\n                \"type\": \"conversation.item.truncate\",\n                \"item_id\": self._current_item_id,\n                \"content_index\": 0,\n                \"audio_end_ms\": math.floor(lenght_to_interruption),\n            }\n            await self.ws.send(json.dumps(event))\n\n    async def _update_session(self, config: Dict[str, Any]) -&gt; None:\n        \"\"\"\n        Update session configuration.\n\n        Args:\n            config (Dict[str, Any]):\n                The new session configuration.\n        \"\"\"\n        event = {\"type\": \"session.update\", \"session\": config}\n        await self.ws.send(json.dumps(event))\n\n    async def _send_text_message(self, text: str) -&gt; None:\n        \"\"\"\n        Send text message to the API.\n\n        Args:\n            text (str):\n                The text message to send.\n        \"\"\"\n        event = {\n            \"type\": \"conversation.item.create\",\n            \"item\": {\n                \"type\": \"message\",\n                \"role\": \"user\",\n                \"content\": [{\"type\": \"input_text\", \"text\": text}],\n            },\n        }\n        await self.ws.send(json.dumps(event))\n        await self._request_response_from_llm()\n\n    async def _send_audio_stream(self, audio_bytes: bytes) -&gt; None:\n        audio_b64 = base64.b64encode(audio_bytes).decode()\n        append_event = {\"type\": \"input_audio_buffer.append\", \"audio\": audio_b64}\n        await self.ws.send(json.dumps(append_event))\n\n    # async def _send_audio_message(self, audio_bytes: bytes) -&gt; None:\n    #     \"\"\"\n    #     Send audio data to the API.\n\n    #     Args:\n    #         audio_bytes (bytes):\n    #             The audio data to send.\n    #     \"\"\"\n    #     # Convert audio to required format (24kHz, mono, PCM16)\n    #     audio = AudioSegment.from_file(io.BytesIO(audio_bytes))\n    #     audio = audio.set_frame_rate(24000).set_channels(1).set_sample_width(2)\n    #     pcm_data = base64.b64encode(audio.raw_data).decode()\n\n    #     # Append audio to buffer\n    #     append_event = {\"type\": \"input_audio_buffer.append\", \"audio\": pcm_data}\n    #     await self.ws.send(json.dumps(append_event))\n\n    #     # Commit the buffer\n    #     commit_event = {\"type\": \"input_audio_buffer.commit\"}\n    #     await self.ws.send(json.dumps(commit_event))\n\n    async def _send_function_result(self, call_id: str, result: Any) -&gt; None:\n        \"\"\"\n        Send function call result back to the API.\n\n        Args:\n            call_id (str):\n                The ID of the function call.\n            result (Any):\n                The result of the function call.\n        \"\"\"\n        event = {\n            \"type\": \"conversation.item.create\",\n            \"item\": {\n                \"type\": \"function_call_output\",\n                \"call_id\": call_id,\n                \"output\": result,\n            },\n        }\n        await self.ws.send(json.dumps(event))\n        await self._request_response_from_llm()\n\n    async def _request_response_from_llm(self) -&gt; None:\n        \"\"\"\n        Asks the LLM for a response to the messages it just received.\n        You need to call this function right after sending a messages that is not streamed like the audio (where the\n        LLM's VAD would decide when to reply instead).\n        \"\"\"\n        event = {\n            \"type\": \"response.create\",\n            \"response\": {\"modalities\": [\"text\", \"audio\"]},\n        }\n        await self.ws.send(json.dumps(event))\n\n    async def _call_tool(self, event: Dict[str, Any]) -&gt; None:\n        \"\"\"\n        Calls the tool requested by the LLM.\n\n        Args:\n            event (Dict[str, Any]):\n                The event containing the tool call information.\n        \"\"\"\n        call_id = event[\"call_id\"]\n        tool_name = event[\"name\"]\n        tool_arguments = json.loads(event[\"arguments\"])\n        log.debug(\n            \"Calling tool\",\n            tool_name=tool_name,\n            tool_arguments=tool_arguments,\n            call_id=call_id,\n        )\n\n        # Check if it's the router\n        if tool_name == self.intent_router.name:\n            self.system_prompt, self.tools = await self.intent_router.run(tool_arguments)\n            await self.update_system_prompt()\n            await self._send_function_result(event[\"call_id\"], \"ok\")\n            return\n\n        # Check if the conversation should end\n        if tool_name == EndConversationTool.name:\n            await self.tools[EndConversationTool.name].run()\n            # await self.disconnect()\n            # self.setup_initial_prompt()\n            await self.emit(\"on_conversation_ended\", {})\n            # await self.connect()\n            return\n\n        # Emit the event\n        self.emit(\"on_tool_invoked\", {\"name\": tool_name, \"args\": tool_arguments})\n\n        # Make sure the tool actually exists\n        if tool_name not in self.tools:\n            log.error(\"Tool '%s' not found in the list of available tools.\", tool_name)\n            await self._send_function_result(call_id, f\"Error: Tool {tool_name} not found\")\n\n        # Invoke the tool and send back the output\n        result = await self.tools.get(tool_name).run(tool_arguments)\n        log.debug(\"Tool run\", tool_name=tool_name, tool_output=result)\n        await self._send_function_result(call_id, str(result))\n</code></pre>"},{"location":"plugins/intentional-openai/docs/reference/#plugins.intentional-openai.src.intentional_openai.realtime_api.RealtimeAPIClient.__init__","title":"<code>__init__(parent, intent_router, config)</code>","text":"<p>A client for interacting with the OpenAI Realtime API that lets you manage the WebSocket connection, send text and audio data, and handle responses and events.</p> Source code in <code>plugins/intentional-openai/src/intentional_openai/realtime_api.py</code> <pre><code>def __init__(self, parent: Callable, intent_router: IntentRouter, config: Dict[str, Any]):\n    \"\"\"\n    A client for interacting with the OpenAI Realtime API that lets you manage the WebSocket connection, send text\n    and audio data, and handle responses and events.\n    \"\"\"\n    log.debug(\"Loading %s from config\", self.__class__.__name__, llm_client_config=config)\n    super().__init__(parent, intent_router)\n\n    self.llm_name = config.get(\"name\")\n    if not self.llm_name:\n        raise ValueError(\"RealtimeAPIClient requires a 'name' configuration key to know which LLM to use.\")\n    if \"realtime\" not in self.llm_name:\n        raise ValueError(\n            \"RealtimeAPIClient requires a 'realtime' LLM to use the Realtime API. \"\n            \"To use any other OpenAI LLM, use the OpenAIClient instead.\"\n        )\n\n    self.api_key_name = config.get(\"api_key_name\", \"OPENAI_API_KEY\")\n    if not os.environ.get(self.api_key_name):\n        raise ValueError(\n            \"RealtimeAPIClient requires an API key to authenticate with OpenAI. \"\n            f\"The provided environment variable name ({self.api_key_name}) is not set or is empty.\"\n        )\n    self.api_key = os.environ.get(self.api_key_name)\n    self.voice = config.get(\"voice\", \"alloy\")\n\n    # WebSocket connection data\n    self.ws = None\n    self.base_url = \"wss://api.openai.com/v1/realtime\"\n\n    # Track current response state\n    self._connecting = False\n    self._updating_system_prompt = False\n    self._current_response_id = None\n    self._current_item_id = None\n\n    # Intent routering data\n    self.intent_router = intent_router\n    self.system_prompt = None\n    self.tools = None\n    self.setup_initial_prompt()\n</code></pre>"},{"location":"plugins/intentional-openai/docs/reference/#plugins.intentional-openai.src.intentional_openai.realtime_api.RealtimeAPIClient.connect","title":"<code>connect()</code>  <code>async</code>","text":"<p>Establish WebSocket connection with the Realtime API.</p> Source code in <code>plugins/intentional-openai/src/intentional_openai/realtime_api.py</code> <pre><code>async def connect(self) -&gt; None:\n    \"\"\"\n    Establish WebSocket connection with the Realtime API.\n    \"\"\"\n    log.debug(\"Initializing websocket connection to OpenAI Realtime API\")\n\n    url = f\"{self.base_url}?model={self.llm_name}\"\n    headers = {\n        \"Authorization\": f\"Bearer {self.api_key}\",\n        \"OpenAI-Beta\": \"realtime=v1\",\n    }\n    self.ws = await websockets.connect(url, extra_headers=headers)\n\n    await self._update_session(\n        {\n            \"modalities\": [\"text\", \"audio\"],\n            \"instructions\": self.system_prompt,\n            \"voice\": self.voice,\n            \"input_audio_format\": \"pcm16\",\n            \"output_audio_format\": \"pcm16\",\n            \"input_audio_transcription\": {\"model\": \"whisper-1\"},\n            \"turn_detection\": {\n                \"type\": \"server_vad\",\n                \"threshold\": 0.5,\n                \"prefix_padding_ms\": 500,\n                \"silence_duration_ms\": 200,\n            },\n            \"tools\": [to_openai_tool(tool) for tool in self.tools.values()],\n            \"tool_choice\": \"auto\",\n            \"temperature\": 0.8,\n        }\n    )\n    # Flag that we're connecting and look for this event in the run loop\n    self._connecting = True\n</code></pre>"},{"location":"plugins/intentional-openai/docs/reference/#plugins.intentional-openai.src.intentional_openai.realtime_api.RealtimeAPIClient.disconnect","title":"<code>disconnect()</code>  <code>async</code>","text":"<p>Close the WebSocket connection.</p> Source code in <code>plugins/intentional-openai/src/intentional_openai/realtime_api.py</code> <pre><code>async def disconnect(self) -&gt; None:\n    \"\"\"\n    Close the WebSocket connection.\n    \"\"\"\n    if self.ws:\n        log.debug(\"Disconnecting from OpenAI Realtime API\")\n        await self.ws.close()\n    else:\n        log.debug(\"Attempted disconnection of a OpenAIRealtimeAPIClient that was never connected, nothing done.\")\n    await self.emit(\"on_llm_disconnection\", {})\n</code></pre>"},{"location":"plugins/intentional-openai/docs/reference/#plugins.intentional-openai.src.intentional_openai.realtime_api.RealtimeAPIClient.handle_interruption","title":"<code>handle_interruption(lenght_to_interruption)</code>  <code>async</code>","text":"<p>Handle user interruption of the current response.</p> <p>Parameters:</p> Name Type Description Default <code>lenght_to_interruption</code> <code>int</code> <p>The length in milliseconds of the audio that was played to the user before the interruption. May be zero if the interruption happened before any audio was played.</p> required Source code in <code>plugins/intentional-openai/src/intentional_openai/realtime_api.py</code> <pre><code>async def handle_interruption(self, lenght_to_interruption: int) -&gt; None:\n    \"\"\"\n    Handle user interruption of the current response.\n\n    Args:\n        lenght_to_interruption (int):\n            The length in milliseconds of the audio that was played to the user before the interruption.\n            May be zero if the interruption happened before any audio was played.\n    \"\"\"\n    log.info(\n        \"[Handling interruption at %s ms]\",\n        lenght_to_interruption,\n        interruption_time=lenght_to_interruption,\n    )\n\n    # Cancel the current response\n    # Cancelling responses is effective when the response is still being generated by the LLM.\n    if self._current_response_id:\n        log.debug(\n            \"Cancelling response due to a user's interruption.\",\n            response_id=self._current_response_id,\n        )\n        event = {\"type\": \"response.cancel\"}\n        await self.ws.send(json.dumps(event))\n    else:\n        log.warning(\"No response ID found to cancel.\")\n\n    # Truncate the conversation item to what was actually played\n    # Truncating the response is effective when the response has already been generated by the LLM and is being\n    # played out.\n    if lenght_to_interruption:\n        log.debug(\n            \"Truncating the response due to a user's interruption at %s ms\",\n            lenght_to_interruption,\n            interruption_time=lenght_to_interruption,\n        )\n        event = {\n            \"type\": \"conversation.item.truncate\",\n            \"item_id\": self._current_item_id,\n            \"content_index\": 0,\n            \"audio_end_ms\": math.floor(lenght_to_interruption),\n        }\n        await self.ws.send(json.dumps(event))\n</code></pre>"},{"location":"plugins/intentional-openai/docs/reference/#plugins.intentional-openai.src.intentional_openai.realtime_api.RealtimeAPIClient.run","title":"<code>run()</code>  <code>async</code>","text":"<p>Handles events coming from the WebSocket connection.</p> <p>This method is an infinite loop that listens for messages from the WebSocket connection and processes them accordingly. It also triggers the event handlers for the corresponding event types.</p> Source code in <code>plugins/intentional-openai/src/intentional_openai/realtime_api.py</code> <pre><code>async def run(self) -&gt; None:  # pylint: disable=too-many-branches, too-many-statements\n    \"\"\"\n    Handles events coming from the WebSocket connection.\n\n    This method is an infinite loop that listens for messages from the WebSocket connection and processes them\n    accordingly. It also triggers the event handlers for the corresponding event types.\n    \"\"\"\n    try:\n        async for message in self.ws:\n            event = json.loads(message)\n            event_name = event.get(\"type\")\n            log.debug(\"Received event\", event_name=event_name)\n\n            # Handle errors\n            if event_name == \"error\":\n                log.error(\"An error response was returned\", event_data=event)\n            elif event_name == \"conversation.item.input_audio_transcription.failed\":\n                log.error(\"An error happened during transcription\", event_data=event)\n\n            elif event_name == \"session.updated\":\n                log.debug(\"Session configuration updated\", event_data=event)\n                # Check why we updated the session and emit the corresponding event\n                if self._connecting:\n                    self._connecting = False\n                    await self.emit(\"on_llm_connection\", event)\n                if self._updating_system_prompt:\n                    self._updating_system_prompt = False\n                    await self.emit(\n                        \"on_system_prompt_updated\",\n                        {\"system_prompt\": event[\"session\"][\"instructions\"]},\n                    )\n\n            # Track agent response state\n            elif event_name == \"response.created\":\n                self._current_response_id = event.get(\"response\", {}).get(\"id\")\n                log.debug(\n                    \"Agent started responding. Response created.\",\n                    response_id=self._current_response_id,\n                )\n\n            elif event_name == \"response.output_item.added\":\n                self._current_item_id = event.get(\"item\", {}).get(\"id\")\n                log.debug(\n                    \"Agent is responding. Added response item.\",\n                    response_id=self._current_item_id,\n                )\n\n            elif event_name == \"response.done\":\n                log.debug(\n                    \"Agent finished generating a response.\",\n                    response_id=self._current_item_id,\n                )\n\n            # Tool call\n            elif event_name == \"response.function_call_arguments.done\":\n                await self._call_tool(event)\n\n            # Events from VAD related to the user's input\n            elif event_name == \"input_audio_buffer.speech_started\":\n                log.debug(\"Speech detected.\")\n\n            elif event_name == \"input_audio_buffer.speech_stopped\":\n                log.debug(\"Speech ended.\")\n\n            # Decode the audio from base64\n            elif event_name == \"response.audio.delta\":\n                audio_b64 = event.get(\"delta\", \"\")\n                audio_bytes = base64.b64decode(audio_b64)\n                event[\"delta\"] = audio_bytes\n\n            # Relay the event to the parent BotStructure - regardless whether it was processed above or not\n            if event_name in self.events_translation:\n                log.debug(\n                    \"Translating event\",\n                    old_event_name=event_name,\n                    new_event_name=self.events_translation[event_name],\n                )\n                event[\"type\"] = self.events_translation[event_name]\n                await self.emit(self.events_translation[event_name], event)\n            else:\n                log.debug(\"Sending native event to parent\", event_name=event_name)\n                await self.emit(event_name, event)\n\n    except websockets.exceptions.ConnectionClosedOK:\n        await asyncio.sleep(1)\n        log.warning(\"Connection closed.\")\n        return\n\n    except Exception:  # pylint: disable=broad-except\n        await asyncio.sleep(1)\n        log.exception(\"Error in message handling\")\n        return\n\n    log.debug(\".run() exited without errors.\")\n</code></pre>"},{"location":"plugins/intentional-openai/docs/reference/#plugins.intentional-openai.src.intentional_openai.realtime_api.RealtimeAPIClient.send","title":"<code>send(data)</code>  <code>async</code>","text":"<p>Stream data to the API.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Any]</code> <p>The data chunk to stream. It should be in the format {\"audio\": bytes}.</p> required Source code in <code>plugins/intentional-openai/src/intentional_openai/realtime_api.py</code> <pre><code>async def send(self, data: Dict[str, Any]) -&gt; None:\n    \"\"\"\n    Stream data to the API.\n\n    Args:\n        data:\n            The data chunk to stream. It should be in the format {\"audio\": bytes}.\n    \"\"\"\n    if \"audio_chunk\" in data:\n        await self._send_audio_stream(data[\"audio_chunk\"])\n</code></pre>"},{"location":"plugins/intentional-openai/docs/reference/#plugins.intentional-openai.src.intentional_openai.realtime_api.RealtimeAPIClient.setup_initial_prompt","title":"<code>setup_initial_prompt()</code>","text":"<p>Setup initial prompt and tools. Used also after conversation end to reset the state.</p> Source code in <code>plugins/intentional-openai/src/intentional_openai/realtime_api.py</code> <pre><code>def setup_initial_prompt(self) -&gt; None:\n    \"\"\"\n    Setup initial prompt and tools. Used also after conversation end to reset the state.\n    \"\"\"\n    self.system_prompt = self.intent_router.get_prompt()\n    self.tools = self.intent_router.current_stage.tools\n</code></pre>"},{"location":"plugins/intentional-openai/docs/reference/#plugins.intentional-openai.src.intentional_openai.realtime_api.RealtimeAPIClient.update_system_prompt","title":"<code>update_system_prompt()</code>  <code>async</code>","text":"<p>Update the system prompt to use in the conversation.</p> Source code in <code>plugins/intentional-openai/src/intentional_openai/realtime_api.py</code> <pre><code>async def update_system_prompt(self) -&gt; None:\n    \"\"\"\n    Update the system prompt to use in the conversation.\n    \"\"\"\n    log.debug(\"Setting new system prompt\", system_prompt=self.system_prompt)\n    log.debug(\"Setting new tools\", tools=list(self.tools.keys()))\n    await self._update_session(\n        {\n            \"instructions\": self.system_prompt,\n            \"tools\": [to_openai_tool(t) for t in self.tools.values()],\n        }\n    )\n    # Flag that we're updating the system prompt and look for this event in the run loop\n    self._updating_system_prompt = True\n</code></pre>"},{"location":"plugins/intentional-openai/docs/reference/#plugins.intentional-openai.src.intentional_openai.tools","title":"<code>tools</code>","text":"<p>Tool utilities to interact with tools in OpenAI.</p>"},{"location":"plugins/intentional-openai/docs/reference/#plugins.intentional-openai.src.intentional_openai.tools.to_openai_tool","title":"<code>to_openai_tool(tool)</code>","text":"<p>The tool definition required by OpenAI.</p> Source code in <code>plugins/intentional-openai/src/intentional_openai/tools.py</code> <pre><code>def to_openai_tool(tool: Tool):\n    \"\"\"\n    The tool definition required by OpenAI.\n    \"\"\"\n    return {\n        \"type\": \"function\",\n        \"name\": tool.name,\n        \"description\": tool.description,\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                param.name: {\n                    \"description\": param.description,\n                    \"type\": param.type,\n                    \"default\": param.default,\n                }\n                for param in tool.parameters\n            },\n            \"required\": [param.name for param in tool.parameters if param.required],\n        },\n    }\n</code></pre>"},{"location":"plugins/intentional-pipecat/","title":"Intentional - Pipecat","text":"<p>Plugin that lets you build transcribed audio voice bots using Pipecat.</p>"},{"location":"plugins/intentional-pipecat/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Installation</li> <li>License</li> </ul>"},{"location":"plugins/intentional-pipecat/#installation","title":"Installation","text":"<p>You can install <code>intentional-pipecat</code> without extras:</p> <pre><code>pip install intentional-pipecat\n</code></pre> <p>However, it will be unusable as-is, because it will come with nearly no dependencies.</p> <p>In order to have any of the underlying dependencies installed (such as Silero's VAD, Deepgram, OpenAI, etc...) you must specify them as extras, like:</p> <pre><code>pip install intentional-pipecat[silero,deepgram,openai]\n</code></pre> <p>For a list of all the available extras, see Pipecat's documentation or their <code>pyproject.toml</code> file. We are going to try to keep this list up-to-date on Intentional's end, but in case of issues you can also install those dependencies by hand by doing:</p> <pre><code>pip install pipecat-ai[&lt;the extras you want&gt;]\n</code></pre> <p>This guarantees that you get the correct extras for your version of Pipecat. Please open an issue if you find any discrepancies.</p>"},{"location":"plugins/intentional-pipecat/#usage","title":"Usage","text":"<p>In order to use Pipecat, you need to specify which components you want to use in the configuration file. For example:</p> <pre><code>bot:\n  type: pipecat\n  llm:\n    client: openai\n    name: gpt-4o\n  vad:\n    module: silero\n    client: SileroVADAnalyzer\n  stt:\n    module: deepgram\n    client: DeepgramSTTService\n  tts:\n    module: azure\n    client: AzureTTSService\n</code></pre> <p>This example would require the extras <code>silero</code>, <code>deepgram</code> and <code>azure</code>:</p> <pre><code>pip install intentional-pipecat[silero,deepgram,azure]\n</code></pre> <p>See Pipecat's documentation for more information about what modules and classes are available for the various pipeline components.</p> <p>At this time, the Pipeline structure itself cannot be configured. Open an issue if you'd like to allow some degree of pipeline customization.</p>"},{"location":"plugins/intentional-pipecat/#license","title":"License","text":"<p><code>intentional</code> is distributed under the terms of the AGPL license. If that doesn't work for you, get in touch.</p>"},{"location":"plugins/intentional-pipecat/docs/reference/","title":"API Reference - Pipecat","text":""},{"location":"plugins/intentional-pipecat/docs/reference/#plugins.intentional-pipecat.src","title":"<code>src</code>","text":""},{"location":"plugins/intentional-pipecat/docs/reference/#plugins.intentional-pipecat.src.intentional_pipecat","title":"<code>intentional_pipecat</code>","text":"<p>Intentional plugin for Pipecat</p>"},{"location":"plugins/intentional-pipecat/docs/reference/#plugins.intentional-pipecat.src.intentional_pipecat.__about__","title":"<code>__about__</code>","text":"<p>Package descriptors for intentional-pipecat.</p>"},{"location":"plugins/intentional-pipecat/docs/reference/#plugins.intentional-pipecat.src.intentional_pipecat.bot_structure","title":"<code>bot_structure</code>","text":"<p>Pipecat bot structure implementation.</p>"},{"location":"plugins/intentional-pipecat/docs/reference/#plugins.intentional-pipecat.src.intentional_pipecat.bot_structure.PipecatBotStructure","title":"<code>PipecatBotStructure</code>","text":"<p>               Bases: <code>BotStructure</code></p> <p>Bot structure that uses Pipecat to make text-only models able to handle spoken input.</p> Source code in <code>plugins/intentional-pipecat/src/intentional_pipecat/bot_structure.py</code> <pre><code>class PipecatBotStructure(BotStructure):  # pylint: disable=too-many-instance-attributes\n    \"\"\"\n    Bot structure that uses Pipecat to make text-only models able to handle spoken input.\n    \"\"\"\n\n    name = \"pipecat\"\n\n    def __init__(self, config: Dict[str, Any], intent_router: IntentRouter):\n        \"\"\"\n        Args:\n            config:\n                The configuration dictionary for the bot structure.\n                It includes only the LLM definition under the `llm` key.\n        \"\"\"\n        super().__init__()\n        log.debug(\"Loading bot structure from config\", bot_structure_config=config)\n\n        # Init the model client\n        llm_config = config.pop(\"llm\", None)\n        if not llm_config:\n            raise ValueError(f\"{self.__class__.__name__} requires a 'llm' configuration key.\")\n        self.llm: LLMClient = load_llm_client_from_dict(parent=self, intent_router=intent_router, config=llm_config)\n\n        # Import the correct VAD, STT and TTS clients from Pipecat\n        self.vad_class, self.vad_params = self._load_class_from_config(\n            config.pop(\"vad\", None), \"vad\", {\"start_secs\": 0.1, \"stop_secs\": 0.1, \"min_volume\": 0.6}\n        )\n        self.stt_class, self.stt_params = self._load_class_from_config(\n            config.pop(\"stt\", None), \"stt\", {\"sample_rate\": 16000}\n        )\n        self.tts_class, self.tts_params = self._load_class_from_config(config.pop(\"tts\", None), \"tts\", {})\n\n        # Pipecat pipeline\n        self.pipecat_task = None\n        self.publisher = None\n        self.transport = None\n        self.assistant_reply = \"\"\n\n    def _load_class_from_config(self, config: Dict[str, Any], key: str, defaults: Optional[Dict[str, Any]] = None):\n        if not config:\n            raise ValueError(f\"{self.__class__.__name__} requires a '{key}' configuration key.\")\n        module = importlib.import_module(PIPECAT_MODULES_FOR_KEY[key] + config[\"module\"])\n        class_ = getattr(module, config[\"class\"])\n        params = config.get(\"params\", {})\n\n        # Load env vars if necessary\n        usable_params = defaults or {}\n        for param_key in params.keys():\n            if param_key.endswith(\"__envvar\"):\n                usable_params[param_key.removesuffix(\"__envvar\")] = os.getenv(params[param_key])\n            else:\n                usable_params[param_key] = params[param_key]\n\n        return class_, usable_params\n\n    async def connect(self) -&gt; None:\n        \"\"\"\n        Initializes the model and connects to it as/if necessary.\n        \"\"\"\n        # Prepares the Pipecat pipeline\n        transport_params = TransportParams(\n            audio_in_enabled=True,\n            audio_out_enabled=True,\n            transcription_enabled=True,\n            vad_enabled=True,\n            vad_analyzer=self.vad_class(params=VADParams(**self.vad_params)),\n            vad_audio_passthrough=True,\n        )\n        self.transport = AudioTransport(transport_params, self.llm.emit)\n\n        stt = self.stt_class(**self.stt_params)\n        tts = self.tts_class(**self.tts_params)\n        user_response = LLMUserResponseAggregator()\n        send_to_llm = UserToLLMFrameProcessor(self.llm)\n        self.publisher = LLMToUserFrameProcessor()\n        pipeline = Pipeline(\n            [\n                self.transport.input(),\n                stt,\n                user_response,\n                send_to_llm,\n                self.publisher,\n                tts,\n                self.transport.output(),\n            ]\n        )\n        self.pipecat_task = PipelineTask(pipeline, PipelineParams(allow_interruptions=True))\n\n        self.add_event_handler(\"on_text_message_from_llm\", self.handle_llm_text_messages)\n        self.add_event_handler(\"on_llm_starts_generating_response\", self.handle_llm_starts_generating_response)\n        self.add_event_handler(\"on_llm_stops_generating_response\", self.handle_llm_stops_generating_response)\n        # Start the pipeline\n        asyncio.create_task(self.pipecat_task.run())\n        # Wait for the pipeline to actually connect and start\n        while True:\n            if self.transport.input().ready:\n                break\n            await asyncio.sleep(0.1)\n        # Connects to the model, if necessary\n        await self.llm.connect()\n\n    async def handle_llm_text_messages(self, event: Dict[str, Any]) -&gt; None:\n        \"\"\"\n        Sends the text message to the Pipecat pipeline to be converted into audio.\n        \"\"\"\n        if event[\"delta\"]:\n            await self.publisher.push_frame(TextFrame(event[\"delta\"]), FrameDirection.DOWNSTREAM)\n            self.assistant_reply += event[\"delta\"]\n\n    async def handle_llm_starts_generating_response(self, _: Dict[str, Any]) -&gt; None:\n        \"\"\"\n        Warns the Pipecat pipeline of the start of a response from the LLM by sending an LLMFullResponseStartFrame()\n        \"\"\"\n        await self.publisher.push_frame(LLMFullResponseStartFrame(), FrameDirection.DOWNSTREAM)\n\n    async def handle_llm_stops_generating_response(self, _: Dict[str, Any]) -&gt; None:\n        \"\"\"\n        Warns the Pipecat pipeline of the end of a response from the LLM by sending an LLMFullResponseEndFrame()\n        \"\"\"\n        await self.publisher.push_frame(LLMFullResponseEndFrame(), FrameDirection.DOWNSTREAM)\n        if self.assistant_reply:\n            await self.llm.emit(\"on_llm_speech_transcribed\", {\"type\": \"assistant\", \"transcript\": self.assistant_reply})\n            self.assistant_reply = \"\"\n\n    async def disconnect(self) -&gt; None:\n        \"\"\"\n        Disconnects from the LLM and unloads/closes it as/if necessary.\n        \"\"\"\n        await self.llm.disconnect()\n\n    async def run(self) -&gt; None:\n        \"\"\"\n        Main loop for the bot.\n        \"\"\"\n        log.debug(\".run() is no-op for PipecatBotStructure, the Pipecat pipeline is self-sufficient.\")\n\n    async def send(self, data: Dict[str, Any]) -&gt; AsyncGenerator[Dict[str, Any], None]:\n        \"\"\"\n        Sends a message to the LLM and forward the response.\n\n        Args:\n            data: The message to send to the model in OpenAI format, like {\"role\": \"user\", \"content\": \"Hello!\"}\n        \"\"\"\n        if \"audio_chunk\" in data:\n            await self.transport.input().send_audio_frame(data[\"audio_chunk\"])\n        else:\n            raise ValueError(\"PipecatBotStructure only supports audio data for now.\")\n\n    async def handle_interruption(self, lenght_to_interruption: int) -&gt; None:\n        \"\"\"\n        Handle an interruption in the streaming.\n\n        Args:\n            lenght_to_interruption: The length of the data that was produced to the user before the interruption.\n                This value could be number of characters, number of words, milliseconds, number of audio frames, etc.\n                depending on the bot structure that implements it.\n        \"\"\"\n        log.warning(\"handle interruptions: TODO\")\n</code></pre>"},{"location":"plugins/intentional-pipecat/docs/reference/#plugins.intentional-pipecat.src.intentional_pipecat.bot_structure.PipecatBotStructure.__init__","title":"<code>__init__(config, intent_router)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>config</code> <code>Dict[str, Any]</code> <p>The configuration dictionary for the bot structure. It includes only the LLM definition under the <code>llm</code> key.</p> required Source code in <code>plugins/intentional-pipecat/src/intentional_pipecat/bot_structure.py</code> <pre><code>def __init__(self, config: Dict[str, Any], intent_router: IntentRouter):\n    \"\"\"\n    Args:\n        config:\n            The configuration dictionary for the bot structure.\n            It includes only the LLM definition under the `llm` key.\n    \"\"\"\n    super().__init__()\n    log.debug(\"Loading bot structure from config\", bot_structure_config=config)\n\n    # Init the model client\n    llm_config = config.pop(\"llm\", None)\n    if not llm_config:\n        raise ValueError(f\"{self.__class__.__name__} requires a 'llm' configuration key.\")\n    self.llm: LLMClient = load_llm_client_from_dict(parent=self, intent_router=intent_router, config=llm_config)\n\n    # Import the correct VAD, STT and TTS clients from Pipecat\n    self.vad_class, self.vad_params = self._load_class_from_config(\n        config.pop(\"vad\", None), \"vad\", {\"start_secs\": 0.1, \"stop_secs\": 0.1, \"min_volume\": 0.6}\n    )\n    self.stt_class, self.stt_params = self._load_class_from_config(\n        config.pop(\"stt\", None), \"stt\", {\"sample_rate\": 16000}\n    )\n    self.tts_class, self.tts_params = self._load_class_from_config(config.pop(\"tts\", None), \"tts\", {})\n\n    # Pipecat pipeline\n    self.pipecat_task = None\n    self.publisher = None\n    self.transport = None\n    self.assistant_reply = \"\"\n</code></pre>"},{"location":"plugins/intentional-pipecat/docs/reference/#plugins.intentional-pipecat.src.intentional_pipecat.bot_structure.PipecatBotStructure.connect","title":"<code>connect()</code>  <code>async</code>","text":"<p>Initializes the model and connects to it as/if necessary.</p> Source code in <code>plugins/intentional-pipecat/src/intentional_pipecat/bot_structure.py</code> <pre><code>async def connect(self) -&gt; None:\n    \"\"\"\n    Initializes the model and connects to it as/if necessary.\n    \"\"\"\n    # Prepares the Pipecat pipeline\n    transport_params = TransportParams(\n        audio_in_enabled=True,\n        audio_out_enabled=True,\n        transcription_enabled=True,\n        vad_enabled=True,\n        vad_analyzer=self.vad_class(params=VADParams(**self.vad_params)),\n        vad_audio_passthrough=True,\n    )\n    self.transport = AudioTransport(transport_params, self.llm.emit)\n\n    stt = self.stt_class(**self.stt_params)\n    tts = self.tts_class(**self.tts_params)\n    user_response = LLMUserResponseAggregator()\n    send_to_llm = UserToLLMFrameProcessor(self.llm)\n    self.publisher = LLMToUserFrameProcessor()\n    pipeline = Pipeline(\n        [\n            self.transport.input(),\n            stt,\n            user_response,\n            send_to_llm,\n            self.publisher,\n            tts,\n            self.transport.output(),\n        ]\n    )\n    self.pipecat_task = PipelineTask(pipeline, PipelineParams(allow_interruptions=True))\n\n    self.add_event_handler(\"on_text_message_from_llm\", self.handle_llm_text_messages)\n    self.add_event_handler(\"on_llm_starts_generating_response\", self.handle_llm_starts_generating_response)\n    self.add_event_handler(\"on_llm_stops_generating_response\", self.handle_llm_stops_generating_response)\n    # Start the pipeline\n    asyncio.create_task(self.pipecat_task.run())\n    # Wait for the pipeline to actually connect and start\n    while True:\n        if self.transport.input().ready:\n            break\n        await asyncio.sleep(0.1)\n    # Connects to the model, if necessary\n    await self.llm.connect()\n</code></pre>"},{"location":"plugins/intentional-pipecat/docs/reference/#plugins.intentional-pipecat.src.intentional_pipecat.bot_structure.PipecatBotStructure.disconnect","title":"<code>disconnect()</code>  <code>async</code>","text":"<p>Disconnects from the LLM and unloads/closes it as/if necessary.</p> Source code in <code>plugins/intentional-pipecat/src/intentional_pipecat/bot_structure.py</code> <pre><code>async def disconnect(self) -&gt; None:\n    \"\"\"\n    Disconnects from the LLM and unloads/closes it as/if necessary.\n    \"\"\"\n    await self.llm.disconnect()\n</code></pre>"},{"location":"plugins/intentional-pipecat/docs/reference/#plugins.intentional-pipecat.src.intentional_pipecat.bot_structure.PipecatBotStructure.handle_interruption","title":"<code>handle_interruption(lenght_to_interruption)</code>  <code>async</code>","text":"<p>Handle an interruption in the streaming.</p> <p>Parameters:</p> Name Type Description Default <code>lenght_to_interruption</code> <code>int</code> <p>The length of the data that was produced to the user before the interruption. This value could be number of characters, number of words, milliseconds, number of audio frames, etc. depending on the bot structure that implements it.</p> required Source code in <code>plugins/intentional-pipecat/src/intentional_pipecat/bot_structure.py</code> <pre><code>async def handle_interruption(self, lenght_to_interruption: int) -&gt; None:\n    \"\"\"\n    Handle an interruption in the streaming.\n\n    Args:\n        lenght_to_interruption: The length of the data that was produced to the user before the interruption.\n            This value could be number of characters, number of words, milliseconds, number of audio frames, etc.\n            depending on the bot structure that implements it.\n    \"\"\"\n    log.warning(\"handle interruptions: TODO\")\n</code></pre>"},{"location":"plugins/intentional-pipecat/docs/reference/#plugins.intentional-pipecat.src.intentional_pipecat.bot_structure.PipecatBotStructure.handle_llm_starts_generating_response","title":"<code>handle_llm_starts_generating_response(_)</code>  <code>async</code>","text":"<p>Warns the Pipecat pipeline of the start of a response from the LLM by sending an LLMFullResponseStartFrame()</p> Source code in <code>plugins/intentional-pipecat/src/intentional_pipecat/bot_structure.py</code> <pre><code>async def handle_llm_starts_generating_response(self, _: Dict[str, Any]) -&gt; None:\n    \"\"\"\n    Warns the Pipecat pipeline of the start of a response from the LLM by sending an LLMFullResponseStartFrame()\n    \"\"\"\n    await self.publisher.push_frame(LLMFullResponseStartFrame(), FrameDirection.DOWNSTREAM)\n</code></pre>"},{"location":"plugins/intentional-pipecat/docs/reference/#plugins.intentional-pipecat.src.intentional_pipecat.bot_structure.PipecatBotStructure.handle_llm_stops_generating_response","title":"<code>handle_llm_stops_generating_response(_)</code>  <code>async</code>","text":"<p>Warns the Pipecat pipeline of the end of a response from the LLM by sending an LLMFullResponseEndFrame()</p> Source code in <code>plugins/intentional-pipecat/src/intentional_pipecat/bot_structure.py</code> <pre><code>async def handle_llm_stops_generating_response(self, _: Dict[str, Any]) -&gt; None:\n    \"\"\"\n    Warns the Pipecat pipeline of the end of a response from the LLM by sending an LLMFullResponseEndFrame()\n    \"\"\"\n    await self.publisher.push_frame(LLMFullResponseEndFrame(), FrameDirection.DOWNSTREAM)\n    if self.assistant_reply:\n        await self.llm.emit(\"on_llm_speech_transcribed\", {\"type\": \"assistant\", \"transcript\": self.assistant_reply})\n        self.assistant_reply = \"\"\n</code></pre>"},{"location":"plugins/intentional-pipecat/docs/reference/#plugins.intentional-pipecat.src.intentional_pipecat.bot_structure.PipecatBotStructure.handle_llm_text_messages","title":"<code>handle_llm_text_messages(event)</code>  <code>async</code>","text":"<p>Sends the text message to the Pipecat pipeline to be converted into audio.</p> Source code in <code>plugins/intentional-pipecat/src/intentional_pipecat/bot_structure.py</code> <pre><code>async def handle_llm_text_messages(self, event: Dict[str, Any]) -&gt; None:\n    \"\"\"\n    Sends the text message to the Pipecat pipeline to be converted into audio.\n    \"\"\"\n    if event[\"delta\"]:\n        await self.publisher.push_frame(TextFrame(event[\"delta\"]), FrameDirection.DOWNSTREAM)\n        self.assistant_reply += event[\"delta\"]\n</code></pre>"},{"location":"plugins/intentional-pipecat/docs/reference/#plugins.intentional-pipecat.src.intentional_pipecat.bot_structure.PipecatBotStructure.run","title":"<code>run()</code>  <code>async</code>","text":"<p>Main loop for the bot.</p> Source code in <code>plugins/intentional-pipecat/src/intentional_pipecat/bot_structure.py</code> <pre><code>async def run(self) -&gt; None:\n    \"\"\"\n    Main loop for the bot.\n    \"\"\"\n    log.debug(\".run() is no-op for PipecatBotStructure, the Pipecat pipeline is self-sufficient.\")\n</code></pre>"},{"location":"plugins/intentional-pipecat/docs/reference/#plugins.intentional-pipecat.src.intentional_pipecat.bot_structure.PipecatBotStructure.send","title":"<code>send(data)</code>  <code>async</code>","text":"<p>Sends a message to the LLM and forward the response.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Any]</code> <p>The message to send to the model in OpenAI format, like {\"role\": \"user\", \"content\": \"Hello!\"}</p> required Source code in <code>plugins/intentional-pipecat/src/intentional_pipecat/bot_structure.py</code> <pre><code>async def send(self, data: Dict[str, Any]) -&gt; AsyncGenerator[Dict[str, Any], None]:\n    \"\"\"\n    Sends a message to the LLM and forward the response.\n\n    Args:\n        data: The message to send to the model in OpenAI format, like {\"role\": \"user\", \"content\": \"Hello!\"}\n    \"\"\"\n    if \"audio_chunk\" in data:\n        await self.transport.input().send_audio_frame(data[\"audio_chunk\"])\n    else:\n        raise ValueError(\"PipecatBotStructure only supports audio data for now.\")\n</code></pre>"},{"location":"plugins/intentional-pipecat/docs/reference/#plugins.intentional-pipecat.src.intentional_pipecat.frame_processor","title":"<code>frame_processor</code>","text":"<p>Pipecat frame processor implementation</p>"},{"location":"plugins/intentional-pipecat/docs/reference/#plugins.intentional-pipecat.src.intentional_pipecat.frame_processor.LLMToUserFrameProcessor","title":"<code>LLMToUserFrameProcessor</code>","text":"<p>               Bases: <code>FrameProcessor</code></p> <p>FrameProcessor that takes the LLM output and sends it to the user.</p> <p>Note: this processor itself is doing nothing else than changing the default behavior of <code>process_frame()</code> to not swallow frames when they reach it. The processor is actually used by <code>PipecatBotStructure</code> to generate frames when a reply from the LLM is received.</p> Source code in <code>plugins/intentional-pipecat/src/intentional_pipecat/frame_processor.py</code> <pre><code>class LLMToUserFrameProcessor(FrameProcessor):\n    \"\"\"\n    FrameProcessor that takes the LLM output and sends it to the user.\n\n    Note: this processor itself is doing nothing else than changing the default behavior of `process_frame()` to not\n    swallow frames when they reach it. The processor is actually used by `PipecatBotStructure` to generate frames when\n    a reply from the LLM is received.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n    async def process_frame(self, frame: Frame, direction: FrameDirection):\n        \"\"\"\n        Simply forwards all framews ahead. The default behavior of FrameProcessor is to block them instead.\n        \"\"\"\n        await super().process_frame(frame, direction)\n        await self.push_frame(frame, direction)\n</code></pre>"},{"location":"plugins/intentional-pipecat/docs/reference/#plugins.intentional-pipecat.src.intentional_pipecat.frame_processor.LLMToUserFrameProcessor.process_frame","title":"<code>process_frame(frame, direction)</code>  <code>async</code>","text":"<p>Simply forwards all framews ahead. The default behavior of FrameProcessor is to block them instead.</p> Source code in <code>plugins/intentional-pipecat/src/intentional_pipecat/frame_processor.py</code> <pre><code>async def process_frame(self, frame: Frame, direction: FrameDirection):\n    \"\"\"\n    Simply forwards all framews ahead. The default behavior of FrameProcessor is to block them instead.\n    \"\"\"\n    await super().process_frame(frame, direction)\n    await self.push_frame(frame, direction)\n</code></pre>"},{"location":"plugins/intentional-pipecat/docs/reference/#plugins.intentional-pipecat.src.intentional_pipecat.frame_processor.UserToLLMFrameProcessor","title":"<code>UserToLLMFrameProcessor</code>","text":"<p>               Bases: <code>FrameProcessor</code></p> <p>FrameProcessor that takes the user input and sends it to the LLM.</p> Source code in <code>plugins/intentional-pipecat/src/intentional_pipecat/frame_processor.py</code> <pre><code>class UserToLLMFrameProcessor(FrameProcessor):\n    \"\"\"\n    FrameProcessor that takes the user input and sends it to the LLM.\n    \"\"\"\n\n    def __init__(self, llm_client: LLMClient):\n        super().__init__()\n        self.llm_client = llm_client\n        self.transcription = \"\"\n\n    async def process_frame(self, frame: Frame, direction: FrameDirection):\n        \"\"\"\n        Processes the incoming frames if relevant.\n        \"\"\"\n        await super().process_frame(frame, direction)\n        if isinstance(frame, LLMMessagesFrame):\n            user_message = frame.messages[-1][\"content\"]\n            log.debug(\"LLMMessageFrame received, sending message to LLM\", user_message=user_message)\n            await self.llm_client.emit(\"on_user_speech_transcribed\", {\"type\": \"user\", \"transcript\": user_message})\n            await self.llm_client.send({\"text_message\": {\"role\": \"user\", \"content\": user_message}})\n        else:\n            if isinstance(frame, UserStartedSpeakingFrame):\n                await self.llm_client.emit(\"on_user_speech_started\", {})\n            elif isinstance(frame, UserStoppedSpeakingFrame):\n                await self.llm_client.emit(\"on_user_speech_ended\", {})\n            await self.push_frame(frame, direction)\n</code></pre>"},{"location":"plugins/intentional-pipecat/docs/reference/#plugins.intentional-pipecat.src.intentional_pipecat.frame_processor.UserToLLMFrameProcessor.process_frame","title":"<code>process_frame(frame, direction)</code>  <code>async</code>","text":"<p>Processes the incoming frames if relevant.</p> Source code in <code>plugins/intentional-pipecat/src/intentional_pipecat/frame_processor.py</code> <pre><code>async def process_frame(self, frame: Frame, direction: FrameDirection):\n    \"\"\"\n    Processes the incoming frames if relevant.\n    \"\"\"\n    await super().process_frame(frame, direction)\n    if isinstance(frame, LLMMessagesFrame):\n        user_message = frame.messages[-1][\"content\"]\n        log.debug(\"LLMMessageFrame received, sending message to LLM\", user_message=user_message)\n        await self.llm_client.emit(\"on_user_speech_transcribed\", {\"type\": \"user\", \"transcript\": user_message})\n        await self.llm_client.send({\"text_message\": {\"role\": \"user\", \"content\": user_message}})\n    else:\n        if isinstance(frame, UserStartedSpeakingFrame):\n            await self.llm_client.emit(\"on_user_speech_started\", {})\n        elif isinstance(frame, UserStoppedSpeakingFrame):\n            await self.llm_client.emit(\"on_user_speech_ended\", {})\n        await self.push_frame(frame, direction)\n</code></pre>"},{"location":"plugins/intentional-pipecat/docs/reference/#plugins.intentional-pipecat.src.intentional_pipecat.transport","title":"<code>transport</code>","text":"<p>Pipecat transport class implementation that is compatible with Intentional.</p>"},{"location":"plugins/intentional-pipecat/docs/reference/#plugins.intentional-pipecat.src.intentional_pipecat.transport.AudioInputTransport","title":"<code>AudioInputTransport</code>","text":"<p>               Bases: <code>BaseInputTransport</code></p> <p>Pipecat input transport class implementation that is compatible with Intentional (supports audio only).</p> <p>This class' task is to take the user's input and convert it into frames that Pipecat can process.</p> Source code in <code>plugins/intentional-pipecat/src/intentional_pipecat/transport.py</code> <pre><code>class AudioInputTransport(BaseInputTransport):\n    \"\"\"\n    Pipecat input transport class implementation that is compatible with Intentional (supports audio only).\n\n    This class' task is to take the user's input and convert it into frames that Pipecat can process.\n    \"\"\"\n\n    def __init__(self, params: TransportParams):\n        super().__init__(params)\n        self.ready = False\n\n    async def send_audio_frame(self, audio: bytes):\n        \"\"\"\n        Public method used by the Intentional bot structure to publish audio of user's speech to the Pipecat pipeline.\n        \"\"\"\n        if not self.ready:\n            log.debug(\"Audio input transport not ready yet, won't send this audio frame\")\n            return\n        frame = InputAudioRawFrame(\n            audio=audio,\n            sample_rate=16000,\n            num_channels=self._params.audio_in_channels,\n        )\n        await self.push_audio_frame(frame)\n\n    async def start(self, frame: StartFrame):\n        \"\"\"\n        Starts the transport's resources.\n        \"\"\"\n        log.debug(\"Starting audio input transport\")\n        await super().start(frame)\n        self.ready = True\n\n    async def cleanup(self):\n        \"\"\"\n        Cleans up the transport's resources.\n        \"\"\"\n        await super().cleanup()\n</code></pre>"},{"location":"plugins/intentional-pipecat/docs/reference/#plugins.intentional-pipecat.src.intentional_pipecat.transport.AudioInputTransport.cleanup","title":"<code>cleanup()</code>  <code>async</code>","text":"<p>Cleans up the transport's resources.</p> Source code in <code>plugins/intentional-pipecat/src/intentional_pipecat/transport.py</code> <pre><code>async def cleanup(self):\n    \"\"\"\n    Cleans up the transport's resources.\n    \"\"\"\n    await super().cleanup()\n</code></pre>"},{"location":"plugins/intentional-pipecat/docs/reference/#plugins.intentional-pipecat.src.intentional_pipecat.transport.AudioInputTransport.send_audio_frame","title":"<code>send_audio_frame(audio)</code>  <code>async</code>","text":"<p>Public method used by the Intentional bot structure to publish audio of user's speech to the Pipecat pipeline.</p> Source code in <code>plugins/intentional-pipecat/src/intentional_pipecat/transport.py</code> <pre><code>async def send_audio_frame(self, audio: bytes):\n    \"\"\"\n    Public method used by the Intentional bot structure to publish audio of user's speech to the Pipecat pipeline.\n    \"\"\"\n    if not self.ready:\n        log.debug(\"Audio input transport not ready yet, won't send this audio frame\")\n        return\n    frame = InputAudioRawFrame(\n        audio=audio,\n        sample_rate=16000,\n        num_channels=self._params.audio_in_channels,\n    )\n    await self.push_audio_frame(frame)\n</code></pre>"},{"location":"plugins/intentional-pipecat/docs/reference/#plugins.intentional-pipecat.src.intentional_pipecat.transport.AudioInputTransport.start","title":"<code>start(frame)</code>  <code>async</code>","text":"<p>Starts the transport's resources.</p> Source code in <code>plugins/intentional-pipecat/src/intentional_pipecat/transport.py</code> <pre><code>async def start(self, frame: StartFrame):\n    \"\"\"\n    Starts the transport's resources.\n    \"\"\"\n    log.debug(\"Starting audio input transport\")\n    await super().start(frame)\n    self.ready = True\n</code></pre>"},{"location":"plugins/intentional-pipecat/docs/reference/#plugins.intentional-pipecat.src.intentional_pipecat.transport.AudioOutputTransport","title":"<code>AudioOutputTransport</code>","text":"<p>               Bases: <code>BaseOutputTransport</code></p> <p>Pipecat output transport class implementation that is compatible with Intentional (supports audio only).</p> <p>This class' task is to take the audio frames generated by the TTS and publish them through events that Intentional can understand.</p> Source code in <code>plugins/intentional-pipecat/src/intentional_pipecat/transport.py</code> <pre><code>class AudioOutputTransport(BaseOutputTransport):\n    \"\"\"\n    Pipecat output transport class implementation that is compatible with Intentional (supports audio only).\n\n    This class' task is to take the audio frames generated by the TTS and publish them through events that Intentional\n    can understand.\n    \"\"\"\n\n    def __init__(self, params: TransportParams, emitter_callback: Callable):\n        super().__init__(params)\n        self._emitter_callback = emitter_callback\n\n    async def start(self, frame: StartFrame):\n        \"\"\"\n        Starts the transport's resources.\n        \"\"\"\n        await super().start(frame)\n\n    async def cleanup(self):\n        \"\"\"\n        Cleans up the transport's resources.\n        \"\"\"\n        await super().cleanup()\n\n    async def process_frame(self, frame: Frame, direction: FrameDirection):\n        \"\"\"\n        When it receives a TTSAudioRawFrame, makes the llm emit a `on_audio_message_from_llm` event with the content\n        of the frame.\n        \"\"\"\n        if isinstance(frame, TTSAudioRawFrame):\n            await self._emitter_callback(\"on_audio_message_from_llm\", {\"delta\": frame.audio})\n        # return await super().process_frame(frame, direction)\n\n    async def _audio_out_task_handler(self):\n        \"\"\"\n        Internal: overrides the method of the base class to not perform a few actions we don't need.\n        \"\"\"\n        try:\n            async for frame in self._next_audio_frame():\n                # Also, push frame downstream in case anyone else needs it.\n                await self.push_frame(frame)\n                # Send audio.\n                await self.write_raw_audio_frames(frame.audio)\n        except asyncio.CancelledError:\n            pass\n        except Exception as e:  # pylint: disable=broad-except\n            log.exception(f\"{self} error writing to microphone: {e}\")\n</code></pre>"},{"location":"plugins/intentional-pipecat/docs/reference/#plugins.intentional-pipecat.src.intentional_pipecat.transport.AudioOutputTransport.cleanup","title":"<code>cleanup()</code>  <code>async</code>","text":"<p>Cleans up the transport's resources.</p> Source code in <code>plugins/intentional-pipecat/src/intentional_pipecat/transport.py</code> <pre><code>async def cleanup(self):\n    \"\"\"\n    Cleans up the transport's resources.\n    \"\"\"\n    await super().cleanup()\n</code></pre>"},{"location":"plugins/intentional-pipecat/docs/reference/#plugins.intentional-pipecat.src.intentional_pipecat.transport.AudioOutputTransport.process_frame","title":"<code>process_frame(frame, direction)</code>  <code>async</code>","text":"<p>When it receives a TTSAudioRawFrame, makes the llm emit a <code>on_audio_message_from_llm</code> event with the content of the frame.</p> Source code in <code>plugins/intentional-pipecat/src/intentional_pipecat/transport.py</code> <pre><code>async def process_frame(self, frame: Frame, direction: FrameDirection):\n    \"\"\"\n    When it receives a TTSAudioRawFrame, makes the llm emit a `on_audio_message_from_llm` event with the content\n    of the frame.\n    \"\"\"\n    if isinstance(frame, TTSAudioRawFrame):\n        await self._emitter_callback(\"on_audio_message_from_llm\", {\"delta\": frame.audio})\n</code></pre>"},{"location":"plugins/intentional-pipecat/docs/reference/#plugins.intentional-pipecat.src.intentional_pipecat.transport.AudioOutputTransport.start","title":"<code>start(frame)</code>  <code>async</code>","text":"<p>Starts the transport's resources.</p> Source code in <code>plugins/intentional-pipecat/src/intentional_pipecat/transport.py</code> <pre><code>async def start(self, frame: StartFrame):\n    \"\"\"\n    Starts the transport's resources.\n    \"\"\"\n    await super().start(frame)\n</code></pre>"},{"location":"plugins/intentional-pipecat/docs/reference/#plugins.intentional-pipecat.src.intentional_pipecat.transport.AudioTransport","title":"<code>AudioTransport</code>","text":"<p>               Bases: <code>BaseTransport</code></p> <p>Pipecat transport class implementation that is compatible with Intentional (supports audio only).</p> <p>This class is a simple wrapper around AudioInputTransport and AudioOutputTransport, that makes sure both classes receive the same parameters at initialization.</p> Source code in <code>plugins/intentional-pipecat/src/intentional_pipecat/transport.py</code> <pre><code>class AudioTransport(BaseTransport):\n    \"\"\"\n    Pipecat transport class implementation that is compatible with Intentional (supports audio only).\n\n    This class is a simple wrapper around AudioInputTransport and AudioOutputTransport, that makes sure both classes\n    receive the same parameters at initialization.\n    \"\"\"\n\n    def __init__(self, params: TransportParams, emitter_callback: Callable):\n        super().__init__(params)\n        self._emitter_callback = emitter_callback\n        self._params = params\n        self._input = AudioInputTransport(self._params)\n        self._output = AudioOutputTransport(self._params, self._emitter_callback)\n\n    def input(self) -&gt; AudioInputTransport:\n        return self._input\n\n    def output(self) -&gt; AudioOutputTransport:\n        return self._output\n</code></pre>"},{"location":"plugins/intentional-telegram/","title":"Intentional - Telegram","text":"<p>Plugin that uses Telegram as the interface to communicate with your Intentional bot</p>"},{"location":"plugins/intentional-telegram/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Installation</li> <li>License</li> </ul>"},{"location":"plugins/intentional-telegram/#installation","title":"Installation","text":"<pre><code>pip install intentional-telegram\n</code></pre>"},{"location":"plugins/intentional-telegram/#usage","title":"Usage","text":"<p>For this bot interface to work, you must set the <code>TELEGRAM_BOT_TOKEN</code> env var with the token of your bot (the one generated by BotFather).</p>"},{"location":"plugins/intentional-telegram/#license","title":"License","text":"<p><code>intentional</code> is distributed under the terms of the AGPL license. If that doesn't work for you, get in touch.</p>"},{"location":"plugins/intentional-telegram/docs/reference/","title":"API Reference - Telegram","text":""},{"location":"plugins/intentional-telegram/docs/reference/#plugins.intentional-telegram.src","title":"<code>src</code>","text":""},{"location":"plugins/intentional-telegram/docs/reference/#plugins.intentional-telegram.src.intentional_telegram","title":"<code>intentional_telegram</code>","text":"<p>Intentional Telegram plugin.</p>"},{"location":"plugins/intentional-telegram/docs/reference/#plugins.intentional-telegram.src.intentional_telegram.__about__","title":"<code>__about__</code>","text":"<p>Package descriptors for intentional-telegram.</p>"},{"location":"plugins/intentional-telegram/docs/reference/#plugins.intentional-telegram.src.intentional_telegram.bot_interface","title":"<code>bot_interface</code>","text":"<p>Telegram bot interface for Intentional.</p>"},{"location":"plugins/intentional-telegram/docs/reference/#plugins.intentional-telegram.src.intentional_telegram.bot_interface.TelegramBotInterface","title":"<code>TelegramBotInterface</code>","text":"<p>               Bases: <code>BotInterface</code></p> <p>Bot that uses Telegram to interact with the user.</p> Source code in <code>plugins/intentional-telegram/src/intentional_telegram/bot_interface.py</code> <pre><code>class TelegramBotInterface(BotInterface):\n    \"\"\"\n    Bot that uses Telegram to interact with the user.\n    \"\"\"\n\n    name = \"telegram\"\n\n    def __init__(self, config: Dict[str, Any], intent_router: IntentRouter):\n        # Init the structure\n        bot_structure_config = config.pop(\"bot\", None)\n        if not bot_structure_config:\n            raise ValueError(\n                f\"{self.__class__.__name__} requires a 'bot' configuration key to know how to structure the bot.\"\n            )\n        log.debug(\"Creating bot structure\", bot_structure_config=bot_structure_config)\n        self.bot: BotStructure = load_bot_structure_from_dict(intent_router=intent_router, config=bot_structure_config)\n\n        # Check the modality\n        self.modality = config.pop(\"modality\")\n        log.debug(\"Modality for %s is set\", self.__class__.__name__, modality=self.modality)\n\n        self.telegram_bot = None\n        self.latest_update = None\n        self.latest_message = \"\"\n\n    async def run(self) -&gt; None:\n        \"\"\"\n        Chooses the specific loop to use for this combination of bot and modality and kicks it off.\n        \"\"\"\n        if self.modality == \"text_messages\":\n            await self._run_text_messages(self.bot)\n        else:\n            raise ValueError(\n                f\"Modality '{self.modality}' is not yet supported. These are the supported modalities: 'text_messages'.\"\n            )\n\n    async def _run_text_messages(self, bot: BotStructure) -&gt; None:\n        \"\"\"\n        Runs the CLI interface for the text turns modality.\n        \"\"\"\n        async with Bot(os.getenv(\"TELEGRAM_BOT_TOKEN\")) as telegram_bot:\n\n            # Startup by checking it there's any new update\n            async for update in updates_generator(telegram_bot):\n                log.debug(\"Past Telegram updates processed\", telegram_updates=update)\n                self.latest_update = update\n                break\n\n            bot.add_event_handler(\"on_text_message_from_llm\", self.handle_text_messages)\n            bot.add_event_handler(\"on_llm_starts_generating_response\", self.handle_start_text_response)\n            bot.add_event_handler(\"on_llm_stops_generating_response\", self.handle_finish_text_response)\n\n            await bot.connect()\n            await self._process_updates(telegram_bot)\n\n    async def handle_text_messages(self, event: Dict[str, Any]) -&gt; None:\n        \"\"\"\n        Accumulates text messages delta\n        \"\"\"\n        if event[\"delta\"]:\n            self.latest_message += event[\"delta\"]\n\n    async def handle_start_text_response(self, _) -&gt; None:\n        \"\"\"\n        Resets the current message's content\n        \"\"\"\n        self.latest_message = \"\"\n\n    async def handle_finish_text_response(self, _) -&gt; None:\n        \"\"\"\n        Publishes the message on the Telegram chat\n        \"\"\"\n        if self.latest_message:\n            await self.latest_update.message.reply_text(self.latest_message)\n            self.latest_message = \"\"\n\n    async def _process_updates(self, telegram_bot) -&gt; None:\n        \"\"\"\n        Process updates from the telegram bot.\n        \"\"\"\n        async for update in updates_generator(telegram_bot):\n            log.info(\"Received Telegram update\", telegram_update=update)\n            if update.message:\n                # If the message is a text message, send it to the bot\n                if update.message.text:\n                    await self.bot.send({\"text_message\": {\"role\": \"user\", \"content\": update.message.text}})\n                    self.latest_update = update\n\n            if update.message_reaction:\n                await self.bot.send(\n                    {\n                        \"text_message\": {\n                            \"role\": \"user\",\n                            \"content\": (update.message_reaction.new_reaction)[0].emoji,\n                        }\n                    }\n                )\n</code></pre>"},{"location":"plugins/intentional-telegram/docs/reference/#plugins.intentional-telegram.src.intentional_telegram.bot_interface.TelegramBotInterface.handle_finish_text_response","title":"<code>handle_finish_text_response(_)</code>  <code>async</code>","text":"<p>Publishes the message on the Telegram chat</p> Source code in <code>plugins/intentional-telegram/src/intentional_telegram/bot_interface.py</code> <pre><code>async def handle_finish_text_response(self, _) -&gt; None:\n    \"\"\"\n    Publishes the message on the Telegram chat\n    \"\"\"\n    if self.latest_message:\n        await self.latest_update.message.reply_text(self.latest_message)\n        self.latest_message = \"\"\n</code></pre>"},{"location":"plugins/intentional-telegram/docs/reference/#plugins.intentional-telegram.src.intentional_telegram.bot_interface.TelegramBotInterface.handle_start_text_response","title":"<code>handle_start_text_response(_)</code>  <code>async</code>","text":"<p>Resets the current message's content</p> Source code in <code>plugins/intentional-telegram/src/intentional_telegram/bot_interface.py</code> <pre><code>async def handle_start_text_response(self, _) -&gt; None:\n    \"\"\"\n    Resets the current message's content\n    \"\"\"\n    self.latest_message = \"\"\n</code></pre>"},{"location":"plugins/intentional-telegram/docs/reference/#plugins.intentional-telegram.src.intentional_telegram.bot_interface.TelegramBotInterface.handle_text_messages","title":"<code>handle_text_messages(event)</code>  <code>async</code>","text":"<p>Accumulates text messages delta</p> Source code in <code>plugins/intentional-telegram/src/intentional_telegram/bot_interface.py</code> <pre><code>async def handle_text_messages(self, event: Dict[str, Any]) -&gt; None:\n    \"\"\"\n    Accumulates text messages delta\n    \"\"\"\n    if event[\"delta\"]:\n        self.latest_message += event[\"delta\"]\n</code></pre>"},{"location":"plugins/intentional-telegram/docs/reference/#plugins.intentional-telegram.src.intentional_telegram.bot_interface.TelegramBotInterface.run","title":"<code>run()</code>  <code>async</code>","text":"<p>Chooses the specific loop to use for this combination of bot and modality and kicks it off.</p> Source code in <code>plugins/intentional-telegram/src/intentional_telegram/bot_interface.py</code> <pre><code>async def run(self) -&gt; None:\n    \"\"\"\n    Chooses the specific loop to use for this combination of bot and modality and kicks it off.\n    \"\"\"\n    if self.modality == \"text_messages\":\n        await self._run_text_messages(self.bot)\n    else:\n        raise ValueError(\n            f\"Modality '{self.modality}' is not yet supported. These are the supported modalities: 'text_messages'.\"\n        )\n</code></pre>"},{"location":"plugins/intentional-telegram/docs/reference/#plugins.intentional-telegram.src.intentional_telegram.bot_interface.updates_generator","title":"<code>updates_generator(telegram_bot)</code>  <code>async</code>","text":"<p>Generator for the Telegram updates.</p> Source code in <code>plugins/intentional-telegram/src/intentional_telegram/bot_interface.py</code> <pre><code>async def updates_generator(telegram_bot):\n    \"\"\"\n    Generator for the Telegram updates.\n    \"\"\"\n    latest_update_id = 0\n    while True:\n        try:\n            # bot.get_updates keep sending the same update over and over.\n            # offset can be used to specify what was the last update we processed, so to send only updates with an\n            # update_id higher that that.\n            update_tuple = await telegram_bot.get_updates(\n                offset=latest_update_id, timeout=10, allowed_updates=Update.ALL_TYPES\n            )\n            # Occasionally we may receive empty updates (no idea why)\n            if not update_tuple:\n                continue\n\n            # Get the update and store its update_id\n            update = update_tuple[0]\n            latest_update_id = update.update_id + 1\n\n            # Give out the update\n            yield update\n\n        except NetworkError:\n            log.exception(\"\")\n            await asyncio.sleep(1)\n        except Forbidden:\n            log.exception(\"The user has removed or blocked the bot.\")\n        except Exception:  # pylint: disable=broad-except\n            log.exception(\"An exception occurred\")\n            await update.message.reply_text(\"An error occurred! Check the logs.\")\n</code></pre>"},{"location":"plugins/intentional-terminal/","title":"Intentional - Local terminal output","text":"<p>Plugin that makes Intentional able to use a local terminal to communicate.</p>"},{"location":"plugins/intentional-terminal/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Installation</li> <li>License</li> </ul>"},{"location":"plugins/intentional-terminal/#installation","title":"Installation","text":"<pre><code>sudo apt install portaudio19-dev\n</code></pre> <pre><code>pip install intentional-terminal\n</code></pre>"},{"location":"plugins/intentional-terminal/#license","title":"License","text":"<p><code>intentional</code> is distributed under the terms of the AGPL license. If that doesn't work for you, get in touch.</p>"},{"location":"plugins/intentional-terminal/docs/reference/","title":"API Reference - Local terminal output","text":""},{"location":"plugins/intentional-terminal/docs/reference/#plugins.intentional-terminal.src","title":"<code>src</code>","text":""},{"location":"plugins/intentional-terminal/docs/reference/#plugins.intentional-terminal.src.intentional_terminal","title":"<code>intentional_terminal</code>","text":"<p>Init file for <code>intentional_terminal</code>.</p>"},{"location":"plugins/intentional-terminal/docs/reference/#plugins.intentional-terminal.src.intentional_terminal.__about__","title":"<code>__about__</code>","text":"<p>Package descriptors for intentional-terminal.</p>"},{"location":"plugins/intentional-terminal/docs/reference/#plugins.intentional-terminal.src.intentional_terminal.bot_interface","title":"<code>bot_interface</code>","text":"<p>Local bot interface for Intentional.</p>"},{"location":"plugins/intentional-terminal/docs/reference/#plugins.intentional-terminal.src.intentional_terminal.bot_interface.TerminalBotInterface","title":"<code>TerminalBotInterface</code>","text":"<p>               Bases: <code>BotInterface</code></p> <p>Bot that uses the local command line interface to interact with the user.</p> Source code in <code>plugins/intentional-terminal/src/intentional_terminal/bot_interface.py</code> <pre><code>class TerminalBotInterface(BotInterface):\n    \"\"\"\n    Bot that uses the local command line interface to interact with the user.\n    \"\"\"\n\n    name = \"terminal\"\n\n    def __init__(self, intent_router: IntentRouter, config: Dict[str, Any]):\n        # Init the structure\n        bot_structure_config = config.pop(\"bot\", None)\n        if not bot_structure_config:\n            raise ValueError(\n                f\"{self.__class__.__name__} requires a 'bot' configuration key to know how to structure the bot.\"\n            )\n        log.debug(\"Creating bot structure\", bot_structure_type=bot_structure_config)\n        self.intent_router = intent_router\n        self.bot_structure_config = bot_structure_config\n        self.bot: BotStructure = load_bot_structure_from_dict(self.intent_router, self.bot_structure_config)\n\n        # Check the modality\n        self.modality = config.pop(\"modality\")\n        log.debug(\"Setting interface modality\", modality=self.modality)\n\n        self.audio_handler = None\n        self.input_handler = None\n\n    async def run(self) -&gt; None:\n        \"\"\"\n        Chooses the specific loop to use for this combination of bot and modality and kicks it off.\n        \"\"\"\n        log.debug(\"Running the bot\", bot_type=self.bot.__class__.__name__, modality=self.modality)\n        if self.modality == \"audio_stream\":\n            await self._run_audio_stream(self.bot)\n        elif self.modality == \"text_messages\":\n            await self._run_text_messages(self.bot)\n        else:\n            raise ValueError(\n                f\"Modality '{self.modality}' is not yet supported for '{self.bot.name}' bots.\"\n                \"These are the supported modalities: 'text_messages', 'audio_stream'.\"\n            )\n\n    async def _run_text_messages(self, bot: BotStructure) -&gt; None:\n        \"\"\"\n        Runs the CLI interface for the text turns modality.\n        \"\"\"\n        bot.add_event_handler(\"on_text_message_from_llm\", self.handle_text_messages)\n        bot.add_event_handler(\"on_llm_starts_generating_response\", self.handle_start_text_response)\n        bot.add_event_handler(\"on_llm_stops_generating_response\", self.handle_finish_text_response)\n        bot.add_event_handler(\"on_llm_connection\", self.handle_llm_connection)\n        bot.add_event_handler(\"on_conversation_ended\", self.handle_conversation_ended)\n        await bot.connect()\n\n    async def _run_audio_stream(self, bot: BotStructure) -&gt; None:\n        \"\"\"\n        Runs the CLI interface for the continuous audio streaming modality.\n        \"\"\"\n        # Create the handlers\n        self.audio_handler = AudioHandler()\n        self.input_handler = InputHandler()\n        self.input_handler.loop = asyncio.get_running_loop()\n\n        # Connect the event handlers\n        bot.add_event_handler(\"*\", self.check_for_transcripts)\n        bot.add_event_handler(\"on_conversation_ended\", self.handle_conversation_ended)\n        # bot.add_event_handler(\"on_text_message_from_llm\", self.handle_text_messages)\n        bot.add_event_handler(\"on_audio_message_from_llm\", self.handle_audio_messages)\n        bot.add_event_handler(\"on_user_speech_started\", self.speech_started)\n        bot.add_event_handler(\"on_user_speech_ended\", self.speech_stopped)\n\n        # Start keyboard listener in a separate thread\n        listener = keyboard.Listener(on_press=self.input_handler.on_press)\n        listener.start()\n\n        try:\n            log.debug(\"Connecting to the LLM\")\n            await bot.connect()\n            asyncio.create_task(bot.run())\n\n            print(\"Chat is ready. Start speaking!\")\n            print(\"Press 'q' to quit\")\n            print(\"\")\n\n            # Start continuous audio streaming\n            asyncio.create_task(self.audio_handler.start_streaming(bot.send))\n\n            # Simple input loop for quit command\n            while True:\n                command, _ = await self.input_handler.command_queue.get()\n\n                if command == \"q\":\n                    break\n\n        except Exception:  # pylint: disable=broad-except\n            log.exception(\"An error occurred\")\n        finally:\n            self.audio_handler.stop_streaming()\n            self.audio_handler.cleanup()\n            await bot.disconnect()\n            print(\"Chat is finished. Bye!\")\n\n    async def check_for_transcripts(self, event: Dict[str, Any]) -&gt; None:\n        \"\"\"\n        Checks for transcripts from the bot.\n\n        Args:\n            event: The event dictionary containing the transcript.\n        \"\"\"\n        if \"transcript\" in event:\n            print(f\"[{event['type']}] Transcript: {event['transcript']}\")\n\n    async def handle_start_text_response(self, _) -&gt; None:\n        \"\"\"\n        Prints to the console when the bot starts generating a text response.\n        \"\"\"\n        print(\"Assistant: \", end=\"\")\n\n    async def handle_finish_text_response(self, _) -&gt; None:\n        \"\"\"\n        Prints to the console when the bot starts generating a text response.\n        \"\"\"\n        print(\"\")\n        await self.bot.send({\"text_message\": {\"role\": \"user\", \"content\": input(\"User: \")}})\n\n    async def handle_llm_connection(self, event: Dict[str, Any]) -&gt; None:\n        \"\"\"\n        Prints to the console when the bot connects to the LLM.\n\n        Args:\n            event: The event dictionary containing the LLM connection event.\n        \"\"\"\n        print(\"==&gt; Chat is ready!\")\n        await self.handle_finish_text_response(event)\n\n    async def handle_text_messages(self, event: Dict[str, Any]) -&gt; None:\n        \"\"\"\n        Prints to the console any text message from the bot.\n\n        Args:\n            event: The event dictionary containing the message.\n        \"\"\"\n        if event[\"delta\"]:\n            print(event[\"delta\"], end=\"\", flush=True)\n\n    async def handle_audio_messages(self, event: Dict[str, Any]) -&gt; None:\n        \"\"\"\n        Plays audio responses from the bot.\n\n        Args:\n            event: The event dictionary containing the audio message.\n        \"\"\"\n        self.audio_handler.play_audio(event[\"delta\"])\n\n    async def speech_started(self, event: Dict[str, Any]) -&gt; None:  # pylint: disable=unused-argument\n        \"\"\"\n        Prints to the console when the bot starts speaking.\n\n        Args:\n            event: The event dictionary containing the speech start event.\n        \"\"\"\n        print(\"[User is speaking]\")\n\n        # Handle interruptions if it is the case\n        played_milliseconds = self.audio_handler.stop_playback_immediately()\n        log.debug(\"Audio response played\", play_duration=played_milliseconds)\n\n        # If we're interrupting the bot, handle the interruption on the LLM side too\n        if played_milliseconds:\n            log.info(\"Handling interruption\", play_duration=played_milliseconds)\n            await self.bot.handle_interruption(played_milliseconds)\n\n    async def speech_stopped(self, event: Dict[str, Any]) -&gt; None:  # pylint: disable=unused-argument\n        \"\"\"\n        Prints to the console when the bot stops speaking.\n\n        Args:\n            event: The event dictionary containing the speech stop event.\n        \"\"\"\n        print(\"[User stopped speaking]\")\n\n    async def handle_conversation_ended(self, _):\n        \"\"\"\n        The conversation is over, so let's ask the user if they want to have another go.\n        \"\"\"\n        restart = input(\"==&gt; The conversation was ended by the bot. Do you want to restart? (y/N)\")\n        if \"y\" not in restart.lower():\n            sys.exit(0)\n\n        self.bot: BotStructure = load_bot_structure_from_dict(self.intent_router, self.bot_structure_config)\n        await self.run()\n</code></pre>"},{"location":"plugins/intentional-terminal/docs/reference/#plugins.intentional-terminal.src.intentional_terminal.bot_interface.TerminalBotInterface.check_for_transcripts","title":"<code>check_for_transcripts(event)</code>  <code>async</code>","text":"<p>Checks for transcripts from the bot.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Dict[str, Any]</code> <p>The event dictionary containing the transcript.</p> required Source code in <code>plugins/intentional-terminal/src/intentional_terminal/bot_interface.py</code> <pre><code>async def check_for_transcripts(self, event: Dict[str, Any]) -&gt; None:\n    \"\"\"\n    Checks for transcripts from the bot.\n\n    Args:\n        event: The event dictionary containing the transcript.\n    \"\"\"\n    if \"transcript\" in event:\n        print(f\"[{event['type']}] Transcript: {event['transcript']}\")\n</code></pre>"},{"location":"plugins/intentional-terminal/docs/reference/#plugins.intentional-terminal.src.intentional_terminal.bot_interface.TerminalBotInterface.handle_audio_messages","title":"<code>handle_audio_messages(event)</code>  <code>async</code>","text":"<p>Plays audio responses from the bot.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Dict[str, Any]</code> <p>The event dictionary containing the audio message.</p> required Source code in <code>plugins/intentional-terminal/src/intentional_terminal/bot_interface.py</code> <pre><code>async def handle_audio_messages(self, event: Dict[str, Any]) -&gt; None:\n    \"\"\"\n    Plays audio responses from the bot.\n\n    Args:\n        event: The event dictionary containing the audio message.\n    \"\"\"\n    self.audio_handler.play_audio(event[\"delta\"])\n</code></pre>"},{"location":"plugins/intentional-terminal/docs/reference/#plugins.intentional-terminal.src.intentional_terminal.bot_interface.TerminalBotInterface.handle_conversation_ended","title":"<code>handle_conversation_ended(_)</code>  <code>async</code>","text":"<p>The conversation is over, so let's ask the user if they want to have another go.</p> Source code in <code>plugins/intentional-terminal/src/intentional_terminal/bot_interface.py</code> <pre><code>async def handle_conversation_ended(self, _):\n    \"\"\"\n    The conversation is over, so let's ask the user if they want to have another go.\n    \"\"\"\n    restart = input(\"==&gt; The conversation was ended by the bot. Do you want to restart? (y/N)\")\n    if \"y\" not in restart.lower():\n        sys.exit(0)\n\n    self.bot: BotStructure = load_bot_structure_from_dict(self.intent_router, self.bot_structure_config)\n    await self.run()\n</code></pre>"},{"location":"plugins/intentional-terminal/docs/reference/#plugins.intentional-terminal.src.intentional_terminal.bot_interface.TerminalBotInterface.handle_finish_text_response","title":"<code>handle_finish_text_response(_)</code>  <code>async</code>","text":"<p>Prints to the console when the bot starts generating a text response.</p> Source code in <code>plugins/intentional-terminal/src/intentional_terminal/bot_interface.py</code> <pre><code>async def handle_finish_text_response(self, _) -&gt; None:\n    \"\"\"\n    Prints to the console when the bot starts generating a text response.\n    \"\"\"\n    print(\"\")\n    await self.bot.send({\"text_message\": {\"role\": \"user\", \"content\": input(\"User: \")}})\n</code></pre>"},{"location":"plugins/intentional-terminal/docs/reference/#plugins.intentional-terminal.src.intentional_terminal.bot_interface.TerminalBotInterface.handle_llm_connection","title":"<code>handle_llm_connection(event)</code>  <code>async</code>","text":"<p>Prints to the console when the bot connects to the LLM.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Dict[str, Any]</code> <p>The event dictionary containing the LLM connection event.</p> required Source code in <code>plugins/intentional-terminal/src/intentional_terminal/bot_interface.py</code> <pre><code>async def handle_llm_connection(self, event: Dict[str, Any]) -&gt; None:\n    \"\"\"\n    Prints to the console when the bot connects to the LLM.\n\n    Args:\n        event: The event dictionary containing the LLM connection event.\n    \"\"\"\n    print(\"==&gt; Chat is ready!\")\n    await self.handle_finish_text_response(event)\n</code></pre>"},{"location":"plugins/intentional-terminal/docs/reference/#plugins.intentional-terminal.src.intentional_terminal.bot_interface.TerminalBotInterface.handle_start_text_response","title":"<code>handle_start_text_response(_)</code>  <code>async</code>","text":"<p>Prints to the console when the bot starts generating a text response.</p> Source code in <code>plugins/intentional-terminal/src/intentional_terminal/bot_interface.py</code> <pre><code>async def handle_start_text_response(self, _) -&gt; None:\n    \"\"\"\n    Prints to the console when the bot starts generating a text response.\n    \"\"\"\n    print(\"Assistant: \", end=\"\")\n</code></pre>"},{"location":"plugins/intentional-terminal/docs/reference/#plugins.intentional-terminal.src.intentional_terminal.bot_interface.TerminalBotInterface.handle_text_messages","title":"<code>handle_text_messages(event)</code>  <code>async</code>","text":"<p>Prints to the console any text message from the bot.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Dict[str, Any]</code> <p>The event dictionary containing the message.</p> required Source code in <code>plugins/intentional-terminal/src/intentional_terminal/bot_interface.py</code> <pre><code>async def handle_text_messages(self, event: Dict[str, Any]) -&gt; None:\n    \"\"\"\n    Prints to the console any text message from the bot.\n\n    Args:\n        event: The event dictionary containing the message.\n    \"\"\"\n    if event[\"delta\"]:\n        print(event[\"delta\"], end=\"\", flush=True)\n</code></pre>"},{"location":"plugins/intentional-terminal/docs/reference/#plugins.intentional-terminal.src.intentional_terminal.bot_interface.TerminalBotInterface.run","title":"<code>run()</code>  <code>async</code>","text":"<p>Chooses the specific loop to use for this combination of bot and modality and kicks it off.</p> Source code in <code>plugins/intentional-terminal/src/intentional_terminal/bot_interface.py</code> <pre><code>async def run(self) -&gt; None:\n    \"\"\"\n    Chooses the specific loop to use for this combination of bot and modality and kicks it off.\n    \"\"\"\n    log.debug(\"Running the bot\", bot_type=self.bot.__class__.__name__, modality=self.modality)\n    if self.modality == \"audio_stream\":\n        await self._run_audio_stream(self.bot)\n    elif self.modality == \"text_messages\":\n        await self._run_text_messages(self.bot)\n    else:\n        raise ValueError(\n            f\"Modality '{self.modality}' is not yet supported for '{self.bot.name}' bots.\"\n            \"These are the supported modalities: 'text_messages', 'audio_stream'.\"\n        )\n</code></pre>"},{"location":"plugins/intentional-terminal/docs/reference/#plugins.intentional-terminal.src.intentional_terminal.bot_interface.TerminalBotInterface.speech_started","title":"<code>speech_started(event)</code>  <code>async</code>","text":"<p>Prints to the console when the bot starts speaking.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Dict[str, Any]</code> <p>The event dictionary containing the speech start event.</p> required Source code in <code>plugins/intentional-terminal/src/intentional_terminal/bot_interface.py</code> <pre><code>async def speech_started(self, event: Dict[str, Any]) -&gt; None:  # pylint: disable=unused-argument\n    \"\"\"\n    Prints to the console when the bot starts speaking.\n\n    Args:\n        event: The event dictionary containing the speech start event.\n    \"\"\"\n    print(\"[User is speaking]\")\n\n    # Handle interruptions if it is the case\n    played_milliseconds = self.audio_handler.stop_playback_immediately()\n    log.debug(\"Audio response played\", play_duration=played_milliseconds)\n\n    # If we're interrupting the bot, handle the interruption on the LLM side too\n    if played_milliseconds:\n        log.info(\"Handling interruption\", play_duration=played_milliseconds)\n        await self.bot.handle_interruption(played_milliseconds)\n</code></pre>"},{"location":"plugins/intentional-terminal/docs/reference/#plugins.intentional-terminal.src.intentional_terminal.bot_interface.TerminalBotInterface.speech_stopped","title":"<code>speech_stopped(event)</code>  <code>async</code>","text":"<p>Prints to the console when the bot stops speaking.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Dict[str, Any]</code> <p>The event dictionary containing the speech stop event.</p> required Source code in <code>plugins/intentional-terminal/src/intentional_terminal/bot_interface.py</code> <pre><code>async def speech_stopped(self, event: Dict[str, Any]) -&gt; None:  # pylint: disable=unused-argument\n    \"\"\"\n    Prints to the console when the bot stops speaking.\n\n    Args:\n        event: The event dictionary containing the speech stop event.\n    \"\"\"\n    print(\"[User stopped speaking]\")\n</code></pre>"},{"location":"plugins/intentional-terminal/docs/reference/#plugins.intentional-terminal.src.intentional_terminal.handlers","title":"<code>handlers</code>","text":"<p>Init file for <code>intentional_terminal</code>.</p>"},{"location":"plugins/intentional-terminal/docs/reference/#plugins.intentional-terminal.src.intentional_terminal.handlers.audio_handler","title":"<code>audio_handler</code>","text":"<p>CLI handler for the bot's audio input and output.</p> <p>Uses PyAudio for audio input and output, and runs a separate thread for recording and playing audio.</p> <p>When playing audio, it uses a buffer to store audio data and plays it continuously to ensure smooth playback.</p>"},{"location":"plugins/intentional-terminal/docs/reference/#plugins.intentional-terminal.src.intentional_terminal.handlers.audio_handler.AudioHandler","title":"<code>AudioHandler</code>","text":"<p>Handles audio input and output for the chatbot.</p> <p>Uses PyAudio for audio input and output, and runs a separate thread for recording and playing audio.</p> <p>When playing audio, it uses a buffer to store audio data and plays it continuously to ensure smooth playback.</p> <p>Parameters:</p> Name Type Description Default <code>audio_format</code> <code>int</code> <p>The audio format (paInt16).</p> <code>paInt16</code> <code>channels</code> <code>int</code> <p>The number of audio channels (1).</p> <code>1</code> <code>rate</code> <code>int</code> <p>The sample rate (24000).</p> <code>24000</code> <code>chunk</code> <code>int</code> <p>The size of the audio buffer (1024).</p> <code>1024</code> Source code in <code>plugins/intentional-terminal/src/intentional_terminal/handlers/audio_handler.py</code> <pre><code>class AudioHandler:\n    \"\"\"\n    Handles audio input and output for the chatbot.\n\n    Uses PyAudio for audio input and output, and runs a separate thread for recording and playing audio.\n\n    When playing audio, it uses a buffer to store audio data and plays it continuously to ensure smooth playback.\n\n    Args:\n        audio_format:\n            The audio format (paInt16).\n        channels:\n            The number of audio channels (1).\n        rate:\n            The sample rate (24000).\n        chunk:\n            The size of the audio buffer (1024).\n    \"\"\"\n\n    def __init__(\n        self,\n        audio_format: int = pyaudio.paInt16,\n        channels: int = 1,\n        rate: int = 24000,\n        chunk: int = 1024,\n    ):\n        # Audio parameters\n        self.audio_format = audio_format\n        self.channels = channels\n        self.rate = rate\n        self.chunk = chunk\n\n        self.audio = pyaudio.PyAudio()\n\n        # Recording attributes\n        self.recording_stream: Optional[pyaudio.Stream] = None\n        self.recording_thread = None\n        self.recording = False\n\n        # LLM streaming attributes\n        self.streaming = False\n        self.llm_stream = None\n\n        # Playback attributes\n        self.playback_stream = None\n        self.playback_play_time = 0\n        self.playback_buffer = queue.Queue()\n        self.playback_event = threading.Event()\n        self.playback_thread = None\n        self.stop_playback = False\n\n        self.frames = []\n        self.currently_playing = False\n\n    async def start_streaming(self, client_streaming_callback):\n        \"\"\"Start continuous audio streaming.\"\"\"\n        if self.streaming:\n            return\n\n        self.streaming = True\n        self.llm_stream = self.audio.open(\n            format=self.audio_format,\n            channels=self.channels,\n            rate=self.rate,\n            input=True,\n            frames_per_buffer=self.chunk,\n        )\n        while self.streaming:\n            try:\n                # Read raw PCM data\n                data = self.llm_stream.read(self.chunk, exception_on_overflow=False)\n                # Stream directly without trying to decode\n                await client_streaming_callback({\"audio_chunk\": data})\n            except Exception:  # pylint: disable=broad-except\n                log.exception(\"Error streaming\")\n                break\n            await asyncio.sleep(0.01)\n\n    def stop_streaming(self):\n        \"\"\"\n        Stop audio streaming.\n        \"\"\"\n        self.streaming = False\n        if self.llm_stream:\n            self.llm_stream.stop_stream()\n            self.llm_stream.close()\n            self.llm_stream = None\n\n    def play_audio(self, audio_data: bytes):\n        \"\"\"\n        Add audio data to the buffer\n\n        Args:\n            audio_data: The audio data to play.\n        \"\"\"\n        audio_segment = AudioSegment(audio_data, sample_width=2, frame_rate=24000, channels=1)\n        try:\n            self.playback_buffer.put_nowait(audio_segment)\n        except queue.Full:\n            # If the buffer is full, remove the oldest chunk and add the new one\n            self.playback_buffer.get_nowait()\n            self.playback_buffer.put_nowait(audio_segment)\n\n        if not self.playback_thread or not self.playback_thread.is_alive():\n            self.stop_playback = False\n            self.playback_event.clear()\n            self.playback_thread = threading.Thread(target=self._continuous_playback)\n            self.playback_thread.start()\n\n    def _continuous_playback(self):\n        \"\"\"\n        Continuously play audio from the buffer.\n        \"\"\"\n        self.playback_stream = self.audio.open(\n            format=self.audio_format,\n            channels=self.channels,\n            rate=self.rate,\n            output=True,\n            frames_per_buffer=self.chunk,\n        )\n        while not self.stop_playback:\n            try:\n                audio_segment = self.playback_buffer.get(timeout=0.1)\n                self.playback_play_time += len(audio_segment)\n                self._play_audio_chunk(audio_segment)\n            except queue.Empty:\n                self.playback_play_time = 0\n                log.debug(\"Audio buffer empty\")\n                continue\n\n            if self.playback_event.is_set():\n                break\n\n        if self.playback_stream:\n            self.playback_stream.stop_stream()\n            self.playback_stream.close()\n            self.playback_stream = None\n\n    def _play_audio_chunk(self, audio_segment: AudioSegment):\n        try:\n            # Ensure the audio is in the correct format for playback\n            audio_data = audio_segment.raw_data\n\n            # Play the audio chunk in smaller portions to allow for quicker interruption\n            chunk_size = 1024  # Adjust this value as needed\n            for i in range(0, len(audio_data), chunk_size):\n                if self.playback_event.is_set():\n                    break\n                chunk = audio_data[i : i + chunk_size]\n                self.playback_stream.write(chunk)\n        except Exception:  # pylint: disable=broad-except\n            log.exception(\"Error playing audio chunk\")\n\n    def stop_playback_immediately(self) -&gt; datetime.timedelta:\n        \"\"\"\n        Stop audio playback immediately. Sets the relevant flags and empties the queue.\n\n        \"\"\"\n        played_milliseconds = 0\n        if self.playback_play_time:\n            played_milliseconds = self.playback_play_time\n            self.playback_play_time = 0\n\n        self.stop_playback = True\n        self.playback_buffer.queue.clear()  # Clear any pending audio\n        self.currently_playing = False\n        self.playback_event.set()\n        return played_milliseconds\n\n    def cleanup(self):\n        \"\"\"\n        Clean up audio resources.\n        \"\"\"\n        self.stop_playback_immediately()\n\n        self.stop_playback = True\n        if self.playback_thread:\n            self.playback_thread.join()\n\n        self.recording = False\n        if self.recording_stream:\n            self.recording_stream.stop_stream()\n            self.recording_stream.close()\n\n        if self.llm_stream:\n            self.llm_stream.stop_stream()\n            self.llm_stream.close()\n\n        self.audio.terminate()\n</code></pre> <code>cleanup()</code> # <p>Clean up audio resources.</p> Source code in <code>plugins/intentional-terminal/src/intentional_terminal/handlers/audio_handler.py</code> <pre><code>def cleanup(self):\n    \"\"\"\n    Clean up audio resources.\n    \"\"\"\n    self.stop_playback_immediately()\n\n    self.stop_playback = True\n    if self.playback_thread:\n        self.playback_thread.join()\n\n    self.recording = False\n    if self.recording_stream:\n        self.recording_stream.stop_stream()\n        self.recording_stream.close()\n\n    if self.llm_stream:\n        self.llm_stream.stop_stream()\n        self.llm_stream.close()\n\n    self.audio.terminate()\n</code></pre> <code>play_audio(audio_data)</code> # <p>Add audio data to the buffer</p> <p>Parameters:</p> Name Type Description Default <code>audio_data</code> <code>bytes</code> <p>The audio data to play.</p> required Source code in <code>plugins/intentional-terminal/src/intentional_terminal/handlers/audio_handler.py</code> <pre><code>def play_audio(self, audio_data: bytes):\n    \"\"\"\n    Add audio data to the buffer\n\n    Args:\n        audio_data: The audio data to play.\n    \"\"\"\n    audio_segment = AudioSegment(audio_data, sample_width=2, frame_rate=24000, channels=1)\n    try:\n        self.playback_buffer.put_nowait(audio_segment)\n    except queue.Full:\n        # If the buffer is full, remove the oldest chunk and add the new one\n        self.playback_buffer.get_nowait()\n        self.playback_buffer.put_nowait(audio_segment)\n\n    if not self.playback_thread or not self.playback_thread.is_alive():\n        self.stop_playback = False\n        self.playback_event.clear()\n        self.playback_thread = threading.Thread(target=self._continuous_playback)\n        self.playback_thread.start()\n</code></pre> <code>start_streaming(client_streaming_callback)</code> <code>async</code> # <p>Start continuous audio streaming.</p> Source code in <code>plugins/intentional-terminal/src/intentional_terminal/handlers/audio_handler.py</code> <pre><code>async def start_streaming(self, client_streaming_callback):\n    \"\"\"Start continuous audio streaming.\"\"\"\n    if self.streaming:\n        return\n\n    self.streaming = True\n    self.llm_stream = self.audio.open(\n        format=self.audio_format,\n        channels=self.channels,\n        rate=self.rate,\n        input=True,\n        frames_per_buffer=self.chunk,\n    )\n    while self.streaming:\n        try:\n            # Read raw PCM data\n            data = self.llm_stream.read(self.chunk, exception_on_overflow=False)\n            # Stream directly without trying to decode\n            await client_streaming_callback({\"audio_chunk\": data})\n        except Exception:  # pylint: disable=broad-except\n            log.exception(\"Error streaming\")\n            break\n        await asyncio.sleep(0.01)\n</code></pre> <code>stop_playback_immediately()</code> # <p>Stop audio playback immediately. Sets the relevant flags and empties the queue.</p> Source code in <code>plugins/intentional-terminal/src/intentional_terminal/handlers/audio_handler.py</code> <pre><code>def stop_playback_immediately(self) -&gt; datetime.timedelta:\n    \"\"\"\n    Stop audio playback immediately. Sets the relevant flags and empties the queue.\n\n    \"\"\"\n    played_milliseconds = 0\n    if self.playback_play_time:\n        played_milliseconds = self.playback_play_time\n        self.playback_play_time = 0\n\n    self.stop_playback = True\n    self.playback_buffer.queue.clear()  # Clear any pending audio\n    self.currently_playing = False\n    self.playback_event.set()\n    return played_milliseconds\n</code></pre> <code>stop_streaming()</code> # <p>Stop audio streaming.</p> Source code in <code>plugins/intentional-terminal/src/intentional_terminal/handlers/audio_handler.py</code> <pre><code>def stop_streaming(self):\n    \"\"\"\n    Stop audio streaming.\n    \"\"\"\n    self.streaming = False\n    if self.llm_stream:\n        self.llm_stream.stop_stream()\n        self.llm_stream.close()\n        self.llm_stream = None\n</code></pre>"},{"location":"plugins/intentional-terminal/docs/reference/#plugins.intentional-terminal.src.intentional_terminal.handlers.input_handler","title":"<code>input_handler</code>","text":"<p>Handles keyboard input for the bot's CLI interface.</p> <p>This module is responsible for capturing keyboard input and translating it into commands for the bot.</p>"},{"location":"plugins/intentional-terminal/docs/reference/#plugins.intentional-terminal.src.intentional_terminal.handlers.input_handler.InputHandler","title":"<code>InputHandler</code>","text":"<p>Handles keyboard input for the chatbot.</p> <p>This class is responsible for capturing keyboard input and translating it into commands for the chatbot.</p> <p>Attributes:</p> Name Type Description <code>text_input</code> <code>str</code> <p>The current text input from the user.</p> <code>text_ready</code> <code>Event</code> <p>An event that is set when the user has finished typing.</p> <code>command_queue</code> <code>Queue</code> <p>A queue that stores commands for the chatbot.</p> <code>loop</code> <code>AbstractEventLoop</code> <p>The event loop for the input handler.</p> Source code in <code>plugins/intentional-terminal/src/intentional_terminal/handlers/input_handler.py</code> <pre><code>class InputHandler:\n    \"\"\"\n    Handles keyboard input for the chatbot.\n\n    This class is responsible for capturing keyboard input and translating it into commands for the chatbot.\n\n    Attributes:\n        text_input (str): The current text input from the user.\n        text_ready (asyncio.Event): An event that is set when the user has finished typing.\n        command_queue (asyncio.Queue): A queue that stores commands for the chatbot.\n        loop (asyncio.AbstractEventLoop): The event loop for the input handler.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Handles keyboard input for the chatbot.\n        \"\"\"\n        self.text_input = \"\"\n        self.text_ready = asyncio.Event()\n        self.command_queue = asyncio.Queue()\n        self.loop = None\n\n    def on_press(self, key):\n        \"\"\"\n        Keyboard event handler.\n        \"\"\"\n        try:\n            if key == keyboard.Key.space:\n                self.loop.call_soon_threadsafe(self.command_queue.put_nowait, (\"space\", None))\n            elif key == keyboard.Key.enter:\n                self.loop.call_soon_threadsafe(self.command_queue.put_nowait, (\"enter\", self.text_input))\n                self.text_input = \"\"\n            elif key == keyboard.KeyCode.from_char(\"r\"):\n                self.loop.call_soon_threadsafe(self.command_queue.put_nowait, (\"r\", None))\n            elif key == keyboard.KeyCode.from_char(\"q\"):\n                self.loop.call_soon_threadsafe(self.command_queue.put_nowait, (\"q\", None))\n            elif hasattr(key, \"char\"):\n                if key == keyboard.Key.backspace:\n                    self.text_input = self.text_input[:-1]\n                else:\n                    self.text_input += key.char\n\n        except AttributeError:\n            log.exception(\"Error processing key event\")\n</code></pre> <code>__init__()</code> # <p>Handles keyboard input for the chatbot.</p> Source code in <code>plugins/intentional-terminal/src/intentional_terminal/handlers/input_handler.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Handles keyboard input for the chatbot.\n    \"\"\"\n    self.text_input = \"\"\n    self.text_ready = asyncio.Event()\n    self.command_queue = asyncio.Queue()\n    self.loop = None\n</code></pre> <code>on_press(key)</code> # <p>Keyboard event handler.</p> Source code in <code>plugins/intentional-terminal/src/intentional_terminal/handlers/input_handler.py</code> <pre><code>def on_press(self, key):\n    \"\"\"\n    Keyboard event handler.\n    \"\"\"\n    try:\n        if key == keyboard.Key.space:\n            self.loop.call_soon_threadsafe(self.command_queue.put_nowait, (\"space\", None))\n        elif key == keyboard.Key.enter:\n            self.loop.call_soon_threadsafe(self.command_queue.put_nowait, (\"enter\", self.text_input))\n            self.text_input = \"\"\n        elif key == keyboard.KeyCode.from_char(\"r\"):\n            self.loop.call_soon_threadsafe(self.command_queue.put_nowait, (\"r\", None))\n        elif key == keyboard.KeyCode.from_char(\"q\"):\n            self.loop.call_soon_threadsafe(self.command_queue.put_nowait, (\"q\", None))\n        elif hasattr(key, \"char\"):\n            if key == keyboard.Key.backspace:\n                self.text_input = self.text_input[:-1]\n            else:\n                self.text_input += key.char\n\n    except AttributeError:\n        log.exception(\"Error processing key event\")\n</code></pre>"},{"location":"plugins/intentional-textual-ui/","title":"Intentional - Textual TUI","text":"<p>Adds a Textual Terminal User Interface (TUI) for Intentional bots</p>"},{"location":"plugins/intentional-textual-ui/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Installation</li> <li>License</li> </ul>"},{"location":"plugins/intentional-textual-ui/#installation","title":"Installation","text":"<pre><code>pip install intentional-textual-ui\n</code></pre>"},{"location":"plugins/intentional-textual-ui/#license","title":"License","text":"<p><code>intentional</code> is distributed under the terms of the AGPL license. If that doesn't work for you, get in touch.</p>"},{"location":"plugins/intentional-textual-ui/docs/reference/","title":"API Reference - Textual TUI","text":""},{"location":"plugins/intentional-textual-ui/docs/reference/#plugins.intentional-textual-ui.src","title":"<code>src</code>","text":""},{"location":"plugins/intentional-textual-ui/docs/reference/#plugins.intentional-textual-ui.src.intentional_textual_ui","title":"<code>intentional_textual_ui</code>","text":"<p>Init file for the intentional_textual_ui package.</p>"},{"location":"plugins/intentional-textual-ui/docs/reference/#plugins.intentional-textual-ui.src.intentional_textual_ui.__about__","title":"<code>__about__</code>","text":"<p>Package descriptors for intentional-textual-ui.</p>"},{"location":"plugins/intentional-textual-ui/docs/reference/#plugins.intentional-textual-ui.src.intentional_textual_ui.audio_stream_ui","title":"<code>audio_stream_ui</code>","text":"<p>Textual UI for audio stream bots.</p>"},{"location":"plugins/intentional-textual-ui/docs/reference/#plugins.intentional-textual-ui.src.intentional_textual_ui.audio_stream_ui.AudioStreamInterface","title":"<code>AudioStreamInterface</code>","text":"<p>               Bases: <code>App</code></p> <p>The main interface class for the audio stream bot UI.</p> Source code in <code>plugins/intentional-textual-ui/src/intentional_textual_ui/audio_stream_ui.py</code> <pre><code>class AudioStreamInterface(App):\n    \"\"\"\n    The main interface class for the audio stream bot UI.\n    \"\"\"\n\n    CSS_PATH = \"example.tcss\"\n\n    def __init__(self, bot: BotStructure, audio_output_handler: \"AudioHandler\"):\n        super().__init__()\n        self.bot = bot\n        self.audio_handler = audio_output_handler\n        self.bot.add_event_handler(\"on_audio_message_from_llm\", self.handle_audio_messages)\n        self.bot.add_event_handler(\"on_llm_speech_transcribed\", self.handle_transcript)\n        self.bot.add_event_handler(\"on_user_speech_transcribed\", self.handle_transcript)\n        self.bot.add_event_handler(\"on_user_speech_started\", self.handle_start_user_response)\n        self.bot.add_event_handler(\"on_user_speech_ended\", self.handle_finish_user_response)\n        self.bot.add_event_handler(\"on_system_prompt_updated\", self.handle_system_prompt_updated)\n        self.bot.add_event_handler(\"on_conversation_ended\", self.handle_conversation_end)\n\n        self.conversation = \"\"\n\n    def compose(self) -&gt; ComposeResult:\n        \"\"\"\n        Layout of the UI.\n        \"\"\"\n        yield Horizontal(\n            Vertical(\n                Markdown(\"# Chat History\"),\n                ScrollableContainer(ChatHistory()),\n                UserStatus(\"# User is silent...\"),\n                classes=\"column bordered chat\",\n            ),\n            Vertical(\n                Markdown(\"# System Prompt\"),\n                SystemPrompt(),\n                classes=\"bordered column\",\n            ),\n        )\n\n    def on_mount(self) -&gt; None:\n        \"\"\"\n        Operations to be performed at mount time.\n        \"\"\"\n        self.query_one(SystemPrompt).update(self.bot.llm.system_prompt)\n\n    async def handle_transcript(self, event: Dict[str, Any]) -&gt; None:\n        \"\"\"\n        Prints the transcripts in the chat history.\n        \"\"\"\n        if event[\"type\"] == \"on_user_speech_transcribed\":\n            self.conversation += f\"\\n**User:** {event['transcript']}\\n\"\n        elif event[\"type\"] == \"on_llm_speech_transcribed\":\n            self.conversation += f\"\\n**Assistant:** {event['transcript']}\\n\"\n        else:\n            log.debug(\"Unknown event with transcript received.\", event_name=event[\"type\"])\n            self.conversation += f\"\\n**{event['type']}:** {event['transcript']}\\n\"\n        self.query_one(ChatHistory).update(self.conversation)\n\n    async def handle_system_prompt_updated(self, event: Dict[str, Any]) -&gt; None:\n        \"\"\"\n        Prints to the console any text message from the bot.\n\n        Args:\n            event: The event dictionary containing the message.\n        \"\"\"\n        self.query_one(SystemPrompt).update(event[\"system_prompt\"])\n\n    async def handle_start_user_response(self, _) -&gt; None:\n        \"\"\"\n        Updates the user status when they start speaking.\n        \"\"\"\n        self.query_one(UserStatus).update(\"# User is speaking...\")\n\n    async def handle_finish_user_response(self, _) -&gt; None:\n        \"\"\"\n        Updates the user status when they stop speaking.\n        \"\"\"\n        self.query_one(UserStatus).update(\"# User is silent...\")\n\n    async def handle_audio_messages(self, event: Dict[str, Any]) -&gt; None:\n        \"\"\"\n        Plays audio responses from the bot and updates the bot status line.\n\n        Args:\n            event: The event dictionary containing the audio message.\n        \"\"\"\n        # self.query_one(BotStatus).update(\"# Bot is speaking...\")\n        if event[\"delta\"]:\n            self.audio_handler.play_audio(event[\"delta\"])\n\n    async def handle_conversation_end(self, _) -&gt; None:\n        \"\"\"\n        At the end of the conversation, closes the UI.\n        \"\"\"\n        self.exit(0)\n        self.audio_handler.stop_streaming()\n        self.audio_handler.cleanup()\n</code></pre>"},{"location":"plugins/intentional-textual-ui/docs/reference/#plugins.intentional-textual-ui.src.intentional_textual_ui.audio_stream_ui.AudioStreamInterface.compose","title":"<code>compose()</code>","text":"<p>Layout of the UI.</p> Source code in <code>plugins/intentional-textual-ui/src/intentional_textual_ui/audio_stream_ui.py</code> <pre><code>def compose(self) -&gt; ComposeResult:\n    \"\"\"\n    Layout of the UI.\n    \"\"\"\n    yield Horizontal(\n        Vertical(\n            Markdown(\"# Chat History\"),\n            ScrollableContainer(ChatHistory()),\n            UserStatus(\"# User is silent...\"),\n            classes=\"column bordered chat\",\n        ),\n        Vertical(\n            Markdown(\"# System Prompt\"),\n            SystemPrompt(),\n            classes=\"bordered column\",\n        ),\n    )\n</code></pre>"},{"location":"plugins/intentional-textual-ui/docs/reference/#plugins.intentional-textual-ui.src.intentional_textual_ui.audio_stream_ui.AudioStreamInterface.handle_audio_messages","title":"<code>handle_audio_messages(event)</code>  <code>async</code>","text":"<p>Plays audio responses from the bot and updates the bot status line.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Dict[str, Any]</code> <p>The event dictionary containing the audio message.</p> required Source code in <code>plugins/intentional-textual-ui/src/intentional_textual_ui/audio_stream_ui.py</code> <pre><code>async def handle_audio_messages(self, event: Dict[str, Any]) -&gt; None:\n    \"\"\"\n    Plays audio responses from the bot and updates the bot status line.\n\n    Args:\n        event: The event dictionary containing the audio message.\n    \"\"\"\n    # self.query_one(BotStatus).update(\"# Bot is speaking...\")\n    if event[\"delta\"]:\n        self.audio_handler.play_audio(event[\"delta\"])\n</code></pre>"},{"location":"plugins/intentional-textual-ui/docs/reference/#plugins.intentional-textual-ui.src.intentional_textual_ui.audio_stream_ui.AudioStreamInterface.handle_conversation_end","title":"<code>handle_conversation_end(_)</code>  <code>async</code>","text":"<p>At the end of the conversation, closes the UI.</p> Source code in <code>plugins/intentional-textual-ui/src/intentional_textual_ui/audio_stream_ui.py</code> <pre><code>async def handle_conversation_end(self, _) -&gt; None:\n    \"\"\"\n    At the end of the conversation, closes the UI.\n    \"\"\"\n    self.exit(0)\n    self.audio_handler.stop_streaming()\n    self.audio_handler.cleanup()\n</code></pre>"},{"location":"plugins/intentional-textual-ui/docs/reference/#plugins.intentional-textual-ui.src.intentional_textual_ui.audio_stream_ui.AudioStreamInterface.handle_finish_user_response","title":"<code>handle_finish_user_response(_)</code>  <code>async</code>","text":"<p>Updates the user status when they stop speaking.</p> Source code in <code>plugins/intentional-textual-ui/src/intentional_textual_ui/audio_stream_ui.py</code> <pre><code>async def handle_finish_user_response(self, _) -&gt; None:\n    \"\"\"\n    Updates the user status when they stop speaking.\n    \"\"\"\n    self.query_one(UserStatus).update(\"# User is silent...\")\n</code></pre>"},{"location":"plugins/intentional-textual-ui/docs/reference/#plugins.intentional-textual-ui.src.intentional_textual_ui.audio_stream_ui.AudioStreamInterface.handle_start_user_response","title":"<code>handle_start_user_response(_)</code>  <code>async</code>","text":"<p>Updates the user status when they start speaking.</p> Source code in <code>plugins/intentional-textual-ui/src/intentional_textual_ui/audio_stream_ui.py</code> <pre><code>async def handle_start_user_response(self, _) -&gt; None:\n    \"\"\"\n    Updates the user status when they start speaking.\n    \"\"\"\n    self.query_one(UserStatus).update(\"# User is speaking...\")\n</code></pre>"},{"location":"plugins/intentional-textual-ui/docs/reference/#plugins.intentional-textual-ui.src.intentional_textual_ui.audio_stream_ui.AudioStreamInterface.handle_system_prompt_updated","title":"<code>handle_system_prompt_updated(event)</code>  <code>async</code>","text":"<p>Prints to the console any text message from the bot.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Dict[str, Any]</code> <p>The event dictionary containing the message.</p> required Source code in <code>plugins/intentional-textual-ui/src/intentional_textual_ui/audio_stream_ui.py</code> <pre><code>async def handle_system_prompt_updated(self, event: Dict[str, Any]) -&gt; None:\n    \"\"\"\n    Prints to the console any text message from the bot.\n\n    Args:\n        event: The event dictionary containing the message.\n    \"\"\"\n    self.query_one(SystemPrompt).update(event[\"system_prompt\"])\n</code></pre>"},{"location":"plugins/intentional-textual-ui/docs/reference/#plugins.intentional-textual-ui.src.intentional_textual_ui.audio_stream_ui.AudioStreamInterface.handle_transcript","title":"<code>handle_transcript(event)</code>  <code>async</code>","text":"<p>Prints the transcripts in the chat history.</p> Source code in <code>plugins/intentional-textual-ui/src/intentional_textual_ui/audio_stream_ui.py</code> <pre><code>async def handle_transcript(self, event: Dict[str, Any]) -&gt; None:\n    \"\"\"\n    Prints the transcripts in the chat history.\n    \"\"\"\n    if event[\"type\"] == \"on_user_speech_transcribed\":\n        self.conversation += f\"\\n**User:** {event['transcript']}\\n\"\n    elif event[\"type\"] == \"on_llm_speech_transcribed\":\n        self.conversation += f\"\\n**Assistant:** {event['transcript']}\\n\"\n    else:\n        log.debug(\"Unknown event with transcript received.\", event_name=event[\"type\"])\n        self.conversation += f\"\\n**{event['type']}:** {event['transcript']}\\n\"\n    self.query_one(ChatHistory).update(self.conversation)\n</code></pre>"},{"location":"plugins/intentional-textual-ui/docs/reference/#plugins.intentional-textual-ui.src.intentional_textual_ui.audio_stream_ui.AudioStreamInterface.on_mount","title":"<code>on_mount()</code>","text":"<p>Operations to be performed at mount time.</p> Source code in <code>plugins/intentional-textual-ui/src/intentional_textual_ui/audio_stream_ui.py</code> <pre><code>def on_mount(self) -&gt; None:\n    \"\"\"\n    Operations to be performed at mount time.\n    \"\"\"\n    self.query_one(SystemPrompt).update(self.bot.llm.system_prompt)\n</code></pre>"},{"location":"plugins/intentional-textual-ui/docs/reference/#plugins.intentional-textual-ui.src.intentional_textual_ui.audio_stream_ui.ChatHistory","title":"<code>ChatHistory</code>","text":"<p>               Bases: <code>Markdown</code></p> <p>A markdown widget that displays the chat history.</p> Source code in <code>plugins/intentional-textual-ui/src/intentional_textual_ui/audio_stream_ui.py</code> <pre><code>class ChatHistory(Markdown):\n    \"\"\"\n    A markdown widget that displays the chat history.\n    \"\"\"\n</code></pre>"},{"location":"plugins/intentional-textual-ui/docs/reference/#plugins.intentional-textual-ui.src.intentional_textual_ui.audio_stream_ui.SystemPrompt","title":"<code>SystemPrompt</code>","text":"<p>               Bases: <code>Markdown</code></p> <p>A markdown widget that displays the system prompt.</p> Source code in <code>plugins/intentional-textual-ui/src/intentional_textual_ui/audio_stream_ui.py</code> <pre><code>class SystemPrompt(Markdown):\n    \"\"\"\n    A markdown widget that displays the system prompt.\n    \"\"\"\n</code></pre>"},{"location":"plugins/intentional-textual-ui/docs/reference/#plugins.intentional-textual-ui.src.intentional_textual_ui.audio_stream_ui.UserStatus","title":"<code>UserStatus</code>","text":"<p>               Bases: <code>Markdown</code></p> <p>A markdown widget that displays the user status (speaking/silent).</p> Source code in <code>plugins/intentional-textual-ui/src/intentional_textual_ui/audio_stream_ui.py</code> <pre><code>class UserStatus(Markdown):\n    \"\"\"\n    A markdown widget that displays the user status (speaking/silent).\n    \"\"\"\n</code></pre>"},{"location":"plugins/intentional-textual-ui/docs/reference/#plugins.intentional-textual-ui.src.intentional_textual_ui.bot_interface","title":"<code>bot_interface</code>","text":"<p>Local bot interface for Intentional.</p>"},{"location":"plugins/intentional-textual-ui/docs/reference/#plugins.intentional-textual-ui.src.intentional_textual_ui.bot_interface.TextualUIBotInterface","title":"<code>TextualUIBotInterface</code>","text":"<p>               Bases: <code>BotInterface</code></p> <p>Bot that uses a Textual UI command line interface to interact with the user.</p> Source code in <code>plugins/intentional-textual-ui/src/intentional_textual_ui/bot_interface.py</code> <pre><code>class TextualUIBotInterface(BotInterface):\n    \"\"\"\n    Bot that uses a Textual UI command line interface to interact with the user.\n    \"\"\"\n\n    name = \"textual_ui\"\n\n    def __init__(self, config: Dict[str, Any], intent_router: IntentRouter):\n        # Init the structure\n        bot_structure_config = config.pop(\"bot\", None)\n        if not bot_structure_config:\n            raise ValueError(\n                f\"{self.__class__.__name__} requires a 'bot' configuration key to know how to structure the bot.\"\n            )\n        log.debug(\"Creating bot structure\", bot_structure_config=bot_structure_config)\n        self.bot: BotStructure = load_bot_structure_from_dict(intent_router=intent_router, config=bot_structure_config)\n\n        # Check the modality\n        self.modality = config.pop(\"modality\")\n        log.debug(\"Setting modality for bot structure\", modality=self.modality)\n\n        # Handlers\n        self.audio_handler = None\n        self.input_handler = None\n        self.app = None\n\n    async def run(self) -&gt; None:\n        \"\"\"\n        Chooses the specific loop to use for this combination of bot and modality and kicks it off.\n        \"\"\"\n        if self.modality == \"audio_stream\":\n            await self._run_audio_stream(self.bot)\n\n        elif self.modality == \"text_messages\":\n            await self._run_text_messages(self.bot)\n\n        else:\n            raise ValueError(\n                f\"Modality '{self.modality}' is not yet supported.\"\n                \"These are the supported modalities: 'audio_stream', 'text_messages'.\"\n            )\n\n    async def _run_text_messages(self, bot: BotStructure) -&gt; None:\n        \"\"\"\n        Runs the CLI interface for the text turns modality.\n        \"\"\"\n        log.debug(\"Running in text turns mode.\")\n        self.app = TextChatInterface(bot=bot)\n        await bot.connect()\n        await self._launch_ui()\n\n    async def _run_audio_stream(self, bot: BotStructure) -&gt; None:\n        \"\"\"\n        Runs the CLI interface for the continuous audio streaming modality.\n        \"\"\"\n        log.debug(\"Running in continuous audio streaming mode.\")\n\n        try:\n            self.audio_handler = AudioHandler()\n            self.app = AudioStreamInterface(bot=bot, audio_output_handler=self.audio_handler)\n            await bot.connect()\n            await self._launch_ui(gather=[self.bot.run()])\n            await self.audio_handler.start_streaming(bot.send)\n\n        except Exception as e:  # pylint: disable=broad-except\n            raise e\n        finally:\n            self.audio_handler.stop_streaming()\n            self.audio_handler.cleanup()\n            await bot.disconnect()\n            print(\"Chat is finished. Bye!\")\n\n    async def _launch_ui(self, gather: List[Callable] = None):\n        \"\"\"\n        Launches the Textual UI interface. If there's any async task that should be run in parallel, it can be passed\n        as a list of callables to the `gather` parameter.\n\n        Args:\n            gather: A list of callables that should be run in parallel with the UI.\n        \"\"\"\n        self.app._loop = asyncio.get_running_loop()  # pylint: disable=protected-access\n        self.app._thread_id = threading.get_ident()  # pylint: disable=protected-access\n        with self.app._context():  # pylint: disable=protected-access\n            try:\n                if not gather:\n                    await self.app.run_async(\n                        headless=False,\n                        inline=False,\n                        inline_no_clear=False,\n                        mouse=True,\n                        size=None,\n                        auto_pilot=None,\n                    )\n                else:\n                    asyncio.gather(\n                        self.app.run_async(\n                            headless=False,\n                            inline=False,\n                            inline_no_clear=False,\n                            mouse=True,\n                            size=None,\n                            auto_pilot=None,\n                        ),\n                        *gather,\n                    )\n            finally:\n                self.app._loop = None  # pylint: disable=protected-access\n                self.app._thread_id = 0  # pylint: disable=protected-access\n</code></pre>"},{"location":"plugins/intentional-textual-ui/docs/reference/#plugins.intentional-textual-ui.src.intentional_textual_ui.bot_interface.TextualUIBotInterface.run","title":"<code>run()</code>  <code>async</code>","text":"<p>Chooses the specific loop to use for this combination of bot and modality and kicks it off.</p> Source code in <code>plugins/intentional-textual-ui/src/intentional_textual_ui/bot_interface.py</code> <pre><code>async def run(self) -&gt; None:\n    \"\"\"\n    Chooses the specific loop to use for this combination of bot and modality and kicks it off.\n    \"\"\"\n    if self.modality == \"audio_stream\":\n        await self._run_audio_stream(self.bot)\n\n    elif self.modality == \"text_messages\":\n        await self._run_text_messages(self.bot)\n\n    else:\n        raise ValueError(\n            f\"Modality '{self.modality}' is not yet supported.\"\n            \"These are the supported modalities: 'audio_stream', 'text_messages'.\"\n        )\n</code></pre>"},{"location":"plugins/intentional-textual-ui/docs/reference/#plugins.intentional-textual-ui.src.intentional_textual_ui.text_chat_ui","title":"<code>text_chat_ui</code>","text":"<p>Textual UI for text-based bots.</p>"},{"location":"plugins/intentional-textual-ui/docs/reference/#plugins.intentional-textual-ui.src.intentional_textual_ui.text_chat_ui.ChatHistory","title":"<code>ChatHistory</code>","text":"<p>               Bases: <code>Markdown</code></p> <p>A markdown widget that displays the chat history.</p> Source code in <code>plugins/intentional-textual-ui/src/intentional_textual_ui/text_chat_ui.py</code> <pre><code>class ChatHistory(Markdown):\n    \"\"\"\n    A markdown widget that displays the chat history.\n    \"\"\"\n</code></pre>"},{"location":"plugins/intentional-textual-ui/docs/reference/#plugins.intentional-textual-ui.src.intentional_textual_ui.text_chat_ui.MessageBox","title":"<code>MessageBox</code>","text":"<p>               Bases: <code>Input</code></p> <p>An input widget that allows the user to type a message.</p> Source code in <code>plugins/intentional-textual-ui/src/intentional_textual_ui/text_chat_ui.py</code> <pre><code>class MessageBox(Input):\n    \"\"\"\n    An input widget that allows the user to type a message.\n    \"\"\"\n\n    placeholder = \"Message...\"\n</code></pre>"},{"location":"plugins/intentional-textual-ui/docs/reference/#plugins.intentional-textual-ui.src.intentional_textual_ui.text_chat_ui.SystemPrompt","title":"<code>SystemPrompt</code>","text":"<p>               Bases: <code>Markdown</code></p> <p>A markdown widget that displays the system prompt.</p> Source code in <code>plugins/intentional-textual-ui/src/intentional_textual_ui/text_chat_ui.py</code> <pre><code>class SystemPrompt(Markdown):\n    \"\"\"\n    A markdown widget that displays the system prompt.\n    \"\"\"\n</code></pre>"},{"location":"plugins/intentional-textual-ui/docs/reference/#plugins.intentional-textual-ui.src.intentional_textual_ui.text_chat_ui.TextChatInterface","title":"<code>TextChatInterface</code>","text":"<p>               Bases: <code>App</code></p> <p>The main interface class for the text-based bot UI.</p> Source code in <code>plugins/intentional-textual-ui/src/intentional_textual_ui/text_chat_ui.py</code> <pre><code>class TextChatInterface(App):\n    \"\"\"\n    The main interface class for the text-based bot UI.\n    \"\"\"\n\n    CSS_PATH = \"example.tcss\"\n\n    def __init__(self, bot: BotStructure):\n        super().__init__()\n        self.bot = bot\n        self.bot.add_event_handler(\"on_text_message_from_llm\", self.handle_text_messages)\n        self.bot.add_event_handler(\"on_llm_starts_generating_response\", self.handle_start_text_response)\n        self.bot.add_event_handler(\"on_llm_stops_generating_response\", self.handle_finish_text_response)\n        self.bot.add_event_handler(\"on_system_prompt_updated\", self.handle_system_prompt_updated)\n\n        self.conversation = \"\"\n        self.generating_response = False\n\n    def compose(self) -&gt; ComposeResult:\n        \"\"\"\n        Layout for the text-based bot UI.\n        \"\"\"\n        yield Horizontal(\n            Vertical(\n                Markdown(\"# Chat History\"),\n                ScrollableContainer(ChatHistory()),\n                MessageBox(placeholder=\"Message...\"),\n                classes=\"column bordered chat\",\n            ),\n            Vertical(\n                Markdown(\"# System Prompt\"),\n                ScrollableContainer(SystemPrompt()),\n                classes=\"column bordered\",\n            ),\n        )\n\n    def on_mount(self) -&gt; None:\n        \"\"\"\n        Operations to perform when the UI is mounted.\n        \"\"\"\n        self.query_one(SystemPrompt).update(self.bot.llm.system_prompt)\n        self.query_one(MessageBox).focus()\n\n    @on(MessageBox.Submitted)\n    async def send_message(self, event: MessageBox.Changed) -&gt; None:\n        \"\"\"\n        Sends a message to the bot when the user presses enter.\n\n        Args:\n            event: The event containing the message to send.\n        \"\"\"\n        self.conversation += \"\\n\\n**User**: \" + event.value\n        self.query_one(MessageBox).clear()\n        self.query_one(ChatHistory).update(self.conversation)\n        await self.bot.send({\"text_message\": {\"role\": \"user\", \"content\": event.value}})\n\n    async def handle_start_text_response(self, _) -&gt; None:\n        \"\"\"\n        Prints to the console when the bot starts generating a text response.\n        \"\"\"\n        if not self.generating_response:  # To avoid the duplication due to function calls.\n            self.generating_response = True\n            self.conversation += \"\\n\\n**Assistant:** \"\n            self.query_one(ChatHistory).update(self.conversation)\n\n    async def handle_finish_text_response(self, _) -&gt; None:\n        \"\"\"\n        Prints to the console when the bot stops generating a text response.\n        \"\"\"\n        self.generating_response = False\n\n    async def handle_text_messages(self, event: Dict[str, Any]) -&gt; None:\n        \"\"\"\n        Prints to the console any text message from the bot. It is usually a chunk as the output is being streamed out.\n\n        Args:\n            event: The event dictionary containing the message chunk.\n        \"\"\"\n        if event[\"delta\"]:\n            self.conversation += event[\"delta\"]\n            self.query_one(ChatHistory).update(self.conversation)\n\n    async def handle_system_prompt_updated(self, event: Dict[str, Any]) -&gt; None:\n        \"\"\"\n        Prints to the console any text message from the bot.\n\n        Args:\n            event: The event dictionary containing the message.\n        \"\"\"\n        self.query_one(SystemPrompt).update(event[\"system_prompt\"])  # self.bot.llm.system_prompt)\n</code></pre>"},{"location":"plugins/intentional-textual-ui/docs/reference/#plugins.intentional-textual-ui.src.intentional_textual_ui.text_chat_ui.TextChatInterface.compose","title":"<code>compose()</code>","text":"<p>Layout for the text-based bot UI.</p> Source code in <code>plugins/intentional-textual-ui/src/intentional_textual_ui/text_chat_ui.py</code> <pre><code>def compose(self) -&gt; ComposeResult:\n    \"\"\"\n    Layout for the text-based bot UI.\n    \"\"\"\n    yield Horizontal(\n        Vertical(\n            Markdown(\"# Chat History\"),\n            ScrollableContainer(ChatHistory()),\n            MessageBox(placeholder=\"Message...\"),\n            classes=\"column bordered chat\",\n        ),\n        Vertical(\n            Markdown(\"# System Prompt\"),\n            ScrollableContainer(SystemPrompt()),\n            classes=\"column bordered\",\n        ),\n    )\n</code></pre>"},{"location":"plugins/intentional-textual-ui/docs/reference/#plugins.intentional-textual-ui.src.intentional_textual_ui.text_chat_ui.TextChatInterface.handle_finish_text_response","title":"<code>handle_finish_text_response(_)</code>  <code>async</code>","text":"<p>Prints to the console when the bot stops generating a text response.</p> Source code in <code>plugins/intentional-textual-ui/src/intentional_textual_ui/text_chat_ui.py</code> <pre><code>async def handle_finish_text_response(self, _) -&gt; None:\n    \"\"\"\n    Prints to the console when the bot stops generating a text response.\n    \"\"\"\n    self.generating_response = False\n</code></pre>"},{"location":"plugins/intentional-textual-ui/docs/reference/#plugins.intentional-textual-ui.src.intentional_textual_ui.text_chat_ui.TextChatInterface.handle_start_text_response","title":"<code>handle_start_text_response(_)</code>  <code>async</code>","text":"<p>Prints to the console when the bot starts generating a text response.</p> Source code in <code>plugins/intentional-textual-ui/src/intentional_textual_ui/text_chat_ui.py</code> <pre><code>async def handle_start_text_response(self, _) -&gt; None:\n    \"\"\"\n    Prints to the console when the bot starts generating a text response.\n    \"\"\"\n    if not self.generating_response:  # To avoid the duplication due to function calls.\n        self.generating_response = True\n        self.conversation += \"\\n\\n**Assistant:** \"\n        self.query_one(ChatHistory).update(self.conversation)\n</code></pre>"},{"location":"plugins/intentional-textual-ui/docs/reference/#plugins.intentional-textual-ui.src.intentional_textual_ui.text_chat_ui.TextChatInterface.handle_system_prompt_updated","title":"<code>handle_system_prompt_updated(event)</code>  <code>async</code>","text":"<p>Prints to the console any text message from the bot.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Dict[str, Any]</code> <p>The event dictionary containing the message.</p> required Source code in <code>plugins/intentional-textual-ui/src/intentional_textual_ui/text_chat_ui.py</code> <pre><code>async def handle_system_prompt_updated(self, event: Dict[str, Any]) -&gt; None:\n    \"\"\"\n    Prints to the console any text message from the bot.\n\n    Args:\n        event: The event dictionary containing the message.\n    \"\"\"\n    self.query_one(SystemPrompt).update(event[\"system_prompt\"])  # self.bot.llm.system_prompt)\n</code></pre>"},{"location":"plugins/intentional-textual-ui/docs/reference/#plugins.intentional-textual-ui.src.intentional_textual_ui.text_chat_ui.TextChatInterface.handle_text_messages","title":"<code>handle_text_messages(event)</code>  <code>async</code>","text":"<p>Prints to the console any text message from the bot. It is usually a chunk as the output is being streamed out.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Dict[str, Any]</code> <p>The event dictionary containing the message chunk.</p> required Source code in <code>plugins/intentional-textual-ui/src/intentional_textual_ui/text_chat_ui.py</code> <pre><code>async def handle_text_messages(self, event: Dict[str, Any]) -&gt; None:\n    \"\"\"\n    Prints to the console any text message from the bot. It is usually a chunk as the output is being streamed out.\n\n    Args:\n        event: The event dictionary containing the message chunk.\n    \"\"\"\n    if event[\"delta\"]:\n        self.conversation += event[\"delta\"]\n        self.query_one(ChatHistory).update(self.conversation)\n</code></pre>"},{"location":"plugins/intentional-textual-ui/docs/reference/#plugins.intentional-textual-ui.src.intentional_textual_ui.text_chat_ui.TextChatInterface.on_mount","title":"<code>on_mount()</code>","text":"<p>Operations to perform when the UI is mounted.</p> Source code in <code>plugins/intentional-textual-ui/src/intentional_textual_ui/text_chat_ui.py</code> <pre><code>def on_mount(self) -&gt; None:\n    \"\"\"\n    Operations to perform when the UI is mounted.\n    \"\"\"\n    self.query_one(SystemPrompt).update(self.bot.llm.system_prompt)\n    self.query_one(MessageBox).focus()\n</code></pre>"},{"location":"plugins/intentional-textual-ui/docs/reference/#plugins.intentional-textual-ui.src.intentional_textual_ui.text_chat_ui.TextChatInterface.send_message","title":"<code>send_message(event)</code>  <code>async</code>","text":"<p>Sends a message to the bot when the user presses enter.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Changed</code> <p>The event containing the message to send.</p> required Source code in <code>plugins/intentional-textual-ui/src/intentional_textual_ui/text_chat_ui.py</code> <pre><code>@on(MessageBox.Submitted)\nasync def send_message(self, event: MessageBox.Changed) -&gt; None:\n    \"\"\"\n    Sends a message to the bot when the user presses enter.\n\n    Args:\n        event: The event containing the message to send.\n    \"\"\"\n    self.conversation += \"\\n\\n**User**: \" + event.value\n    self.query_one(MessageBox).clear()\n    self.query_one(ChatHistory).update(self.conversation)\n    await self.bot.send({\"text_message\": {\"role\": \"user\", \"content\": event.value}})\n</code></pre>"}]}